[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The {targets} R package user manual",
    "section": "",
    "text": "1 Introduction\nPipeline tools coordinate the pieces of computationally demanding analysis projects. The targets package is a Make-like pipeline tool for statistics and data science in R. The package skips costly runtime for tasks that are already up to date, orchestrates the necessary computation with implicit parallel computing, and abstracts files as R objects. If all the current output matches the current upstream code and data, then the whole pipeline is up to date, and the results are more trustworthy than otherwise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "The {targets} R package user manual",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nData analysis can be slow. A round of scientific computation can take several minutes, hours, or even days to complete. After it finishes, if you update your code or data, your hard-earned results may no longer be valid. Unchecked, this invalidation creates chronic Sisyphean loop:\n\nLaunch the code.\nWait while it runs.\nDiscover an issue.\nRestart from scratch.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#pipeline-tools",
    "href": "index.html#pipeline-tools",
    "title": "The {targets} R package user manual",
    "section": "1.2 Pipeline tools",
    "text": "1.2 Pipeline tools\nPipeline tools like GNU Make break the cycle. They watch the dependency graph of the whole workflow and skip steps, or “targets”, whose code, data, and upstream dependencies have not changed since the last run of the pipeline. When all targets are up to date, this is evidence that the results match the underlying code and data, which helps us trust the results and confirm the computation is reproducible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#the-targets-package",
    "href": "index.html#the-targets-package",
    "title": "The {targets} R package user manual",
    "section": "1.3 The targets package",
    "text": "1.3 The targets package\nUnlike most pipeline tools, which are language agnostic or Python-focused, the targets package allows data scientists and researchers to work entirely within R. targets implicitly nudges users toward a clean, function-oriented programming style that fits the intent of the R language and helps practitioners maintain their data analysis projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#about-this-manual",
    "href": "index.html#about-this-manual",
    "title": "The {targets} R package user manual",
    "section": "1.4 About this manual",
    "text": "1.4 About this manual\nThis manual is a step-by-step user guide to targets. The most important chapters are the walkthrough, help guide, and debugging guide. Subsequent chapters explain how to write code, manage projects, utilize high-performance computing, transition from drake, and more. See the documentation website for most other major resources, including installation instructions, links to example projects, and a reference page with all user-side functions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-about-drake",
    "href": "index.html#what-about-drake",
    "title": "The {targets} R package user manual",
    "section": "1.5 What about drake?",
    "text": "1.5 What about drake?\nThe drake is an older R-focused pipeline tool, and targets is drake’s long-term successor. There is a special chapter to explain why targets was created, what this means for drake’s future, advice for drake users transitioning to targets, and the main technical advantages of targets over drake.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "2  A walkthrough to get started",
    "section": "",
    "text": "2.1 About this example\nThis chapter walks through a short example of a targets-powered data analysis project. The source code is available at https://github.com/wlandau/targets-four-minutes, and you can visit https://rstudio.cloud/project/3946303 to try out the code in a web browser (no download or installation required). The documentation website links to other examples. The contents of the chapter are also explained in a four-minute video tutorial:\nThe goal of this short analysis is to assess the relationship among ozone and temperature in base R’s airquality dataset. We track a data file, prepare a dataset, fit a model, and plot the model against the data.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#file-structure",
    "href": "walkthrough.html#file-structure",
    "title": "2  A walkthrough to get started",
    "section": "2.2 File structure",
    "text": "2.2 File structure\nThe file structure of the project looks like this.\n\n├── _targets.R\n├── data.csv\n├── R/\n│   ├── functions.R\n\ndata.csv contains the data we want to analyze.\nOzone,Solar.R,Wind,Temp,Month,Day\n36,118,8.0,72,5,2\n12,149,12.6,74,5,3\n...\nR/functions.R contains our custom user-defined functions. (See the functions chapter for a discussion of function-oriented workflows.)\n\n# R/functions.R\nget_data &lt;- function(file) {\n  read_csv(file, col_types = cols()) %&gt;%\n    filter(!is.na(Ozone))\n}\n\nfit_model &lt;- function(data) {\n  lm(Ozone ~ Temp, data) %&gt;%\n    coefficients()\n}\n\nplot_model &lt;- function(model, data) {\n  ggplot(data) +\n    geom_point(aes(x = Temp, y = Ozone)) +\n    geom_abline(intercept = model[1], slope = model[2])\n}",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#target-script-file",
    "href": "walkthrough.html#target-script-file",
    "title": "2  A walkthrough to get started",
    "section": "2.3 Target script file",
    "text": "2.3 Target script file\nWhereas files data.csv and functions.R are typical user-defined components of a project-oriented workflow, the target script file _targets.R file is special. Every targets workflow needs a target script file to configure and define the pipeline.1 The use_targets() function in targets version &gt;= 0.12.0 creates an initial target script with comments to help you fill it in. Ours looks like this:\n\n# _targets.R file\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = c(\"readr\", \"dplyr\", \"ggplot2\"))\nlist(\n  tar_target(file, \"data.csv\", format = \"file\"),\n  tar_target(data, get_data(file)),\n  tar_target(model, fit_model(data)),\n  tar_target(plot, plot_model(model, data))\n)\n\nAll target script files have these requirements.\n\nLoad the packages needed to define the pipeline, e.g. targets itself.2\nUse tar_option_set() to declare the packages that the targets themselves need, as well as other settings such as the default storage format.\nLoad your custom functions and small input objects into the R session: in our case, with source(\"R/functions.R\").\nWrite the pipeline at the bottom of _targets.R. A pipeline is a list of target definition objects, which you can create with tar_target(). Each target is a step of the analysis. It looks and feels like a variable in R, but during tar_make(), it will save the output as a file in _targets/objects/.\n\n\n\n\n\n\n\nTipStart small\n\n\n\nEven if you plan to create a large-scale heavy-duty pipeline with hundreds of time-consuming targets, it is best to start small. First create a version of the pipeline with a small number of quick-to-run targets, follow the sections below to inspect and test it, and then scale up to the full-sized pipeline after you are sure everything is working.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#inspect-the-pipeline",
    "href": "walkthrough.html#inspect-the-pipeline",
    "title": "2  A walkthrough to get started",
    "section": "2.4 Inspect the pipeline",
    "text": "2.4 Inspect the pipeline\nBefore you run the pipeline for real, it is best to check for obvious errors. tar_manifest() lists verbose information about each target.\n\ntar_manifest(fields = all_of(\"command\"))\n#&gt; # A tibble: 4 × 2\n#&gt;   name  command                  \n#&gt;   &lt;chr&gt; &lt;chr&gt;                    \n#&gt; 1 file  \"\\\"data.csv\\\"\"           \n#&gt; 2 data  \"get_data(file)\"         \n#&gt; 3 model \"fit_model(data)\"        \n#&gt; 4 plot  \"plot_model(model, data)\"\n\ntar_visnetwork() displays the dependency graph of the pipeline, showing a natural left-to-right flow of work. It is good practice to make sure the graph has the correct nodes connected with the correct edges. Read more about dependencies and the graph in the dependencies section of a later chapter.\n\ntar_visnetwork()",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#run-the-pipeline",
    "href": "walkthrough.html#run-the-pipeline",
    "title": "2  A walkthrough to get started",
    "section": "2.5 Run the pipeline",
    "text": "2.5 Run the pipeline\ntar_make() runs the pipeline. It creates a reproducible new external R process which then reads the target script and runs the correct targets in the correct order.3\n\ntar_make()\n#&gt; + file dispatched\n#&gt; ✔ file completed [348ms, 2.89 kB]\n#&gt; + data dispatched\n#&gt; ✔ data completed [100ms, 1.36 kB]\n#&gt; + model dispatched\n#&gt; ✔ model completed [2ms, 111 B]\n#&gt; + plot dispatched\n#&gt; ✔ plot completed [29ms, 112.68 kB]\n#&gt; ✔ ended pipeline [779ms, 4 completed, 0 skipped]\n\nThe output of the pipeline is saved to the _targets/ data store, and you can read the output with tar_read() (see also tar_load()).\n\ntar_read(plot)\n\n\n\n\n\n\n\n\nThe next time you run tar_make(), targets skips everything that is already up to date, which saves a lot of time in large projects with long runtimes.\n\ntar_make()\n#&gt; ✔ skipped pipeline [66ms, 4 skipped]\n\nYou can use tar_visnetwork() and tar_outdated() to check ahead of time which targets are up to date.\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_outdated()\n#&gt; character(0)",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#changes",
    "href": "walkthrough.html#changes",
    "title": "2  A walkthrough to get started",
    "section": "2.6 Changes",
    "text": "2.6 Changes\nThe targets package notices when you make changes to code and data, and those changes affect which targets rerun and which targets are skipped.4\n\n2.6.1 Change code\nIf you change one of your functions, the targets that depend on it will no longer be up to date, and tar_make() will rebuild them. For example, let’s increase the font size of the plot.\n\n# Edit functions.R...\nplot_model &lt;- function(model, data) {\n  ggplot(data) +\n    geom_point(aes(x = Temp, y = Ozone)) +\n    geom_abline(intercept = model[1], slope = model[2]) +\n    theme_gray(24) # Increased the font size.\n}\n\ntargets detects the change. plot is “outdated” (i.e. invalidated) and the others are still up to date.\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_outdated()\n#&gt; [1] \"plot\"\n\nThus, tar_make() reruns plot and nothing else.5\n\ntar_make()\n#&gt; + plot dispatched\n#&gt; ✔ plot completed [140ms, 230.54 kB]\n#&gt; ✔ ended pipeline [712ms, 1 completed, 3 skipped]\n\nSure enough, we have a new plot.\n\ntar_read(plot)\n\n\n\n\n\n\n\n\n\n\n2.6.2 Change data\nIf we change the data file data.csv, targets notices the change. This is because file is a file target (i.e. with format = \"file\" in tar_target()), and the return value from last tar_make() identified \"data.csv\" as the file to be tracked for changes. Let’s try it out. Below, let’s use only the first 100 rows of the airquality dataset.\n\nwrite_csv(head(airquality, n = 100), \"data.csv\")\n\nSure enough, raw_data_file and everything downstream is out of date, so all our targets are outdated.\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_outdated()\n#&gt; [1] \"file\"  \"plot\"  \"data\"  \"model\"\n\n\ntar_make()\n#&gt; + file dispatched\n#&gt; ✔ file completed [354ms, 1.88 kB]\n#&gt; + data dispatched\n#&gt; ✔ data completed [100ms, 1.00 kB]\n#&gt; + model dispatched\n#&gt; ✔ model completed [3ms, 112 B]\n#&gt; + plot dispatched\n#&gt; ✔ plot completed [65ms, 229.74 kB]\n#&gt; ✔ ended pipeline [886ms, 4 completed, 0 skipped]",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#read-metadata",
    "href": "walkthrough.html#read-metadata",
    "title": "2  A walkthrough to get started",
    "section": "2.7 Read metadata",
    "text": "2.7 Read metadata\n\n\n\n\n\n\nTipPerformance\n\n\n\nSee the performance chapter for options, settings, and other choices to make the pipeline more efficient. This chapter also has guidance for monitoring the progress of a running pipeline.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "walkthrough.html#footnotes",
    "href": "walkthrough.html#footnotes",
    "title": "2  A walkthrough to get started",
    "section": "",
    "text": "By default, the target script is a file called _targets.R in the project’s root directory. However, you can set the target script file path to something other than _targets.R. You can either set the path persistently for your project using tar_config_set(), or you can set it temporarily for an individual function call using the script argument of tar_make() and related functions.↩︎\ntarget scripts created with tar_script() automatically insert a library(targets) line at the top by default.↩︎\nIn targets version 0.3.1.9000 and above, you can set the path of the local data store to something other than _targets/. A project-level _targets.yaml file keeps track of the path. Functions tar_config_set() and tar_config_get() can help.↩︎\nInternally, special rules called “cues” decide whether a target reruns. The tar_cue() function lets you suppress some of these cues, and the tarchetypes package supports nuanced cue factories and target factories to further customize target invalidation behavior. The tar_cue() function documentation explains cues in detail, as well as specifics on how targets detects changes to upstream dependencies.↩︎\nWe would see similar behavior if we changed the R expressions in any tar_target() calls in the target script file.↩︎",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>A walkthrough to get started</span>"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "3  Help",
    "section": "",
    "text": "3.1 Before you post\nWorking with targets can be challenging, and sometimes you may need help from a human. However, offering that help can be just as challenging because every use case is different. If your questions are easy to answer, you will get faster, more useful responses. This chapter shares advice about asking effective questions. Please read this page in full before reaching out for support.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "help.html#before-you-post",
    "href": "help.html#before-you-post",
    "title": "3  Help",
    "section": "",
    "text": "3.1.1 Code of conduct\nAll developers, users, and other discussants must follow the code of conduct. Please read it before engaging in discussion.\n\n\n3.1.2 Sensitive information\nThe discussion forums from Section 3.4 are open to the public, and anyone with an internet connection can read them. Please do not share any company confidential information, personally identifiable information, private research data, or any other sensitive information.\n\n\n3.1.3 Search existing posts\nIt is tedious to answer the same question repeatedly, so please check if your question already has an answer. Section 3.4 links to searchable online discussion forums with thousands of existing posts. If a relevant thread already exists, please read it first. Then, if necessary, please comment there instead of starting a new post.\n\n\n3.1.4 Try troubleshooting\nIf you are struggling with targets, please follow the steps below and take an active role in the troubleshooting process. Even if you do not solve the problem on your own, your findings from troubleshooting will help you ask effective questions when you talk to a live human.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "help.html#troubleshooting",
    "href": "help.html#troubleshooting",
    "title": "3  Help",
    "section": "3.2 Troubleshooting",
    "text": "3.2 Troubleshooting\n\n3.2.1 Update your R packages\nIf you encounter a bug in targets or tarchetypes, the bug may have already been fixed in a newer version. Before posting, please try again with the latest CRAN release of targets (or tarchetypes), then again with the GitHub development version if needed. Please see https://docs.ropensci.org/targets/#installation for installation instructions.\n\n\n3.2.2 Isolate the source of the error\nAn error in a targets pipeline might not originate from the targets package itself. The underlying cause might be a bug in one of the R packages that targets depends on, or it might come from the custom R code you inject into your pipeline. As best you can, please try to find the true source of the error. For example, try running similar code without the targets package and see if you get the same error. If the error does not come from targets itself, please do not post your question on a targets-focused discussion forum.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "help.html#when-posting",
    "href": "help.html#when-posting",
    "title": "3  Help",
    "section": "3.3 When posting",
    "text": "3.3 When posting\nIf you post a question or bug report, please clearly and concisely explain:\n\nWhat you are trying to do (1-2 sentences).\nWhat happened that you do not like.\nWhat you want to happen instead.\n\nIn addition, please post a reprex (explained below).\n\n3.3.1 Write a reprex\nWhen something goes wrong, it is natural to simply share the error and ask why it happened. But this is not enough information, not even for an expert in targets. Any response would be guesswork, and guesses about computer code are usually wrong. The person helping you needs to actually run your code. This is the only way to test that a solution really works.\nWhen you ask for help with a problem, please provide a reprex1. A reprex is a small sample of runnable code that emulates the problem. If you provide a reprex, the person helping you can troubleshoot empirically, and the responses you get will be faster and more useful.\nA viable reprex must:\n\nBe runnable by anyone else. For example, do not reference private files that only you can access.\nActually generate the expected error when someone else runs it.\n\nIn addition, a good reprex:\n\nHas clean and readable code.\nRuns quickly on any machine.\nIs simple and concise enough for someone else to digest quickly and understand without much effort.\n\nThe following posts explain how to write a good reprex.\n\nhttps://stackoverflow.com/help/minimal-reproducible-example\nhttps://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example\nhttps://reprex.tidyverse.org/\nhttps://www.tidyverse.org/blog/2017/12/workflow-vs-script/\n\nThe discussion forums of targets and tarchetypes have excellent reprexes from users.\n\nhttps://github.com/ropensci/tarchetypes/discussions/90\nhttps://github.com/ropensci/tarchetypes/discussions/102\nhttps://github.com/ropensci/tarchetypes/discussions/126\nhttps://github.com/ropensci/tarchetypes/discussions/129\nhttps://github.com/ropensci/targets/discussions/884\nhttps://github.com/ropensci/targets/discussions/953#discussioncomment-3842883\nhttps://github.com/ropensci/targets/discussions/944\nhttps://github.com/ropensci/targets/discussions/945\nhttps://github.com/ropensci/targets/discussions/954\nhttps://github.com/ropensci/targets/discussions/1481",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "help.html#sec-contact",
    "href": "help.html#sec-contact",
    "title": "3  Help",
    "section": "3.4 Contact",
    "text": "3.4 Contact\nThere are many ways to reach out.\n\n3.4.1 Community\nYou can reach out to the R community at these public forums.\n\nStack Overflow\nPosit Community\nrOpenSci Discuss\nMastodon (#rstats hashtag)\nR subreddit\n\n\n\n3.4.2 Maintainer\nTo contact the maintainer directly, please post to the relevant public GitHub Discussions page of the package (if your question does not already exist).2 Examples:\n\ntargets: https://github.com/ropensci/targets/discussions\ntarchetypes: https://github.com/ropensci/tarchetypes/discussions\ngittargets: https://github.com/ropensci/gittargets/discussions\njagstargets: https://github.com/ropensci/jagstargets/discussions\nstantargets: https://github.com/ropensci/stantargets/discussions\n\nPublic discussions are searchable, and they help answer questions from other users in the future. So please avoid private emails, instant messages, or mentions on social media.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "help.html#sec-help-user",
    "href": "help.html#sec-help-user",
    "title": "3  Help",
    "section": "3.5 How you can help",
    "text": "3.5 How you can help\nIf you have a free moment to help someone else in the discussion and issue forums, we would greatly appreciate it. Any amount of engagement, no matter how small, makes a tremendous difference. C.f. https://ropensci.org/blog/2024/02/29/targets-call-help/.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "help.html#footnotes",
    "href": "help.html#footnotes",
    "title": "3  Help",
    "section": "",
    "text": "Also known as a reproducible example, minimal reproducible example, or minimal working example.↩︎\nYou may need to create a free GitHub account, but the process is straightforward.↩︎",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Help</span>"
    ]
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "4  Debugging pipelines",
    "section": "",
    "text": "4.1 Debugging in targets is different\nThis chapter offers advice on debugging targets pipelines. The repository at https://github.com/wlandau/targets-debug has example R code. To practice the debugging techniques explained below, download the code and step through the interactive R scripts demo_small.R, demo_browser.R, and demo_workspace.R. The help chapter has advice on asking effective questions if you need help from a human.\nR code is easiest to debug when it is interactive. In the R console or RStudio IDE, you have full control over the code and the objects in the environment, and you are free to dissect, tinker, and test until you find and fix the issue. However, a pipeline is deliberately non-interactive because it tries to be automated and reproducible. In targets, several layers of encapsulation separate you from the code you want to debug:\nAlthough these layers are essential for reproducibility and scale, you will need to peel them back to diagnose and solve problems. This chapter explains how.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#debugging-in-targets-is-different",
    "href": "debugging.html#debugging-in-targets-is-different",
    "title": "4  Debugging pipelines",
    "section": "",
    "text": "The pipeline runs in an external non-interactive callr::r() process where you cannot use the R console.\nThe targets in the pipeline may run on parallel workers.\ntar_make() automatically saves output data to disk.\ntar_make() has its own error-catching system.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#debugging-example",
    "href": "debugging.html#debugging-example",
    "title": "4  Debugging pipelines",
    "section": "4.2 Debugging example",
    "text": "4.2 Debugging example\nThe following pipeline simulates a repeated measures dataset and analyzes it with generalized least squares.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\")\n)\n\nsimulate_data &lt;- function(units) {\n  options(warn = -1)\n  tibble(unit = seq_len(units), factor = rnorm(n = units, mean = 2)) %&gt;%\n    expand_grid(measurement = seq_len(4)) %&gt;%\n    mutate(outcome = sqrt(factor) + rnorm(n()))\n}\n\nanalyze_data &lt;- function(data) {\n  gls(\n    model = outcome ~ factor,\n    data = data,\n    correlation = corSymm(form = ~ measurement | unit),\n    weights = varIdent(form = ~ 1 | measurement)\n  ) %&gt;%\n    tidy(conf.int = TRUE, conf.level = 0.95)\n}\n\nlist(\n  tar_target(name = data, command = simulate_data(100)),\n  tar_target(name = model, command = analyze_data(data))\n)\n\nThis pipeline has an error.\n\n# R console\ntar_make()\n#&gt; + data dispatched\n#&gt; ✔ data completed [173ms, 4.37 kB]\n#&gt; + model dispatched\n#&gt; ✖ model errored\n#&gt; ✖ errored pipeline [281ms, 1 completed, 0 skipped]\n#&gt; Error:\n#&gt; ! Error in tar_make():\n#&gt;   missing values in object\n#&gt;   See https://books.ropensci.org/targets/debugging.html",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#finish-the-pipeline-anyway",
    "href": "debugging.html#finish-the-pipeline-anyway",
    "title": "4  Debugging pipelines",
    "section": "4.3 Finish the pipeline anyway",
    "text": "4.3 Finish the pipeline anyway\nEven if you hit an error, you can still finish the successful parts of the pipeline. The error argument of tar_option_set() and tar_target() tells each target what to do if it hits an error. For example, tar_option_set(error = \"null\") tells errored targets to return NULL. The output as a whole will not be correct or up to date, but the pipeline will finish so you can look at preliminary results. This is especially helpful with dynamic branching.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\"),\n  error = \"null\" # produce a result even if the target errored out.\n)\n\n# Functions etc...\n\n\n# R console\ntar_make()\n#&gt; + data dispatched\n#&gt; ✔ data completed [632ms, 4.37 kB]\n#&gt; + model dispatched\n#&gt; ✖ model errored\n#&gt; ✖ errored pipeline [804ms, 1 completed, 0 skipped]\n\n# We have a NULL placeholder for the model target.\ntar_read(model)\n#&gt; NULL\n\n# But it is not up to date.\ntar_outdated()\n#&gt; [1] \"model\"",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#error-messages",
    "href": "debugging.html#error-messages",
    "title": "4  Debugging pipelines",
    "section": "4.4 Error messages",
    "text": "4.4 Error messages\nStill, it is important to fix known errors. The metadata in _targets/meta/meta is a good place to start. It stores the most recent error and warning messages for each target. tar_meta() can retrieve these messages.1\n\n# R console\ntar_meta(fields = error, complete_only = TRUE)\n#&gt; # A tibble: 1 × 2\n#&gt;   name  error                   \n#&gt;   &lt;chr&gt; &lt;chr&gt;                   \n#&gt; 1 model missing values in object\n\nIt looks like missing values in the data are responsible for the error in the model target. Maybe this clue alone is enough to repair the code.2 If not, read on.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#debugging-in-functions",
    "href": "debugging.html#debugging-in-functions",
    "title": "4  Debugging pipelines",
    "section": "4.5 Debugging in functions",
    "text": "4.5 Debugging in functions\nMost errors are come from custom user-defined functions like simulate_data() and analyze_data(). See if you can reproduce the error in the R console.\n\n# R console\n\n# Restart your R session.\nrstudioapi::restartSession()\n\nlibrary(targets)\nlibrary(tarchetypes)\n\n# Loads globals like tar_option_set() packages, simulate_data(), and analyze_data():\ntar_load_globals()\n\n# Load the data that the target depends on.\ntar_load(data)\n\n# Run the command of the errored target.\nanalyze_data(data)\n#&gt; Error in `na.fail.default()`:\n#&gt;   missing values in object\n\nIf you see the same error here that you saw in the pipeline, then good! Now that you are in an interactive R session, all the usual debugging techniques and tools such as debug() and browser() can help you figure out how to fix your code, and you can exclude targets from the rest of the debugging process.\n\n# R console\ndebug(analyze_data)\nanalyze_data(data)\n#&gt; debugging in: analyze_data(data)\n#&gt; ...\nBrowse[2]&gt; anyNA(data$outcome) # Do I need to handle missing values?\n#&gt; [1] TRUE\n\nIn some cases, however, you may not see the original error:\n\n# R console\nnew_data &lt;- simulate_data(100)\nanalyze_data(new_data)\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic  p.value conf.low conf.high\n#&gt; * &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    0.595    0.152       3.92 1.05e- 4    0.297     0.892\n#&gt; 2 factor         0.362    0.0476      7.61 2.04e-13    0.269     0.455\n\nAbove, the random number generator seed in your local session is different from the seed assigned to the target in the pipeline. The dataset from the pipeline has missing values, whereas the one in the local session does not.\nIf you cannot reproduce the error in an interactive R session, read on.",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#pause-the-pipeline-with-browser",
    "href": "debugging.html#pause-the-pipeline-with-browser",
    "title": "4  Debugging pipelines",
    "section": "4.6 Pause the pipeline with browser()",
    "text": "4.6 Pause the pipeline with browser()\nSometimes, you may still need to run the pipeline to find the problem. The following trick lets you pause the pipeline and tinker with a running target interactively:\n\nInsert browser() into the function that produces the error.\nRestart your R session to remove detritus from memory.3\nCall tar_make() with callr_function = NULL, use_crew = FALSE, and as_job = FALSE to run the whole pipeline in your interactive session without launching a new callr::r() process, parallel crew workers, or an RStudio job.\nPoke around until you find the bug.\n\n\n# _targets.R\n# ...\nanalyze_data &lt;- function(data) {\n  browser() # Pause the pipeline here.\n  gls(\n    model = outcome ~ factor,\n    data = data,\n    correlation = corSymm(form = ~ measurement | unit),\n    weights = varIdent(form = ~ 1 | measurement)\n  ) %&gt;%\n    tidy(conf.int = TRUE, conf.level = 0.95)\n}\n# ...\n\n\n# R console\n\n# Restart your R session.\nrstudioapi::restartSession()\n\nlibrary(targets)\nlibrary(tarchetypes)\n\n# Run the whole pipeline in your interactive R session\n# (no callr process, parallel workers, or RStudio job).\ntar_make(callr_function = NULL, use_crew = FALSE, as_job = FALSE)\n#&gt; + model dispatched\n#&gt; Called from: analyze_data(dataset1)\n\n# Tinker with the R session to see if you can reproduce the error.\nBrowse[1]&gt; model &lt;- gls(\n+   model = outcome ~ factor,\n+   data = data,\n+   correlation = corSymm(form = ~ measurement | unit),\n+   weights = varIdent(form = ~ 1 | measurement)\n+ )\n#&gt; Error in `na.fail.default()`: \n#&gt;   missing values in object\n\n# Figure out what it would take to fix the error.\nBrowse[1]&gt; model &lt;- gls(\n+   model = outcome ~ factor,\n+   data = data,\n+   correlation = corSymm(form = ~ measurement | unit),\n+   weights = varIdent(form = ~ 1 | measurement),\n+   na.action = na.omit\n+ )\n\n# Confirm that the bug is fixed.\nBrowse[1]&gt; tidy(model, conf.int = TRUE, conf.level = 0.95)\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic       p.value conf.low conf.high\n#&gt; * &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    0.795    0.148       5.36 0.000000145      0.504     1.09 \n#&gt; 2 factor         0.275    0.0466      5.92 0.00000000717    0.184     0.367",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#pause-the-pipeline-with-the-debug-option",
    "href": "debugging.html#pause-the-pipeline-with-the-debug-option",
    "title": "4  Debugging pipelines",
    "section": "4.7 Pause the pipeline with the debug option",
    "text": "4.7 Pause the pipeline with the debug option\nIt may be too tedious to comb through all targets with browser(). For example, what if the pipeline has hundreds of simulated datasets? The following pipeline simulates 100 datasets with 58 experimental units each and 100 datasets with 70 experimental units each. Each dataset is analyzed with gls(). tar_map_rep() from the tarchetypes package organizes this simulation structure and batches the replications for computational efficiency.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\")\n)\n\nsimulate_data &lt;- function(units) {\n  options(warn = -1)\n  tibble(unit = seq_len(units), factor = rnorm(units, mean = 3)) %&gt;%\n    expand_grid(measurement = seq_len(4)) %&gt;%\n    mutate(outcome = sqrt(factor) + rnorm(n()))\n}\n\nanalyze_data &lt;- function(data) {\n  gls(\n    model = outcome ~ factor,\n    data = data,\n    correlation = corSymm(form = ~ measurement | unit),\n    weights = varIdent(form = ~ 1 | measurement)\n  ) %&gt;%\n    tidy(conf.int = TRUE, conf.level = 0.95)\n}\n\nsimulate_and_analyze_one_dataset &lt;- function(units) {\n  data &lt;- simulate_data(units)\n  analyze_data(data)\n}\n\nlist(\n  tar_map_rep( # from the {tarchetypes} package\n    name = analysis,\n    command = simulate_and_analyze_one_dataset(units),\n    values = data.frame(units = c(58, 70)), # 2 data size scenarios.\n    names = all_of(\"units\"), # The columns of values to use to name the targets.\n    batches = 20, # For each scenario, divide the 100 simulations into 20 dynamic branch targets.\n    reps = 5 # Each branch target (batch) runs simulate_and_analyze_one_dataset(n = 100) 5 times.\n  )\n)\n\n\n# R console\ntar_make()\n#&gt; + analysis_batch dispatched\n#&gt; ✔ analysis_batch completed [0ms, 97 B]\n#&gt; + analysis_58 declared [20 branches]\n#&gt; ✖ analysis_58_0ea05e90e3c60147 errored                      \n#&gt; ✖ errored pipeline [810ms, 2 completed, 0 skipped]\n#&gt; Error:\n#&gt; ! Error in tar_make():\n#&gt;   missing values in object\n#&gt;   See https://books.ropensci.org/targets/debugging.html\n\nRemember, if you just want to see the results that succeeded, run the pipeline with error = \"null\" in tar_option_set(). This temporary workaround is especially helpful with so many simulations.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\"),\n  error = \"null\"\n)\n\n# Functions etc...\n\n\n# R console\ntar_make()\n#&gt; + analysis_batch dispatched\n#&gt; ✔ analysis_batch completed [0ms, 97 B]\n#&gt; + analysis_58 declared [20 branches]\n#&gt; ✖ analysis_58_0ea05e90e3c60147 errored \n#&gt; ✖ analysis_58_6b77f315fc663bee errored \n#&gt; ✖ analysis_58_cef9803445fd2742 errored \n#&gt; ✖ analysis_58_66e82c295e9c8fb7 errored \n#&gt; ✖ analysis_58_62ba11e34d87b6d0 errored \n#&gt; ✖ analysis_58_0aa7ec9d3a482b98 errored \n#&gt; + analysis_70 declared [20 branches]  \n#&gt; ✖ analysis_70_0ea05e90e3c60147 errored \n#&gt; ✖ analysis_70_e693a7e1ba8177f8 errored  \n#&gt; ✖ analysis_70_cef9803445fd2742 errored  \n#&gt; ✖ analysis_70_66e82c295e9c8fb7 errored\n#&gt; ✖ analysis_70_6319f7f8f2676866 errored \n#&gt; ✖ analysis_70_ca6bd7e8ae4fc65f errored\n#&gt; ✖ analysis_70_62ba11e34d87b6d0 errored \n#&gt; ✖ analysis_70_1ae85d59f0c9cedf errored \n#&gt; ✖ analysis_70_d1f66671ad88fe8e errored \n#&gt; + analysis_58_combine dispatched   \n#&gt; ✔ analysis_58_combine completed [0ms, 7.26 kB] \n#&gt; + analysis_70_combine dispatched            \n#&gt; ✔ analysis_70_combine completed [0ms, 5.79 kB]  \n#&gt; + analysis dispatched                     \n#&gt; ✔ analysis completed [1ms, 12.69 kB]  \n#&gt; ✖ errored pipeline [11.8s, 29 completed, 0 skipped]\n\n# Read the simulations that succeeded.\ntar_read(analysis)\n#&gt; # A tibble: 250 × 12\n#&gt;    term        estimate std.error statistic  p.value conf.low conf.high units\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 (Intercept)    0.775    0.167       4.64 5.78e- 6   0.448      1.10     58\n#&gt;  2 factor         0.285    0.0525      5.43 1.42e- 7   0.182      0.388    58\n#&gt;  3 (Intercept)    0.892    0.161       5.53 8.75e- 8   0.576      1.21     58\n#&gt;  4 factor         0.262    0.0477      5.48 1.09e- 7   0.168      0.355    58\n#&gt;  5 (Intercept)    1.04     0.186       5.61 5.93e- 8   0.676      1.40     58\n#&gt;  6 factor         0.203    0.0607      3.34 9.72e- 4   0.0839     0.322    58\n#&gt;  7 (Intercept)    0.665    0.163       4.08 6.23e- 5   0.346      0.985    58\n#&gt;  8 factor         0.350    0.0508      6.88 5.75e-11   0.250      0.449    58\n#&gt;  9 (Intercept)    1.03     0.162       6.36 1.07e- 9   0.713      1.35     58\n#&gt; 10 factor         0.244    0.0514      4.74 3.77e- 6   0.143      0.345    58\n#&gt; # ℹ 240 more rows\n#&gt; # ℹ 4 more variables: tar_batch &lt;int&gt;, tar_rep &lt;int&gt;, tar_seed &lt;int&gt;,\n#&gt; #   tar_group &lt;int&gt;\n#&gt; # ℹ Use `print(n = ...)` to see more rows\n\nNow let’s seriously debug this pipeline. If each call to simulate_and_analyze_one_dataset() takes a long time to run, then the first step is to set one rep per batch in tar_map_rep() while keeping the total number of reps the same. In other words, increase batches from 20 to 100 and decrease reps from 5 to 1.4. Also remove the units = 70 scenario because we can reproduce the error without it.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\"),\n)\n\n# Functions...\n\nlist(\n  tar_map_rep(\n    name = analysis,\n    command = simulate_and_analyze_one_dataset(units),\n    values = data.frame(units = 58), # Remove the units = 70 scenario.\n    names = all_of(\"units\"),\n    batches = 100, # 100 batches now\n    reps = 1 # 1 rep per batch now\n  )\n)\n\n\n# R console\ntar_make()\n#&gt; + analysis_batch dispatched\n#&gt; ✔ analysis_batch completed [0ms, 97 B]\n#&gt; + analysis_58 declared [100 branches]\n#&gt; ✖ analysis_58_8edfc70f9a7feaf4 errored                       \n#&gt; ✖ errored pipeline [1.1s, 8 completed, 0 skipped]          \n#&gt; Error:\n#&gt; ! Error in tar_make():\n#&gt;   missing values in object\n#&gt;   See https://books.ropensci.org/targets/debugging.html\n\n8 targets ran successfully, and target analysis_58_8edfc70f9a7feaf4 hit an error. Let’s interactively debug analysis_58_8edfc70f9a7feaf4 without interfering with any other targets:\n\nSet the debug option to \"analysis_58_8edfc70f9a7feaf4\" in tar_option_set().\nOptional: set cue = tar_cue(mode = \"never\") in tar_option_set() to force skip all targets except:\n\nanalysis_58_8edfc70f9a7feaf4 and other targets in the debug option.\ntargets that do not already exist in the metadata.\ntargets that set their own cues.\n\nRestart your R session to remove detritus from memory.\nRun tar_make(callr_function = NULL, use_crew = FALSE, as_job = FALSE).\n\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\"),\n  debug = \"analysis_58_8edfc70f9a7feaf4\", # Set the target you want to debug.\n  cue = tar_cue(mode = \"never\") # Force skip non-debugging outdated targets.\n)\n\n# Functions etc...\n\n\n# R console\n\n# Restart your R session.\nrstudioapi::restartSession()\n\nlibrary(targets)\n\n# Run the pipeline in your interactive R session\n# (no callr process, no parallel crew workers, no RStudio job)\ntar_make(callr_function = NULL, use_crew = FALSE, as_job = FALSE)\n#&gt; + analysis_58 declared [100 branches]\n#&gt; → You are now running an interactive debugger in target analysis_58_8edfc70f9a7feaf4.\n#&gt; You can enter code and print objects as with the normal R console.\n#&gt; How to use: https://adv-r.hadley.nz/debugging.html#browser\n#&gt; \n#&gt; → The debugger is poised to run the command of target analysis_58_8edfc70f9a7feaf4:\n#&gt; \n#&gt;     tarchetypes::tar_rep_run(command = tarchetypes::tar_append_static_values(object = simulate_and_analyze_one_dataset(58), \n#&gt;     values = list(units = 58)), batch = analysis_batch, reps = 1, \n#&gt;     rep_workers = 1L, iteration = \"vector\")\n#&gt;\n#&gt; → Tip: run debug(your_function) and then enter \"c\"          \n#&gt; to move the debugger inside your_function(),   [7ms, 0+, 1-]\n#&gt; where your_function() is called from the command of target\n#&gt; analysis_58_8edfc70f9a7feaf4.\n#&gt; Then debug the function as you would normally (without `targets`).\n#&gt; Called from: eval(expr = expr, envir = envir)\nBrowse[1]&gt;\n\nAt this point, we are in an interactive debugger again. Only this time, we quickly skipped straight to the target we want to debug. We can follow the advice in the prompt above, or we can tinker in other ways.\n\n# R console\n# Jump to the function we want to debug.\nBrowse[1]&gt; debug(analyze_data)\nBrowse[1]&gt; c # Continue to the next breakpoint.\n#&gt; debugging in: analyze_data(data)\n\n# Tinker with the R session to see if you can reproduce the error.\nBrowse[2]&gt; model &lt;- gls(\n+   model = outcome ~ factor,\n+   data = data,\n+   correlation = corSymm(form = ~ measurement | unit),\n+   weights = varIdent(form = ~ 1 | measurement)\n+ )\n#&gt; Error in na.fail.default(): \n#&gt;   missing values in object\n\n# Figure out what it would take to fix the error.\nBrowse[1]&gt;  model &lt;- gls(\n+   model = outcome ~ factor,\n+   data = data,\n+   correlation = corSymm(form = ~ measurement | unit),\n+   weights = varIdent(form = ~ 1 | measurement),\n+   na.action = na.omit\n+ )\n\n# Confirm that the bug is fixed.\nBrowse[1]&gt; tidy(model, conf.int = TRUE, conf.level = 0.95)\n#&gt; # A tibble: 2 × 7\n#&gt;   term        estimate std.error statistic   p.value conf.low conf.high\n#&gt; * &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    0.925    0.205       4.51 0.0000103    0.523     1.33 \n#&gt; 2 factor         0.279    0.0646      4.32 0.0000230    0.153     0.406",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#workspaces",
    "href": "debugging.html#workspaces",
    "title": "4  Debugging pipelines",
    "section": "4.8 Workspaces",
    "text": "4.8 Workspaces\nA workspace is a special file that helps locally reconstruct the environment of a target outside the pipeline. By default, every target that throws an error generates a workspace.56\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(\n  packages = c(\"broom\", \"broom.mixed\", \"dplyr\", \"nlme\", \"tibble\", \"tidyr\")\n)\n\nsimulate_data &lt;- function(units) {\n  options(warn = -1)\n  tibble(unit = seq_len(units), factor = rnorm(units, mean = 3)) %&gt;%\n    expand_grid(measurement = seq_len(4)) %&gt;%\n    mutate(outcome = sqrt(factor) + rnorm(n()))\n}\n\nanalyze_data &lt;- function(data) {\n  gls(\n    model = outcome ~ factor,\n    data = data,\n    correlation = corSymm(form = ~ measurement | unit),\n    weights = varIdent(form = ~ 1 | measurement)\n  ) %&gt;%\n    tidy(conf.int = TRUE, conf.level = 0.95)\n}\n\nlist(\n  tar_target(rep, seq_len(100)),\n  tar_target(data, simulate_data(100), pattern = map(rep)),\n  tar_target(analysis, analyze_data(data), pattern = map(data))\n)\n\n\n# R console\ntar_make()\n#&gt; + rep dispatched\n#&gt; ✔ rep completed [145ms, 97 B]\n#&gt; + data declared [100 branches]\n#&gt; ✔ data completed [410ms, 442.17 kB]                           \n#&gt; + analysis declared [100 branches]                            \n#&gt; ✖ analysis_9f60c6e05a6c5414 errored                            \n#&gt; ✖ errored pipeline [1.2s, 101 completed, 0 skipped]            \n#&gt; Error:\n#&gt; ! Error in tar_make():\n#&gt;   missing values in object\n#&gt;   See https://books.ropensci.org/targets/debugging.html\n\nWhat went wrong with target analysis_9f60c6e05a6c5414? To find out, we load the workspace in an interactive session.\n\n# R console\n# List the available workspaces.\ntar_workspaces()\n#&gt; [1] \"analysis_9f60c6e05a6c5414\"\n\n# Load the workspace.\ntar_workspace(analysis_9f60c6e05a6c5414)\n\nAt this point, the global objects, functions, and upstream dependencies of target analysis_9f60c6e05a6c5414 are in memory. In addition, the target’s original random number generator seed is set.7\n\n# R console\nls()\n#&gt; [1] \"analyze_data\"  \"data\"          \"simulate_data\"\n\nWith the data and functions in hand, you can reproduce the error locally.\n\n# R console\nanalyze_data(data)\n#&gt; Error in na.fail.default():\n#&gt;   missing values in object\n\nFor more assistance, you can load the [traceback] from the workspace file.\n\ntar_traceback(analysis_9f60c6e05a6c5414)\n#&gt;  [1] \"analyze_data(data)\"                                                           \n#&gt;  [2] \"gls(model = outcome ~ factor, data = data, correlation = corSymm(form = ...\"  \n#&gt;  [3] \"tidy(., conf.int = TRUE, conf.level = 0.95)\"                                  \n#&gt;  [4] \"gls(model = outcome ~ factor, data = data, correlation = corSymm(form = ...\"  \n#&gt;  [5] \"do.call(model.frame, mfArgs)\"                                                 \n#&gt;  [6] \"(function (formula, ...)  UseMethod(\\\"model.frame\\\"))(formula = ~measureme...\"\n#&gt;  [7] \"model.frame.default(formula = ~measurement + unit + outcome +      facto...\"  \n#&gt;  [8] \"(function (object, ...)  UseMethod(\\\"na.fail\\\"))(structure(list(measuremen...\"\n#&gt;  [9] \"na.fail.default(structure(list(measurement = c(1L, 2L, 3L, 4L,  1L, 2L, ...\"  \n#&gt; [10] \"stop(\\\"missing values in object\\\")\"                                           \n#&gt; [11] \".handleSimpleError(function (condition)  {     state$error &lt;- build_mess...\"  \n#&gt; [12] \"h(simpleError(msg, call))\"",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "debugging.html#footnotes",
    "href": "debugging.html#footnotes",
    "title": "4  Debugging pipelines",
    "section": "",
    "text": "tar_meta(fields = warnings, complete_only = TRUE) retrieves warnings.↩︎\nYou can fix the bug by either removing the missing values from the dataset or by setting na.action = na.omit in gls().↩︎\nWith callr_function = NULL, a messy local R environment can accidentally change the functions and objects that a target depends on, which can invalidate those targets and erase hard-earned results that were previously correct. This is why targets uses callr in the first place, and it is why callr_function = NULL is for debugging only. If you do need callr_function = NULL, please restart your R session first.↩︎\nIn tarchetypes version 0.7.1.9000 and above, this re-batching will not change the random number generator seed assigned to each call to simulate_and_analyze_one_dataset().↩︎\nSee the workspace_on_error argument of tar_option_set().↩︎\ntar_option_set() also has a workspaces argument to let you choose which targets save workspace files, regardless of whether they hit errors.↩︎\nYou can retrieve this seed with tar_meta(names = analysis_9f60c6e05a6c5414, fields = seed). In the pipeline, targets sets this seed with withr::with_seed() just before running the target. However, other functions or target factories may set their own seeds. For example, tarchetypes::tar_map_rep() sets its own target seeds so they are resilient to re-batching. For more details on seeds, see the documentation of the seed argument of tar_option_set().↩︎",
    "crumbs": [
      "How to use {targets}",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Debugging pipelines</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "5  Functions",
    "section": "",
    "text": "5.1 Problems with script-based workflows\ntargets expects users to adopt a function-oriented style of programming. User-defined R functions are essential to express the complexities of data generation, analysis, and reporting. This chapter explains what makes functions useful and how to leverage them in your pipelines. It is based on the example from the walkthrough chapter.\nTraditional data analysis projects consist of imperative scripts, often with with numeric prefixes.\nTo run the project, the user runs each of the scripts in order.\nsource(\"01-data.R\")\nsource(\"02-model.R\")\nsource(\"03-plot.R\")\nEach script executes a different part of the workflow.\n# 01-data.R\nlibrary(tidyverse)\ndata &lt;- \"data.csv\" %&gt;%\n  read_csv(col_types = cols()) %&gt;%\n  filter(!is.na(Ozone))\nwrite_csv(data, \"data.rds\")\n# 02-model.R\nlibrary(biglm)\nlibrary(tidyverse)\ndata &lt;- read_rds(\"data.rds\", col_types = cols())\nmodel &lt;- lm(Ozone ~ Temp, data) %&gt;%\n  coefficients()\nsaveRDS(model, \"model.rds\")\n# 03-plot.R\nlibrary(tidyverse)\nmodel &lt;- readRDS(\"model.rds\")\ndata &lt;- readRDS(\"data.rds\")\nplot &lt;- ggplot(data) +\n  geom_point(aes(x = Temp, y = Ozone)) +\n  geom_abline(intercept = model[1], slope = model[2]) +\n  theme_gray(24)\nggsave(\"plot.png\", plot)\nAlthough this approach may feel convenient at first, it scales poorly for medium-sized workflows. These imperative scripts are monolithic, and they grow too large and complicated to understand or maintain.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#problems-with-script-based-workflows",
    "href": "functions.html#problems-with-script-based-workflows",
    "title": "5  Functions",
    "section": "",
    "text": "01-data.R\n02-model.R\n03-plot.R",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#functions",
    "href": "functions.html#functions",
    "title": "5  Functions",
    "section": "5.2 Functions",
    "text": "5.2 Functions\nFunctions are the building blocks of most computer code. They make code easier to think about, and they break down complicated ideas into small manageable pieces. Out of context, you can develop and test a function in isolation without mentally juggling the rest of the project. In the context of the whole workflow, functions are convenient shorthand to make your work easier to read.\nIn addition, functions are a nice mental model to express data science. A data analysis workflow is a sequence of transformations: datasets map to analyses, and analyses map to summaries. In fact, a function for data science typically falls into one of three categories:\n\nProcess a dataset.\nAnalyze a dataset.\nSummarize an analysis.\n\nThe example from the walkthrough chapter is a simple instance of this structure.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#writing-functions",
    "href": "functions.html#writing-functions",
    "title": "5  Functions",
    "section": "5.3 Writing functions",
    "text": "5.3 Writing functions\nLet us begin with our imperative code for data processing. Every time you look at it, you need to read it carefully and relearn what it does. And test it, you need to copy the entire block into the R console.\n\ndata &lt;- \"data.csv\" %&gt;%\n  read_csv(col_types = cols()) %&gt;%\n  filter(!is.na(Ozone))\n\nIt is better to encapsulate this code in a function.\n\nget_data &lt;- function(file) {\n  read_csv(file, col_types = cols()) %&gt;%\n    as_tibble() %&gt;%\n    filter(!is.na(Ozone))\n}\n\nNow, instead of invoking a whole block of text, all you need to do is type a small reusable command. The function name speaks for itself, so you can recall what it does without having to mentally process all the details again.\n\nget_data(\"data.csv\")\n\nAs with the data, we can write a function to fit a model,\n\nfit_model &lt;- function(data) {\n  lm(Ozone ~ Temp, data) %&gt;%\n    coefficients()\n}\n\nand another function to plot the model against the data.\n\nplot_model &lt;- function(model, data) {\n  ggplot(data) +\n    geom_point(aes(x = Temp, y = Ozone)) +\n    geom_abline(intercept = model[1], slope = model[2]) +\n    theme_gray(24)\n}",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#functions-in-pipelines",
    "href": "functions.html#functions-in-pipelines",
    "title": "5  Functions",
    "section": "5.4 Functions in pipelines",
    "text": "5.4 Functions in pipelines\nWithout those functions, our pipeline in the walkthrough chapter would look long, complicated, and difficult to digest.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = c(\"tibble\", \"readr\", \"dplyr\", \"ggplot2\"))\nlist(\n  tar_target(file, \"data.csv\", format = \"file\"),\n  tar_target(\n    data,\n    read_csv(file, col_types = cols()) %&gt;%\n      filter(!is.na(Ozone))\n  ),\n  tar_target(\n    model,\n    lm(Ozone ~ Temp, data) %&gt;%\n      coefficients()\n  ),\n  tar_target(\n    plot,\n    ggplot(data) +\n      geom_point(aes(x = Temp, y = Ozone)) +\n      geom_abline(intercept = model[1], slope = model[2]) +\n      theme_gray(24)\n  )\n)\n\nBut if we write our functions in R/functions.R and source() them into the target script file (default: _targets.R) the pipeline becomes much easier to read. We can even condense out raw_data and data targets together without creating a large command.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = c(\"tibble\", \"readr\", \"dplyr\", \"ggplot2\"))\nlist(\n  tar_target(file, \"data.csv\", format = \"file\"),\n  tar_target(data, get_data(file)),\n  tar_target(model, fit_model(data)),\n  tar_target(plot, plot_model(model, data))\n)",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#tracking-changes",
    "href": "functions.html#tracking-changes",
    "title": "5  Functions",
    "section": "5.5 Tracking changes",
    "text": "5.5 Tracking changes\nTo help figure out which targets to rerun and which ones to skip, the targets package tracks changes to the functions you define. To track changes to a function, targets computes a hash. This hash fingerprints the deparsed function (body and arguments) together with the hashes of all global functions and objects called that function. So if the function’s body, arguments, or dependencies change nontrivially, that change will be detected.\nThis hashing system is not perfect. For example, functions created by Rcpp::cppFunction() do not show the state of the underlying C++ code. As a workaround, you can use a wrapper that inserts the C++ code into the R function body so targets can track it for meaningful changes.\n\ncpp_function &lt;- function(code) {\n  out &lt;- Rcpp::cppFunction(code)\n  body(out) &lt;- rlang::call2(\"{\", code, body(out))\n  out\n}\n\nyour_function &lt;- cpp_function(\n  \"int your_function(int x, int y, int z) {\n     int sum = x + y + z;\n     return sum;\n   }\"\n)\n\nFunctions produced by Vectorize() and purrr::safely() suffer similar issues because the actual function code is in the closure of the function instead of the body. In addition, functions from packages are not automatically tracked, and extra steps documented in the packages chapter are required to enable this.\nIt is impossible to eliminate every edge case, so before running the pipeline, please run the dependency graph and other utilities to check your understanding of the state of project.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "targets.html",
    "href": "targets.html",
    "title": "6  Targets",
    "section": "",
    "text": "6.1 Target names\nA target is a high-level step of the computational pipeline, and a piece of work that you define with your custom functions. A target runs some R code and saves the returned R object to storage, usually a single file inside _targets/objects/.\nA target is an abstraction. The targets package automatically manages data storage and retrieval under the hood, which means you do not need to reference a target’s data file directly (e.g. _targets/objects/your_target_name). Instead, your R code should refer to a target name as if it were a variable in an R session. In other words, from the point of view of the user, a target is an R object in memory. That means a target name must be a valid visible symbol name for an R variable. The name must not begin with a dot, and it must be a string that lets you assign a value, e.g. your_target_name &lt;- TRUE. For stylistic considerations, please refer to the tidyverse style guide syntax chapter.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#what-a-target-should-do",
    "href": "targets.html#what-a-target-should-do",
    "title": "6  Targets",
    "section": "6.2 What a target should do",
    "text": "6.2 What a target should do\nLike a good function, a good target generally does one of three things:\n\nCreate a dataset.\nAnalyze a dataset with a model.\nSummarize an analysis or dataset.\n\nIf a function gets too long, you can split it into nested sub-functions that make your larger function easier to read and maintain.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#how-much-a-target-should-do",
    "href": "targets.html#how-much-a-target-should-do",
    "title": "6  Targets",
    "section": "6.3 How much a target should do",
    "text": "6.3 How much a target should do\nThe targets package automatically skips targets that are already up to date, so it is best to define targets that maximize time savings. Good targets usually\n\nAre large enough to subtract a decent amount of runtime when skipped.\nAre small enough that some targets can be skipped even if others need to run.\nInvoke no side effects such as modifications to the global environment. (But targets with tar_target(format = \"file\") can save files.)\nReturn a single value that is\n\nEasy to understand and introspect.\nMeaningful to the project.\nEasy to save as a file, e.g. with readRDS(). Please avoid non-exportable objects as target return values or global variables.\n\n\nRegarding the last point above, it is possible to customize the storage format of the target. For details, enter ?tar_target in the console and scroll down to the description of the format argument.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#working-with-tools-outside-r",
    "href": "targets.html#working-with-tools-outside-r",
    "title": "6  Targets",
    "section": "6.4 Working with tools outside R",
    "text": "6.4 Working with tools outside R\nEach target runs R code, so to invoke a tool outside R, consider system2() or processx to call the appropriate system commands. This technique allows you to run shell scripts, Python scripts, etc. from within R. External scripts should ideally be tracked as input files using tar_target(format = \"file\") as described in section on external input files. There are also specialized R packages to retrieve data from remote sources and invoke web APIs, including rnoaa, ots, and aws.s3, and you may wish to use custom cues to automatically invalidate a target when the upstream remote data changes.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#side-effects",
    "href": "targets.html#side-effects",
    "title": "6  Targets",
    "section": "6.5 Side effects",
    "text": "6.5 Side effects\nLike a good pure function, a good target should return a single value and not produce side effects. (The exception is output file targets which create files and return their paths.) Avoid modifying the global environment with calls to data() or source(). If you need to source scripts to define global objects, please do so at the top of your target script file (default: _targets.R) just like source(\"R/functions.R\") from the walkthrough vignette.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#dependencies",
    "href": "targets.html#dependencies",
    "title": "6  Targets",
    "section": "6.6 Dependencies",
    "text": "6.6 Dependencies\nConsider the following pipeline.\n\n# _targets.R file\nlibrary(targets)\nlibrary(tarchetypes)\n\nglobal_object &lt;- 3\n\ninner_function &lt;- function(argument) {\n  local_object &lt;- 1\n  argument + global_object + local_object + 2\n}\n\nouter_function &lt;- function(object) {\n  object + inner_function(object) + 1\n}\n\nlist(\n  tar_target(\n    name = second_target,\n    command = outer_function(first_target) + 2\n  ),\n  tar_target(\n    name = first_target,\n    command = 2\n  )\n)\n\nIn order to run properly, second_target needs up-to-date versions of first_target and outer_function(). In other words, first_target and outer_function() are dependencies of second_target. Likewise, inner_function() is a dependency of outer_function(), and global_object is a dependency of inner_function(). The targets package searches commands and functions for dependencies, noting global symbols like global_object and ignoring local symbols like argument and local_object. The tar_deps() function emulates behavior for you.1\n\ntar_deps(outer_function(first_target) + 2)\n#&gt; [1] \"+\"              \"first_target\"   \"outer_function\"\n\n\ntar_deps(\n  function(argument) {\n    local_object &lt;- 1\n    argument + global_object + local_object + 2\n  }\n)\n#&gt; [1] \"+\"             \"&lt;-\"            \"global_object\" \"{\"\n\nAfter it discards dangling symbols like { and &lt;-, targets translates the dependency information into a dependency graph that you can visualize with tar_visnetwork(). It is good practice to make sure this graph has the correct nodes connected with the correct edges.\n\n# R console\ntar_visnetwork()\n\n\n\n\n\n\n\nThe dependency graph is a directed acyclic graph (DAG) representation of the pipeline, where each node is a target or global object and each directed edge indicates where a downstream node depends on an upstream node. The DAG is not always a tree, but it never contains a cycle because no target is allowed to directly or indirectly depend on itself. The dependency graph should show a natural progression of work from left to right.2 targets uses static code analysis to build the graph, so the order of tar_target() calls in the target list does not matter. However, targets does not support self-referential loops or other cycles.\nWhen you run the pipeline with tar_make()3, targets runs the correct targets in the correct order with the correct resources according to the graph. For example, by the time second_target starts running, targets makes sure:\n\nDependency target first_target has already finished running.\nDependencies first_target and outer_function() are up to date.\nDependencies first_target and outer_function() are loaded into memory for second_target to use.\n\n\n# R console\ntar_make()\n#&gt; + first_target dispatched\n#&gt; ✔ first_target completed [1ms, 50 B]\n#&gt; + second_target dispatched\n#&gt; ✔ second_target completed [0ms, 51 B]\n#&gt; ✔ ended pipeline [144ms, 2 completed, 0 skipped]\n\nAt this point, any of the following changes will cause the next tar_make() to rerun second_target.\n\nChange the value of global_object.\nChange the body or arguments of inner_function().\nChange the body or arguments of outer_function().\nChange the command or value of first_target.\nChange the command of second_target.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#return-value",
    "href": "targets.html#return-value",
    "title": "6  Targets",
    "section": "6.7 Return value",
    "text": "6.7 Return value\nThe return value of a target should be an R object that can be saved to disk and hashed.\n\n6.7.1 Saving\nThe object should be compatible with the storage format you choose using the format argument of tar_target() or tar_option_set(). For example, if the format is \"rds\" (default), then the target should return an R object that can be saved with saveRDS() and safely loaded properly into another session. Please avoid returning non-exportable objects such as connection objects, Rcpp pointers, xgboost matrices, and greta models4.\n\n\n6.7.2 Hashing\nOnce a target is saved to disk, targets computes a hash to track changes to the data file(s). These hashes are used to decide whether each target is up to date or needs to rerun. In order for the hash to be useful, the data you return from a target must be an accurate reflection of the underlying content of the data. So please try to return the actual data instead of an object that wraps or points to the data. Otherwise, the package will make incorrect decisions regarding which targets can skip and which need to rerun.\n\n\n6.7.3 Workaround\nAs a workaround, you can write custom functions to create temporary instances of these non-exportable/non-hashable objects and clean them up after the task is done. The following sketch creates a target that returns a database table while managing a transient connection object.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\nget_from_database &lt;- function(table, ...) {\n  con &lt;- DBI::dbConnect(...)\n  on.exit(close(con))\n  dbReadTable(con, table)\n}\n\nlist(\n  tar_target(\n    table_from_database,\n    get_from_database(\"my_table\", ...), # ... has use-case-specific arguments.\n    format = \"feather\" # Requires that the return value is a data frame.\n  )\n)",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "targets.html#footnotes",
    "href": "targets.html#footnotes",
    "title": "6  Targets",
    "section": "",
    "text": "tar_deps() uses the findGlobals() function from the codetools package, with some minor adjustments. See https://adv-r.hadley.nz/expressions.html?q=ast#ast-funs for more information on static code analysis.↩︎\nIf you have hundreds of targets, then tar_visnetwork() may be slow. If that happens, consider temporarily commenting out some targets in _targets.R just for visualization purposes.↩︎\nor tar_make_clustermq() or tar_make_future()↩︎\nSpecial exceptions are granted to Keras and Torch models, which can be safely returned from targets if you specify format = \"keras\" or format = \"torch\".↩︎",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Targets</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "7  Packages",
    "section": "",
    "text": "7.1 Loading and configuring R packages\nThis chapter describes the recommended roles of R packages in targets pipelines and how to manage them in different situations.\nFor most pipelines, it is straightforward to load the R packages that your targets need in order to run. You can either:\n2. is often faster, especially for utilities like tar_visnetwork(), because it avoids loading packages unless absolutely necessary.\nSome package management workflows are more complicated. If your use special configuration with conflicted, box, import, or similar utility, please do your configuration inside a project-level .Rprofile file instead of the target script file (default: _targets.R). In addition, if you use distributed workers inside external containers (Docker, Singularity, AWS AMI, etc.) make sure each container has a copy of this same .Rprofile file where the R worker process spawns. This approach is ensures that all remote workers are configured the same way as the local main process.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#loading-and-configuring-r-packages",
    "href": "packages.html#loading-and-configuring-r-packages",
    "title": "7  Packages",
    "section": "",
    "text": "Call library() at the top of the target script file (default: _targets.R) to load each package the conventional way, or\nName the required packages using the packages argument of tar_option_set().",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#package-management-with-renv",
    "href": "packages.html#package-management-with-renv",
    "title": "7  Packages",
    "section": "7.2 Package management with renv",
    "text": "7.2 Package management with renv\ntargets and renv work extremely well together as an overall reproducibility solution for data analysis pipelines. targets makes sure your results are up to date, and renv keeps track of the packages you use.\nIf you use renv, then overhead from project initialization could slow down pipelines and workers. If you experience slowness, please make sure your renv library is on a fast file system. (For example, slow network drives can severely reduce performance.) In addition, you can disable the slowest initialization checks. After confirming at https://rstudio.github.io/renv/reference/config.html that you can safely disable these checks, you can write the following lines in your user-level .Renviron file:\nRENV_CONFIG_SANDBOX_ENABLED=false\nRENV_CONFIG_SYNCHRONIZED_CHECK=false\nIf you disable the synchronization check, remember to call renv::status() periodically to check the health of your renv project library.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#r-packages-as-projects",
    "href": "packages.html#r-packages-as-projects",
    "title": "7  Packages",
    "section": "7.3 R packages as projects",
    "text": "7.3 R packages as projects\nIt is good practice to organize the files of a targets project similar to a research compendium or R package. However, unless have a specific reason to do so, it is usually not necessary to literally implement your targets pipeline as an installable R package with its own DESCRIPTION file. A research compendium backed by a renv library and Git-backed version control is enough reproducibility for most projects.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#target-factories",
    "href": "packages.html#target-factories",
    "title": "7  Packages",
    "section": "7.4 Target Factories",
    "text": "7.4 Target Factories\nTo make specific targets pipelines reusable, it is usually better to create a package with specialized target factories tailored to your use case. Packages stantargets and jagstargets are examples, and you can find more information on the broader R Targetopia at https://wlandau.github.io/targetopia/.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#package-based-invalidation",
    "href": "packages.html#package-based-invalidation",
    "title": "7  Packages",
    "section": "7.5 Package-based invalidation",
    "text": "7.5 Package-based invalidation\nStill, it is sometimes desirable to treat functions and objects from a package as dependencies when it comes to deciding which targets to rerun and which targets to skip. targets does not track package functions by default because this is not a common need. Usually, local package libraries do not need to change very often, and it is best to maintain a reproducible project library using renv.\nHowever, if you are developing a package alongside a targets pipeline that uses it, you may wish to invalidate certain targets as you make changes to your package. For example, if you are working on a novel statistical method, it is good practice to implement the method itself as an R package and perform the computation for the research paper in one or more targets pipelines.\nTo track the contents of packages package1 and package2, you must\n\nFully install these packages with install.packages() or equivalent. devtools::load_all() is insufficient because it does not make the packages available to parallel workers.\nWrite the following in your target script file (default: _targets.R):\n\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(\n  packages = c(\"package1\", \"package2\", ...), # `...` is for other packages.\n  imports = c(\"package1\", \"package2\")\n)\nlist(\n  tar_target(name = output, command = function_from_package1())\n)\n\npackages = c(\"package1\", \"package2\", ...) tells targets to call library(package1), library(package2), etc. before running each target. imports = c(\"package1\", \"package2\") tells targets to dive into the environments of package1 and package2 and reproducibly track all the objects and datasets as if they were part of the global environment. For example, if you define a function function_from_package1() in package1, then you should see a function node for function_from_package1() in the graph produced by tar_visnetwork(targets_only = FALSE), and targets downstream of function_from_package1() will invalidate if you install an update to package1 with a new version of function_from_package1(). The next time you call tar_make(), those invalidated targets will automatically rerun.\nOne limitation is that entire namespaced calls like package1::function_from_package1() are not registered in the dependency graph because of the limitations of the static code analysis capabilities of targets (powered by codetools::findGlobals()). tar_target(name = output, command = function_from_package1()) has a dependency function_from_package1(), but tar_target(name = output, command = package1::function_from_package1()) does not have a dependency package1::function_from_package1(). This is because :: is treated as a function with arguments package and function_from_package1.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Packages</span>"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "8  Projects",
    "section": "",
    "text": "8.1 Extra reproducibility\nA project is a targets pipeline together with its supporting source code, data, and configuration settings. This chapter explains best practices when it comes to organizing and configuring targets projects.\nFor extra reproducibility, it is good practice to use the renv R package for package management and Git/GitHub for code version control. The entire _targets/ data store should generally not be committed to Git because of its large size.1 The broader R community has excellent resources and tutorials on getting started with these third-party tools.\nIf you use renv, then overhead from project initialization could slow down pipelines and workers. If you experience slowness, please make sure your renv library is on a fast file system. (For example, slow network drives can severely reduce performance.) In addition, you can disable the slowest initialization checks. After confirming at https://rstudio.github.io/renv/reference/config.html that you can safely disable these checks, you can write the following lines in your user-level .Renviron file:\nIf you disable the synchronization check, remember to call renv::status() periodically to check the health of your renv project library.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "projects.html#extra-reproducibility",
    "href": "projects.html#extra-reproducibility",
    "title": "8  Projects",
    "section": "",
    "text": "RENV_CONFIG_SANDBOX_ENABLED=false\nRENV_CONFIG_SYNCHRONIZED_CHECK=false",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "projects.html#project-files",
    "href": "projects.html#project-files",
    "title": "8  Projects",
    "section": "8.2 Project files",
    "text": "8.2 Project files\ntargets is mostly indifferent to how you organize the files in your project. However, it is good practice to follow the overall structure of a research compendium or R package (not necessarily with a DESCRIPTION file). It also is good practice to give each project its own unique folder with one targets pipeline, one renv library for package management, and one Git/GitHub repository for code version control. As described later, it is possible to create multiple overlapping projects within a single folder, but this is not recommended for most situations.\nThe walkthrough chapter shows the file structure for a minimal targets project. For more serious projects, the file system may expand to look something like this:\n\n├── .git/\n├── .Rprofile\n├── .Renviron\n├── renv/\n├── index.Rmd\n├── _targets/\n├── _targets.R\n├── _targets.yaml\n├── R/\n├──── functions_data.R\n├──── functions_analysis.R\n├──── functions_visualization.R\n├── data/\n└──── input_data.csv\n\nSome of these files are optional, and they have the following roles.\n\n.git/: a folder automatically created by Git for version control purposes.\n.Rprofile: a text file automatically created by renv to automatically load the project library when you start R at the project root folder. You may wish to add other global configuration here, e.g. declare package precedence using the conflicted package.\n.Renviron: a text file of key-value pairs defining project-level environment variables, e.g. API keys and package settings. See Sys.getenv() for more information on environment variables and how to work with them in R.\n\nindex.Rmd: Target Markdown report source file to define the pipeline.\n_targets/: the data store where tar_make() and similar functions write target storage and metadata when they run the pipeline.\n_targets.R: the target script file. All targets pipelines must have a target script file that returns a target list at the end. If you use Target Markdown (e.g. index.Rmd above) then the target script will be written automatically. Otherwise, you may write it by hand. Unless you apply the custom configuration described later in this chapter, the target script file will always be called _targets.R and live at the project root folder.\n_targets.yaml: a YAML file to set default arguments to critical functions like tar_make(). As described below, you can access and modify this file with functions tar_config_get(), tar_config_set(), and tar_config_unset(). targets will attempt to look for _targets.yaml unless you set a different path in the TAR_CONFIG environment variable.\nR/: directory of scripts containing custom user-defined R code. Most of the code will likely contain custom functions you write to support your targets. You can load these functions with source(\"R/function_script.R\") or eval(parse(text = \"R/function_script.R\"), either in a tar_globals = TRUE code chunk in Target Markdown or directly in _targets.R if you are not using Target Markdown.\ndata/: directory of local input data files. As described in the files chapter, it is good practice to track input files using format = \"file\" in tar_target() and then reference those file targets in downstream targets that directly depend on those files.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "projects.html#multiple-projects",
    "href": "projects.html#multiple-projects",
    "title": "8  Projects",
    "section": "8.3 Multiple projects",
    "text": "8.3 Multiple projects\nIt is generally good practice to give each project its own unique folder with one targets pipeline, one renv library for package management, and one Git/GitHub repository for code version control. However, sometimes it is reasonable to maintain multiple pipelines within a project: for example, if different pipelines have similar research goals and share the same code base of custom user-defined functions. This section explains how to maintain and navigate such a collection of overlapping projects.\nThe functionality below assumes you have targets version 0.7.0.9001 or higher, which you may need to install from GitHub.\n\nremotes::install_github(\"ropensci/targets\")\n\n\n8.3.1 Create each project.\nTo begin, write the shared code base of custom user-defined functions in R/, and write one targets pipeline per project. For convenience, we will directly write to the targets script files, but the principles generalize to Target Markdown. The file structure looks something like this:\n\n├── _targets.yaml\n├── script_a.R\n├── script_b.R\n├── R/\n├──── functions_data.R\n├──── functions_analysis.R\n├──── functions_visualization.R\n...\n\nAll projects share the same functions defined in the scripts in R/, and each project uses a different target script and data store. script_a.R defines the targets for project A.\n\n# script_a.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = \"tidyverse\")\nlist(\n  tar_target(target_abc, f(..)),\n  tar_target(tarbet_xyz, g(...))\n)\n\nLikewise, script_b.R defines the targets for project B.\n\n# script_b.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = \"tidyverse\")\nlist(\n  tar_target(target_123, f(...)),\n  tar_target(target_456, h(...))\n)\n\n\n\n8.3.2 Configure each project.\nTo establish a different store and script per project, write a top-level _targets.yaml configuration to specify these paths explicitly. You can do this from R with tar_config_set().\n\ntar_config_set(script = \"script_a.R\", store = \"store_a\", project = \"project_a\")\ntar_config_set(script = \"script_b.R\", store = \"store_b\", project = \"project_b\")\n\nThe R code above writes the following _targets.yaml configuration file.\nproject_a:\n  store: store_a\n  script: script_a.R\nproject_b:\n  store: store_b\n  script: script_b.R\n\n\n8.3.3 Run each project\nTo run each project, run tar_make() with the correct target script and data store. To select the correct script and store, set the TAR_PROJECT environment variable to the correct project name. that way, tar_config_get() automatically supplies the correct script and store arguments to tar_make().\n\nSys.setenv(TAR_PROJECT = \"project_a\")\ntar_make()\ntar_read(target_abc)\nSys.setenv(TAR_PROJECT = \"project_b\")\ntar_make()\ntar_read(target_123)\n\nAlternatively, you can manually select the appropriate script and store for each project. This is a less convenient approach, but if you do it, you do not need to set the TAR_PROJECT environment variable or rely on _targets.yaml.\n\ntar_make(script = \"script_a.R\", store = \"store_a\")\ntar_read(target_abc, store = \"store_a\")\ntar_make(script = \"script_b.R\", store = \"store_b\")\ntar_read(target_abc, store = \"store_b\")",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "projects.html#interdependent-projects",
    "href": "projects.html#interdependent-projects",
    "title": "8  Projects",
    "section": "8.4 Interdependent projects",
    "text": "8.4 Interdependent projects\n\n8.4.1 Config inheritance\n_targets.yaml can control more than just the script and store, and different projects can inherit settings from one another. In the following example, project B inherits from project A, so projects A and B both set reporter = \"summary\" and shorcut = TRUE by default in tar_make().\n\ntar_config_set(\n  script = \"script_a.R\",\n  store = \"store_a\",\n  reporter_make = \"summary\",\n  shortcut = TRUE,\n  project = \"project_a\"\n)\ntar_config_set(\n  script = \"script_b.R\",\n  store = \"store_b\",\n  inherits = \"project_a\",\n  project = \"project_b\",\n)\n\n\nwriteLines(readLines(\"_targets.yaml\"))\n#&gt; project_a:\n#&gt;   reporter_make: balanced\n#&gt;   script: script_a.R\n#&gt;   shortcut: yes\n#&gt;   store: store_a\n#&gt; project_b:\n#&gt;   inherits: project_a\n#&gt;   script: script_b.R\n#&gt;   store: store_b\nSys.setenv(TAR_PROJECT = \"project_b\")\ntar_config_get(\"script\")\n#&gt; [1] \"script_b.R\"\ntar_config_get(\"reporter_make\")\n#&gt; [1] \"balanced\"\ntar_config_get(\"shortcut\")\n#&gt; [1] TRUE\n\n\n\n8.4.2 Sharing targets\nFor some workflows, the output of one project serves as the input to another project. The easiest way to set this up is through global objects. The first project remains unchanged, and the second project reads from the first before the pipeline begins.\n\n# script_b.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = \"tidyverse\")\n\nobject_from_project_a &lt;- tar_read(target_from_project_a, store = \"store_a\")\n\nlist(\n  tar_target(new_target, some_function(object_from_project_a)),\n  ...\n)\n\nThis approach is the most convenient and versatile, but it can be inefficient if target_from_project_a is large. A higher-performant solution for large data is to treat the file in project A’s data store as an input file target in project B. This second approach requires an understanding of the data store and an awareness of which targets are stored locally and which are stored on the cloud. For a target with repository = \"local\", you can begin from the file store_a/objects/target_from_project_a. Otherwise, the target’s file exists on the cloud (AWS or GCP) and you may need to access the target as a URL in a target with format = \"url\".\n\n# script_b.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_source()\ntar_option_set(packages = \"tidyverse\")\nlist(\n  tar_target(file_from_project_a, \"store_a/objects/target_name\", format = \"file\"),\n  tar_target(data_from_project_a, readRDS(file_from_project_a)), # Assumes format = \"rds\" in project A.\n  tar_target(new_target, analyze_data(data_from_project_a)),\n  ...\n)",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "projects.html#the-config-package",
    "href": "projects.html#the-config-package",
    "title": "8  Projects",
    "section": "8.5 The config package",
    "text": "8.5 The config package\nThe _targets.yaml config interface borrows heavily from the ideas in the config R package. However, it does not actually use the config package, nor does it copy or use the config source code in any way. And there are major differences in user-side behavior:\n\nThere is no requirement to have a configuration (i.e. project) named “default”.\nThe default project is called “main”, and other projects do not inherit from it automatically.\nNot all fields need to be populated in _targets.yaml because the targets package already has system defaults.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "projects.html#footnotes",
    "href": "projects.html#footnotes",
    "title": "8  Projects",
    "section": "",
    "text": "However, you may wish to commit _targets/meta/meta, which is critical to checking the status of each target and reading targets into memory.↩︎",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Projects</span>"
    ]
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "9  Pseudo-random numbers",
    "section": "",
    "text": "9.1 Overview\nThis chapter explains the behaviors, limitations, and trade-offs of targets with respect to pseudo-random number generation. It assumes basic familiarity with pseudo-random numbers, especially base R functions like sample() and set.seed().\nA targets pipeline may run stochastic methods, and targets tries to ensure that the results are repeatable and correct. There are two major statistical challenges:",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pseudo-random numbers</span>"
    ]
  },
  {
    "objectID": "random.html#overview",
    "href": "random.html#overview",
    "title": "9  Pseudo-random numbers",
    "section": "",
    "text": "Reproducibility: different runs of the same pipeline should produce the same results, even if a target runs a stochastic function like rnorm().\nIndependence: pseudo-random numbers should behave like independent random samples. Pseudo-random number sequences in different targets should overlap as little as possible.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pseudo-random numbers</span>"
    ]
  },
  {
    "objectID": "random.html#reproducibility",
    "href": "random.html#reproducibility",
    "title": "9  Pseudo-random numbers",
    "section": "9.2 Reproducibility",
    "text": "9.2 Reproducibility\nEach target runs with its own deterministic seed. The target seed is a function of:\n\nIts name, and\nThe pipeline-level seed from tar_option_get(\"seed\").\n\nConsider the following simple pipeline.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(seed = 3)\nlist(\n  tar_target(name = target_1, command = runif(n = 1)),\n  tar_target(name = target_2, command = runif(n = 1)),\n  tar_target(name = target_3, command = runif(n = 1))\n)\n\nThe seed of the target_1 target is:\n\nexpected_seed &lt;- tar_seed_create(\"target_1\", global_seed = 3)\n\nexpected_seed\n#&gt; [1] 1432713270\n\nAnd the target runs the equivalent of:\n\nwithr::with_seed(seed = expected_seed, code = runif(n = 1))\n#&gt; [1] 0.1995242\n\nWe can run the pipeline with tar_make(), view the seed with tar_meta(), and view the result with tar_read().1\n\ntar_make()\n#&gt; + target_1 dispatched\n#&gt; ✔ target_1 completed [0ms, 55 B]\n#&gt; + target_2 dispatched\n#&gt; ✔ target_2 completed [1ms, 55 B]\n#&gt; + target_3 dispatched\n#&gt; ✔ target_3 completed [1ms, 55 B]\n#&gt; ✔ ended pipeline [147ms, 3 completed, 0 skipped]\n\ntar_meta(names = any_of(\"target_1\"), fields = any_of(\"seed\"))\n#&gt; # A tibble: 1 × 2\n#&gt;   name           seed\n#&gt;   &lt;chr&gt;         &lt;int&gt;\n#&gt; 1 target_1 1432713270\n\ntar_read(target_1)\n#&gt; [1] 0.1995242\n\nThe seed argument of tar_option_set() offers flexibility:\n\nIf you set seed to a different integer, you will get a different (but still reproducible) set of stochastic results.\nIf you set seed to NA, then targets will not set a seed at all. Different runs of the pipeline will produce results, and those results will not be reproducible.\n\nFor (2), each target will always appear outdated in tar_make() and tar_outdated(). To force a target to be up to date, set cue = tar_cue(seed = FALSE) in tar_target() or tar_option_set().",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pseudo-random numbers</span>"
    ]
  },
  {
    "objectID": "random.html#independence",
    "href": "random.html#independence",
    "title": "9  Pseudo-random numbers",
    "section": "9.3 Independence",
    "text": "9.3 Independence\nWithin a pipeline, different targets are guaranteed to have different names. Barring the vanishingly small chance of hash collisions in tar_seed_create(), that means they should also have different seeds.\n\ntar_meta(targets_only = TRUE, fields = any_of(\"seed\"))\n#&gt; # A tibble: 3 × 2\n#&gt;   name           seed\n#&gt;   &lt;chr&gt;         &lt;int&gt;\n#&gt; 1 target_1 1432713270\n#&gt; 2 target_2 -740972651\n#&gt; 3 target_3  227569066\n\nThus, different targets should have non-identical sequences of pseudo-random numbers.\n\ntar_read(target_1)\n#&gt; [1] 0.1995242\ntar_read(target_2)\n#&gt; [1] 0.2481659\ntar_read(target_3)\n#&gt; [1] 0.5339015\n\nIn theory, these parallel random number generator streams could overlap and produce statistically correlated results. However, the risk is extremely small in practice. See https://docs.ropensci.org/targets/reference/tar_seed_create.html#rng-overlap for details, references, and justification.",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pseudo-random numbers</span>"
    ]
  },
  {
    "objectID": "random.html#tarchetypes",
    "href": "random.html#tarchetypes",
    "title": "9  Pseudo-random numbers",
    "section": "9.4 tarchetypes",
    "text": "9.4 tarchetypes\nMany target factories in the tarchetypes package support batched replication:\n\ntar_rep()\ntar_map_rep()\ntar_map2_count()\ntar_map2_size()\ntar_quarto_rep()\ntar_render_rep()\n\nIn batched replication, each target is a batch that runs multiple replications of a stochastic task. If you change the number of batches or number of replications per batch, the target name changes, which changes the seed of each target. To make pipelines more resilient, tar_rep() and friends set their own unique deterministic seeds from tar_seed_create() based on:\n\ntar_option_get(\"seed\").\nThe parent name of the dynamnic target\nThe index of each replicate in the sequence.\n\nIf you return data frames or lists, those seeds are available in the tar_seed element of the output. Each replicate gets its own seed, and the default seeds from tar_meta() no longer apply.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(seed = 3)\nlist(\n  tar_rep(\n    name = tasks,\n    command = runif(n = 1),\n    batches = 2,\n    reps = 3\n  )\n)\n\n\ntar_make()\n#&gt; + tasks_batch dispatched\n#&gt; ✔ tasks_batch completed [0ms, 98 B]\n#&gt; + tasks declared [2 branches]\n#&gt; ✔ tasks completed [12ms, 433 B]\n#&gt; ✔ ended pipeline [162ms, 3 completed, 0 skipped]\n\ntar_read(tasks)\n#&gt; # A tibble: 6 × 4\n#&gt;   result tar_batch tar_rep   tar_seed\n#&gt;    &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;      &lt;int&gt;\n#&gt; 1  0.882         1       1 1161495390\n#&gt; 2  0.781         1       2 1040766653\n#&gt; 3  0.213         1       3  942098819\n#&gt; 4  0.913         2       1 -720434756\n#&gt; 5  0.545         2       2 1717229114\n#&gt; 6  0.298         2       3 -115675171\n\nIf you change the batching structure, the tar_rep and tar_batch columns will change, but the results and the seeds will stay the same.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(seed = 3)\nlist(\n  tar_rep(\n    name = tasks,\n    command = runif(n = 1),\n    batches = 3, # previously 2\n    reps = 2     # previously 3\n  )\n)\n\n\ntar_make()\n#&gt; + tasks_batch dispatched\n#&gt; ✔ tasks_batch completed [1ms, 99 B]\n#&gt; + tasks declared [3 branches]\n#&gt; ✔ tasks completed [12ms, 601 B]\n#&gt; ✔ ended pipeline [163ms, 4 completed, 0 skipped]\n\ntar_read(tasks)\n#&gt; # A tibble: 6 × 4\n#&gt;   result tar_batch tar_rep   tar_seed\n#&gt;    &lt;dbl&gt;     &lt;int&gt;   &lt;int&gt;      &lt;int&gt;\n#&gt; 1  0.882         1       1 1161495390\n#&gt; 2  0.781         1       2 1040766653\n#&gt; 3  0.213         2       1  942098819\n#&gt; 4  0.913         2       2 -720434756\n#&gt; 5  0.545         3       1 1717229114\n#&gt; 6  0.298         3       2 -115675171",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pseudo-random numbers</span>"
    ]
  },
  {
    "objectID": "random.html#footnotes",
    "href": "random.html#footnotes",
    "title": "9  Pseudo-random numbers",
    "section": "",
    "text": "tar_make() does not interfere with the pseudo-random number generator state of the calling R process.↩︎",
    "crumbs": [
      "Concepts and best practices",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pseudo-random numbers</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "10  Local data",
    "section": "",
    "text": "10.1 Memory\nDuring a pipeline, targets manages R objects in memory and writes them to files on disk. It also stores target-level metadata in a compact central text file.\nIn addition to persistent storage on disk, targets uses random access memory (RAM) while the pipeline is running. Each target loads its upstream dependencies into memory and returns an R object in memory. After a target runs or loads, tar_make() either keeps the object in memory or discards it, depending on the settings in tar_target() and tar_option_set(). Set memory = \"transient\" to release the target whenever possible. Alternatively, set memory = \"persistent\" to keep the object in memory and reduce costly interactions with the file system. The trade-off between memory and file I/O depends on your computing platform. See the performance chapter for more details.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Local data</span>"
    ]
  },
  {
    "objectID": "data.html#local-data-store",
    "href": "data.html#local-data-store",
    "title": "10  Local data",
    "section": "10.2 Local data store",
    "text": "10.2 Local data store\nIn addition to memory, the pipeline writes data to files on disk. tar_make() creates a special data folder called _targets/ at the root of your project.\n\nfs::dir_tree(\"_targets\")\n_targets\n├── meta\n│   ├── crew\n│   ├── meta\n│   ├── process\n│   └── progress\n├── objects\n│   ├── target1\n│   ├── target2\n│   ├── dynamic_branch_c7bcb4bd\n│   ├── dynamic_branch_285fb6a9\n│   └── dynamic_branch_874ca381\n├── scratch # tar_make() deletes this folder after it finishes.\n└── user # for gittargets user data\n\nThe two most important parts are:\n\n_targets/meta/meta: a text file with key target-level metadata.\n_targets/objects/: a folder with the output data of each target.\n\nConsider this pipeline:\n\nlibrary(targets)\nlibrary(tarchetypes)\nlist(\n  tar_target(\n    name = target1,\n    command = 11 + 46,\n    format = \"rds\",\n    repository = \"local\"\n  )\n)\n\ntar_make() does the following:\n\nRun the command of target1 and observe a return value of 57.\nSave the value 57 to _targets/objects/target1 using saveRDS().\nAppend a line to _targets/meta/meta containing the hash, time stamp, file size, warnings, errors, and execution time of target1.\nAppend a line to _targets/meta/progress to indicate that target1 finished.\n\nRemarks:\n\nTo read the value of target1 back into R, tar_read(target1) is much better than readRDS(\"_targets/objects/target1\").\nThe format argument of tar_target() controls how tar_make() saves the return value. The default is \"rds\", and there are more efficient formats such as \"qs\" and \"feather\". Some of these formats require external packages. See https://docs.ropensci.org/targets/reference/tar_target.html#storage-formats for details.\nFor efficiency, tar_make() does not write to _targets/meta/meta or _targets/meta/progress every single time a target completes. Instead, it waits and gathers a backlog of text lines in memory, then writes whole batches of lines at a time. This behavior risks losing metadata in the event of a crash, but it reduces costly interactions with the file system. The seconds_meta argument controls how often tar_make() writes metadata. seconds_reporter does the same for messages printed to the R console.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Local data</span>"
    ]
  },
  {
    "objectID": "data.html#external-files",
    "href": "data.html#external-files",
    "title": "10  Local data",
    "section": "10.3 External files",
    "text": "10.3 External files\nSome pipelines work with custom external files outside _targets/. The user is still responsible for reading and writing these files. However, the pipeline can track them, detect changes, and decide whether to rerun or skip the targets that the files depend on. Simply create a file target.\nIn a file target,\n\ntar_target() has format = \"file\".\nThe command returns a character vector of file paths.\n\nConsider this pipeline:\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\n\ncreate_output &lt;- function(file) {\n  data &lt;- read.csv(file)\n  output &lt;- head(data)\n  write.csv(output, \"output.csv\")\n  \"output.csv\"\n}\n\nlist(\n  tar_target(name = input, command = \"data.csv\", format = \"file\"),\n  tar_target(name = output, command = create_output(input), format = \"file\")\n)\n\nIn the dependency graph, output depends on input because the command of output mentions the symbol input.\n\ntar_visnetwork()\n\n\n\n\n\n\n\nBefore the pipeline first runs, data.csv exists, but output.csv does not. During tar_make(), the input target tracks data.csv, and the output target creates and tracks output.csv. If data.csv changes before the next tar_make(), then both input and output rerun. If something outside the pipeline changes output.csv, then output reruns.\nRemarks:\n\nA file target can have both input and output files.\nA file target can include directory paths as well as individual file paths.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Local data</span>"
    ]
  },
  {
    "objectID": "data.html#clean-up-local-files",
    "href": "data.html#clean-up-local-files",
    "title": "10  Local data",
    "section": "10.4 Clean up local files",
    "text": "10.4 Clean up local files\nThere are multiple functions to remove or clean up local target storage. Some of them also delete cloud data if your pipeline uses an AWS or GCP bucket (see the next chapter).\n\ntar_destroy() removes _targets/ data store and any cloud data from the pipeline.\ntar_prune() deletes data and metadata irrelevant to the current pipeline in _targets.R.\ntar_delete() deletes specific data files from _targets/objects/ and the cloud. It does not modify metadata.\ntar_invalidate() removes metadata from specific targets but keeps their data files in _targets/objects/.\ntar_meta_delete() removes _targets/meta/ files and their copies on the cloud.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Local data</span>"
    ]
  },
  {
    "objectID": "cloud-storage.html",
    "href": "cloud-storage.html",
    "title": "11  Cloud storage",
    "section": "",
    "text": "11.1 Benefits\ntargets can store data and metadata on the cloud, either with Amazon Web Service (AWS) Simple Storage Service (S3) or Google Cloud Platform (GCP) Google Cloud Storage (GCS).",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cloud storage</span>"
    ]
  },
  {
    "objectID": "cloud-storage.html#benefits",
    "href": "cloud-storage.html#benefits",
    "title": "11  Cloud storage",
    "section": "",
    "text": "11.1.1 Store less data locally\n\nUse tar_option_set() and tar_target() to opt into cloud storage and configure options.\ntar_make() uploads regular target data to a cloud bucket instead of the local _targets/objects/ folder.\nIf the repository_meta is not \"local\", then every seconds_meta seconds, tar_make() uploads metadata and still keeps local copies in _targets/meta/ folder.1\n\n\n\n11.1.2 Inspect the results on a different computer\n\ntar_meta_download() downloads the latest metadata from the bucket to the local _targets/meta/ folder.2 This allows you to interactively inspect the downloaded snapshot of the pipeline using tar_visnetwork(), tar_progress(), etc.3\nHelpers like tar_read() read local metadata and access target data in the bucket.\nAs of targets version 1.10.1.9002, tar_make() uploads debugging workspaces to the cloud. tar_workspace_download() can download a workspace file so you can load it locally with tar_workspace().4\n\n\n\n11.1.3 Track history\n\nTurn on versioning in your bucket.\ntar_make() records the versions of the target data in _targets/meta/meta.\nCommit _targets/meta/meta to the same version-controlled repository as your R code.\nRoll back to a prior commit to roll back the local metadata and give targets access to prior versions of the target data.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cloud storage</span>"
    ]
  },
  {
    "objectID": "cloud-storage.html#setup",
    "href": "cloud-storage.html#setup",
    "title": "11  Cloud storage",
    "section": "11.2 Setup",
    "text": "11.2 Setup\n\n11.2.1 AWS setup\nSkip these steps if you already have an AWS account and bucket.\n\nSign up for a free tier account at https://aws.amazon.com/free.\nRead the Simple Storage Service (S3) instructions and practice in the web console.\nInstall the paws.storage R package: install.packages(\"paws.storage\").\nFollow the paws documentation to set your AWS security credentials.\nCreate an S3 bucket, either in the web console or with paws.storage::s3()$create_bucket().\n\n\n\n11.2.2 GCP setup\nSkip these steps if you already have an GCP account and bucket.\n\nActivate a Google Cloud Platform account at https://cloud.google.com.\nInstall the googleCloudStorageR R package: install.packages(\"googleCloudStorageR\").\nFollow the googleCloudStorageR setup instructions to authenticate into Google Cloud and enable required APIs.\nCreate a Google Cloud Storage (GCS) bucket either in the web console or googleCloudStorageR::gcs_create_bucket().\n\n\n\n11.2.3 Pipeline setup\nUse tar_option_set() to opt into cloud storage and declare options. For AWS:5\n\nresources = tar_resources(aws = tar_resources_aws(bucket = \"YOUR_BUCKET\", prefix = \"YOUR/PREFIX\"))\nrepository = \"aws\"\nOptional: repository_meta = \"aws\" (for metadata uploads).\n\nDetails:\n\nThe process is analogous for GCP.\nThe prefix is just like tar_config_get(\"store\"), but for the cloud. It controls where the data objects live in the bucket, and it should not conflict with other projects.\nArguments repository, resources, and cue of tar_target() override their counterparts in tar_option_set().\nIn tar_option_set(), repository controls the target data.\nIn tar_option_set(), repository_meta controls the metadata. For example, set to \"aws\" to periodically upload the metadata to the AWS S3 bucket configured in resources. Set to \"local\" to opt out.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cloud storage</span>"
    ]
  },
  {
    "objectID": "cloud-storage.html#example",
    "href": "cloud-storage.html#example",
    "title": "11  Cloud storage",
    "section": "11.3 Example",
    "text": "11.3 Example\nConsider a pipeline with two simple targets.\n\n# Example _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\n\ntar_option_set(\n  repository = \"aws\",\n  repository_meta = \"aws\", # Just for metadata uploads, not required.\n  resources = tar_resources(\n    aws = tar_resources_aws(\n      bucket = \"my-test-bucket-25edb4956460647d\",\n      prefix = \"my_project_name\"\n    )\n  )\n)\n\nwrite_file &lt;- function(data) {\n  saveRDS(data, \"file.rds\")\n  \"file.rds\"\n}\n\nlist(\n  tar_target(data, rnorm(5), format = \"qs\"), \n  tar_target(file, write_file(data), format = \"file\")\n)\n\nAs usual, tar_make() runs the correct targets in the correct order. Both data files now live in bucket my-test-bucket-25edb4956460647d at S3 key paths which begin with prefix my_project_name. The file file.rds still exists locally because of format = \"file\", but _targets/objects/data exists only in the cloud.\n\ntar_make()\n#&gt; + data dispatched\n#&gt; ✔ data completed [0ms, 87 B]\n#&gt; + file dispatched\n#&gt; ✔ file completed [0ms, 87 B]\n#&gt; ✔ ended pipeline [524ms, 2 completed, 0 skipped]\n\nrepository_meta = \"aws\" lets you switch to a different computer and download your metadata with tar_meta_download(). At that point, your results will appear up to date from the new computer.\n\ntar_make()\n#&gt; ✔ skipped pipeline [50ms, 2 skipped]\n\ntar_read() read local metadata and cloud target data.\n\ntar_read(data)\n#&gt; [1] -0.74654607 -0.59593497 -1.57229983  0.40915323  0.02579023\n\nFor a file target, tar_read() downloads the file to its original location and returns the path.\n\npath &lt;- tar_read(file)\npath\n#&gt; [1] \"file.rds\"\nreadRDS(path)\n#&gt; [1] -0.74654607 -0.59593497 -1.57229983  0.40915323  0.02579023",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cloud storage</span>"
    ]
  },
  {
    "objectID": "cloud-storage.html#footnotes",
    "href": "cloud-storage.html#footnotes",
    "title": "11  Cloud storage",
    "section": "",
    "text": "Metadata snapshots are synchronous, so a long target with deployment = \"main\" may block the main R process and delay uploads.↩︎\nFunctions tar_meta_upload(), tar_meta_sync(), and tar_meta_delete() also manage cloud metadata.↩︎\nRequires that the repository_meta option (in tar_option_get()) is not equal to \"local\" in _targets.R↩︎\nRequires that the repository_meta option (in tar_option_get()) is not equal to \"local\" in _targets.R↩︎\ncue = tar_cue(file = FALSE) is no longer recommended for cloud storage. This unwise shortcut is no longer necessary, as of https://github.com/ropensci/targets/pull/1181 (targets version &gt;= 1.3.2.9003).↩︎",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Cloud storage</span>"
    ]
  },
  {
    "objectID": "literate-programming.html",
    "href": "literate-programming.html",
    "title": "12  Literate programming",
    "section": "",
    "text": "12.1 Scope\nLiterate programming is the practice of mixing code and descriptive writing in order to execute and explain a data analysis simultaneously in the same document. The targets package supports literate programming through tight integration with Quarto, R Markdown, and knitr. It is recommended to learn one of these three tools before proceeding.\nThere are two kinds of literate programming in targets:\nThis chapter focuses on (1), which is more ubiquitous and versatile.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#scope",
    "href": "literate-programming.html#scope",
    "title": "12  Literate programming",
    "section": "",
    "text": "A literate programming source document (or Quarto project) that renders inside an individual target. Here, you define a special kind of target that runs a lightweight R Markdown report or Quarto report/project which depends on upstream targets.\nThe tar_tangle() function available in tarchetypes &gt;= 0.13.2.9002, which lets you express the pipeline itself as a Quarto or R Markdown document.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#r-markdown-targets",
    "href": "literate-programming.html#r-markdown-targets",
    "title": "12  Literate programming",
    "section": "12.2 R Markdown targets",
    "text": "12.2 R Markdown targets\nHere, literate programming serves to display, summarize, and annotate results from upstream in the targets pipeline. The document(s) have little to no computation of their own, and they make heavy use of tar_read() and tar_load() to leverage output from other targets.\nAs an example, let us extend the walkthrough example chapter with the following R Markdown source file report.Rmd.\n\nThis document depends on targets fit and hist. If we previously ran the pipeline and the data store _targets/ exists, then tar_read() and tar_load() will read those targets and show them in the rendered HTML output report.html.\n\nWith the tar_render() function in tarchetypes, we can go a step further and include report.Rmd as a target in the pipeline. This new targets re-renders report.Rmd whenever fit or hist changes, which means tar_make() brings the output file report.html up to date.\n\nlibrary(targets)\nlibrary(tarchetypes)\ntarget &lt;- tar_render(report, \"report.Rmd\") # Just defines a target definition object.\ntarget$command$expr[[1]]\n#&gt; tarchetypes::tar_render_run(path = \"report.Rmd\", args = list(input = \"report.Rmd\", \n#&gt;     knit_root_dir = getwd(), quiet = TRUE), deps = list(fit, \n#&gt;     hist))\n\ntar_render() is like tar_target(), except that you supply the file path to the R Markdown report instead of an R command. Here it is at the bottom of the example _targets.R file below:\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nsource(\"R/functions.R\")\nlist(\n  tar_target(\n    raw_data_file,\n    \"data/raw_data.csv\",\n    format = \"file\"\n  ),\n  tar_target(\n    raw_data,\n    read_csv(raw_data_file, col_types = cols())\n  ),\n  tar_target(\n    data,\n    raw_data %&gt;%\n      mutate(Ozone = replace_na(Ozone, mean(Ozone, na.rm = TRUE)))\n  ),\n  tar_target(hist, create_plot(data)),\n  tar_target(fit, biglm(Ozone ~ Wind + Temp, data)),\n  tar_render(report, \"report.Rmd\") # Here is our call to tar_render().\n)\n\nWhen we visualize the pipeline, we see that our report target depends on targets fit and hist. tar_render() automatically detects these upstream dependencies by statically analyzing report.Rmd for calls to tar_load() and tar_read().\n\n# R console\ntar_visnetwork()",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#quarto-targets",
    "href": "literate-programming.html#quarto-targets",
    "title": "12  Literate programming",
    "section": "12.3 Quarto targets",
    "text": "12.3 Quarto targets\ntarchetypes &gt;= 0.6.0.9000 supports a tar_quarto() function, which is like tar_render(), but for Quarto. For an individual source document, tar_quarto() works exactly the same way as tar_render(). However, tar_quarto() is more powerful: you can supply the path to an entire Quarto project, such as a book, blog, or website. tar_quarto() looks for target dependencies in all the source documents (e.g. listed in _quarto.yml), and it tracks the important files in the project for changes (run tar_quarto_files() to see which ones).",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#parameterized-documents",
    "href": "literate-programming.html#parameterized-documents",
    "title": "12  Literate programming",
    "section": "12.4 Parameterized documents",
    "text": "12.4 Parameterized documents\ntarchetypes functions make it straightforward to use parameterized R Markdown and parameterized Quarto in a targets pipeline. The next two subsections walk through the major use cases.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#single-parameter-set",
    "href": "literate-programming.html#single-parameter-set",
    "title": "12  Literate programming",
    "section": "12.5 Single parameter set",
    "text": "12.5 Single parameter set\nIn this scenario, the pipeline renders your parameterized report one time using a single set of parameters. These parameters can be upstream targets, global objects, or fixed values. Simply pass a params argument to tar_render() or an execute_params argument to tar_quarto(). Example:\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlist(\n  tar_target(data, data.frame(x = seq_len(26), y = letters))\n  tar_quarto(report, \"report.qmd\", execute_params = list(your_param = data))\n)\n\nInternally, the report target runs:\n\n# R console\nquarto::quarto_render(\"report.qmd\", params = list(your_param = your_target))\n\nwhere report.qmd looks like this:\n\nSee tar_quarto() examples and tar_render() examples for more.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#multiple-parameter-sets",
    "href": "literate-programming.html#multiple-parameter-sets",
    "title": "12  Literate programming",
    "section": "12.6 Multiple parameter sets",
    "text": "12.6 Multiple parameter sets\nIn this scenario, you still have a single report, but you render it multiple times over a grid of parameters. This time, use tar_quarto_rep() or tar_render_rep(). Each of these functions takes as input a grid of parameters with one column per parameter and one row per parameter set, where each parameter set is used to render an instance of the document. In other words, the number of rows in the parameter grid is the number of output documents you will produce. Below is an example _targets.R file using tar_render_rep(). Usage with tar_quarto_rep() is the same1.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\ntar_option_set(packages = \"tibble\")\nlist(\n  tar_target(x, \"value_of_x\"),\n  tar_render_rep(\n    report,\n    \"report.Rmd\",\n    execute_params = tibble(\n      your_param = c(\"par_val_1\", \"par_val_2\", \"par_val_3\", \"par_val_4\"),\n      output_file = c(\"f1.html\", \"f2.html\", \"f3.html\", \"f4.html\")\n    ),\n    batches = 2\n  )\n)\n\nwhere report.Rmd has the following YAML front matter:\ntitle: report\noutput_format: html_document\nparams:\n  par: \"default value\"\nand the following R code chunk:\n\nprint(params$par)\nprint(tar_read(x))\n\ntar_render_rep() creates a target for the parameter grid and uses dynamic branching to render the output reports in batches. In this case, we have two batches (dynamic branches) that each produce two reports (four output reports total).\n\n# R console\ntar_make()\n#&gt; + x dispatched\n#&gt; ✔ x completed [3ms, 62 B]\n#&gt; + report_params dispatched\n#&gt; ✔ report_params completed [2ms, 204 B]\n#&gt; + report declared [2 branches]\n#&gt; ✔ report completed [8.4s, 1.81 MB]                          \n#&gt; ✔ ended pipeline [8.6s, 4 completed, 0 skipped] \n\nThe third output file f3.html is below, and the rest look similar.\n\nFor more information, see these examples.",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "literate-programming.html#footnotes",
    "href": "literate-programming.html#footnotes",
    "title": "12  Literate programming",
    "section": "",
    "text": "except the parameter grid argument is called execute_params in tar_quarto_rep().↩︎",
    "crumbs": [
      "Data and files",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Literate programming</span>"
    ]
  },
  {
    "objectID": "crew.html",
    "href": "crew.html",
    "title": "13  Distributed computing",
    "section": "",
    "text": "13.1 How it works\nTo efficiently process a large and complex pipeline, tar_make() can run multiple targets at the same time. Thanks to integration with crew and blazing fast scheduling from mirai behind the scenes, those targets can run on a variety of high-performance computing platforms, and they can scale out to the hundreds and beyond.\nThe crew controller from (1) allows tar_make() to launch external R processes called “workers” which can each run one or more targets. By delegating long-running targets to these workers, the local R session is free to focus on other tasks, and the pipeline finishes faster.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#how-it-works",
    "href": "crew.html#how-it-works",
    "title": "13  Distributed computing",
    "section": "",
    "text": "Write your pipeline as usual, but set the controller argument of tar_option_set to the crew controller of your choice.\nRun the pipeline with a simple tar_make().",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#example",
    "href": "crew.html#example",
    "title": "13  Distributed computing",
    "section": "13.2 Example",
    "text": "13.2 Example\nThe following _targets.R file uses a local process controller with 2 workers. That means up to 2 workers can be running at any given time, and each worker is an separate R process on the same computer as the local R process.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(crew)\ntar_option_set(\n  controller = crew_controller_local(workers = 2)\n)\ntar_source()\nlist(\n  tar_target(name = data, command = get_data()),\n  tar_target(name = model1, command = run_model1(data)),\n  tar_target(name = model2, command = run_model2(data)),\n  tar_target(name = model3, command = run_model3(data)),\n  tar_target(name = plot1, command = plot_model(model1)),\n  tar_target(name = plot2, command = plot_model(model2)),\n  tar_target(name = plot3, command = plot_model(model3))\n)\n\n\n# R console\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\nRun the pipeline with a simple call to tar_make(). Please note that real-life pipelines will have longer execution times, especially for the models.\n\n# R console\ntar_make()\n\n\n#&gt; + data dispatched\n#&gt; ✔ data completed [0ms, 48 B]\n#&gt; + model1 dispatched\n#&gt; ✔ model1 completed [0ms, 48 B]\n#&gt; + model2 dispatched\n#&gt; ✔ model2 completed [0ms, 48 B]\n#&gt; + model3 dispatched\n#&gt; ✔ model3 completed [0ms, 48 B]\n#&gt; + plot1 dispatched\n#&gt; ✔ plot1 completed [0ms, 48 B]\n#&gt; + plot2 dispatched\n#&gt; ✔ plot2 completed [1ms, 48 B]\n#&gt; + plot3 dispatched\n#&gt; ✔ plot3 completed [1ms, 48 B]\n#&gt; ✔ ended pipeline [187ms, 7 completed, 0 skipped]\n\nLet’s talk through what happens in the above call to tar_make(). First, a new worker launches and sends the data target to the crew queue. After the data target completes, all three models are ready to begin. A second worker automatically launches to meet the increased demand of the workload, and each of the two workers starts to run a model. After one of the models finishes, its worker is free to either run the downstream plot or the third model. The process continues until all the targets are complete. The workers shut down when the pipeline is done.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#configuration-and-auto-scaling",
    "href": "crew.html#configuration-and-auto-scaling",
    "title": "13  Distributed computing",
    "section": "13.3 Configuration and auto-scaling",
    "text": "13.3 Configuration and auto-scaling\nAdding more workers might speed up your pipeline, but not always. Beyond a certain point, the efficiency gains will diminish, and the extra workers will have nothing to do. With proper configuration, you can find the right balance.\nAs mentioned above, new workers launch automatically in response to increasing demand. By default, they stay running for the duration of the pipeline. However, you can customize the controller to scale down when circumstances allow, which helps help avoid wasting resources1 The most useful arguments for down-scaling, in order of importance, are:\n\nseconds_idle: automatically shut down a worker if it spends too long waiting for a target.\ntasks_max: maximum number of tasks a worker can run before shutting down.\nseconds_wall: soft wall time of a worker.\n\nOn the other hand, it is not always helpful to eagerly down-scale workers. Because the workload can fluctuate rapidly, some workers may quit and relaunch so often that it creates noticeable overhead. crew and its plugins try to set reasonable defaults, but you may need to adjust for optimal efficiency.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#plugins",
    "href": "crew.html#plugins",
    "title": "13  Distributed computing",
    "section": "13.4 Plugins",
    "text": "13.4 Plugins\ncrew is a platform for multiple computing platforms, not just local processes, but also traditional high-performance computing systems and cloud computing services. For example, to run each worker as a job on a Sun Grid Engine cluster, use crew_controller_sge() from the crew.cluster package.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(crew.cluster)\ntar_option_set(\n  controller = crew_controller_sge(\n    workers = 3,\n    options_cluster = crew_options_sge(\n      script_lines = \"module load R\",\n      log_output = \"log_folder/\"\n    )\n  )\n)\ntar_source()\nlist(\n  tar_target(name = data, command = get_data()),\n  tar_target(name = model1, command = run_model1(data)),\n  tar_target(name = model2, command = run_model2(data)),\n  tar_target(name = model3, command = run_model3(data)),\n  tar_target(name = plot1, command = plot_model(model1)),\n  tar_target(name = plot2, command = plot_model(model2)),\n  tar_target(name = plot3, command = plot_model(model3))\n)\n\nIf crew.cluster and other official packages do not meet your needs, then you can write your own launcher plugin tailored to your own specific computing environment. crew makes this process straightforward, and the vignette at https://wlandau.github.io/crew/articles/launcher_plugins.html walks through the details step by step.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#heterogeneous-workers",
    "href": "crew.html#heterogeneous-workers",
    "title": "13  Distributed computing",
    "section": "13.5 Heterogeneous workers",
    "text": "13.5 Heterogeneous workers\nDifferent targets may have different computing requirements, from memory to GPUs and beyond. You can send different targets to different kinds of workers using crew controller groups. In the _targets.R file below, we create a local process controller alongside a Sun Grid Engine controller a memory requirement and a GPU. We combine them in a crew controller group which we supply to the controller argument of tar_option_set. Next, we use tar_resources() and tar_resources_crew() to tell model2 to run on Sun Grid Engine and all other targets to run on local processes. The deployment = \"main\" argument tells the plots to avoid worker processes altogether and run on the main central R process.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(crew)\nlibrary(crew.cluster)\ncontroller_local &lt;- crew_controller_local(\n  name = \"my_local_controller\",\n  workers = 2,\n  seconds_idle = 10\n)\ncontroller_sge &lt;- crew_controller_sge(\n  name = \"my_sge_controller\",\n  workers = 3,\n  seconds_idle = 15,\n  options_cluster = crew_options_sge(\n    script_lines = \"module load R\",\n    log_output = \"log_folder/\",\n    memory_gigabytes_required = 64,\n    gpu = 1\n  )\n)\ntar_option_set(\n  controller = crew_controller_group(controller_local, controller_sge),\n  resources = tar_resources(\n    crew = tar_resources_crew(controller = \"my_local_controller\")\n  )\n)\ntar_source()\nlist(\n  tar_target(name = data, command = get_data()),\n  tar_target(name = model1, command = run_model1(data)),\n  tar_target(\n    name = model2,\n    command = run_model2(data),\n    resources = tar_resources(\n      crew = tar_resources_crew(controller = \"my_sge_controller\")\n    )\n  ),\n  tar_target(name = model3, run_model3(data)),\n  tar_target(name = plot1, command = plot_model(model1), deployment = \"main\"),\n  tar_target(name = plot2, command = plot_model(model2), deployment = \"main\"),\n  tar_target(name = plot3, command = plot_model(model3), deployment = \"main\")\n)",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#resource-usage",
    "href": "crew.html#resource-usage",
    "title": "13  Distributed computing",
    "section": "13.6 Resource usage",
    "text": "13.6 Resource usage\nThe autometric package can monitor the CPU and memory consumption of the various processes in a targets pipeline, both local processes and parallel workers. Please read https://wlandau.github.io/crew/articles/logging.html for details and examples.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#thanks",
    "href": "crew.html#thanks",
    "title": "13  Distributed computing",
    "section": "13.7 Thanks",
    "text": "13.7 Thanks\nThe crew package is an extension of mirai, a sleek and sophisticated task scheduler that efficiently processes intense workloads. crew is only possible because of the amazing work by Charlie Gao in packages mirai and nanonext.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "crew.html#footnotes",
    "href": "crew.html#footnotes",
    "title": "13  Distributed computing",
    "section": "",
    "text": "Automatic down-scaling also helps comply with wall time restrictions on shared computing clusters. See the arguments of crew_controller_local() for details.↩︎",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Distributed computing</span>"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "14  Performance",
    "section": "",
    "text": "14.1 Basic efficiency\nThis chapter explains simple options and settings to improve the efficiency of your targets pipelines. It also explains how to monitor the progress of a pipeline currently running.1\nThese basic tips help most pipelines run efficiently.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#basic-efficiency",
    "href": "performance.html#basic-efficiency",
    "title": "14  Performance",
    "section": "",
    "text": "14.1.1 Storage\nThe default data storage format is RDS, which can be slow and bulky for large data. For large data pipelines, consider alternative formats to more efficiently store and manage your data. Set the storage format using tar_option_set() or tar_target():\n\ntar_option_set(format = \"qs\")\n\n\n\n\n\n\n\nTipTips about data formats\n\n\n\n\n\nSome formats such as \"qs\" work on all kinds of data, whereas others like \"feather\" works only on data frames. Most non-default formats store the data faster and in smaller files than the default \"rds\" format, but they require extra packages to be installed. For example, format = \"qs\" requires the qs package, and format = \"feather\" requires the arrow package.\nFor extremely large datasets that cannot fit into memory, consider format = \"file\" to treat the data as a file on disk. Downstream targets are free to load only the subsets of the data they need.\n\n\n\n\n\n14.1.2 Memory\ntargets makes decisions about how long to keep target results in memory and when to run garbage collection. You can customize this behavior with the memory and garbage_collection arguments of tar_option_set().2\n\ntar_option_set(\n  memory = \"transient\",\n  garbage_collection = 10 # Can be an integer in version &gt;= 1.8.9003\n)\n\nThese arguments are available to tar_target() as well, although garbage_collection is a TRUE/FALSE value indicating whether to run garbage collection to run just before that specific target runs.\n\ntar_target(\n  name = example_data,\n  command = get_data(),\n  memory = \"transient\"\n  garbage_collection = TRUE\n)\n\n\n\n\n\n\n\nTipAbout memory and garbage collection\n\n\n\n\n\nmemory = \"transient\" tells targets to remove data from the R environment as soon as it is no longer needed. However, the computer memory itself is not freed until garbage collection is run, and even then, R may not decrease the size of its heap. You can run garbage collection yourself with the gc() function in R.\nTransient memory and garbage collection have tradeoffs: the pipeline reads data from storage far more often, and these data reads take additional time. In addition, garbage collection is usually a slow operation, and repeated garbage collections could slow down even a small pipeline with a mere thousand targets.\ngarbage_collection = 100 tells targets to run gc() every 100th active target, both locally and on each parallel worker. The default value is 1000 if not specified directly.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#overhead",
    "href": "performance.html#overhead",
    "title": "14  Performance",
    "section": "14.2 Overhead",
    "text": "14.2 Overhead\nEach target incurs overhead, and it is not good practice to create millions of targets which each run quickly. Instead, consider grouping the same amount of work into a smaller number of targets. See the sections on what a target should do and how much a target should do.\n\n\n\n\n\n\nTipAbout batching\n\n\n\n\n\nSimulation studies and other iterative stochastic pipelines may need to run hundreds of thousands of independent random replications. For these pipelines, consider batching to reduce the number of targets while preserving the number of replications. In batching, each batch is a dynamic branch target that performs a subset of the replications. For 1000 replications, you might want 40 batches of 25 replications each, 10 batches with 100 replications each, or a different balance depending on the use case. Functions tarchetypes::tar_rep(), tarchetypes::tar_map_rep(), and stantargets::tar_stan_mcmc_rep_summary() are examples of target factories that set up the batching structure without needing to understand dynamic branching.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#parallel-processing",
    "href": "performance.html#parallel-processing",
    "title": "14  Performance",
    "section": "14.3 Parallel processing",
    "text": "14.3 Parallel processing\nThese tips add parallel processing to your pipeline and help you use it effectively.\n\n14.3.1 Distributed computing\nConsider distributed computing with crew, as explained at https://books.ropensci.org/targets/crew.html. The targets package knows how to run independent tasks in parallel and wait for tasks that depend on upstream dependencies. crew supports backends such as crew.cluster for traditional clusters and crew.aws.batch for AWS Batch.\n\n\n14.3.2 Worker storage\nIf you run tar_make() with a crew controller, then parallel processes will run your targets, but the main R process still manages all the data by default. To delegate data management to the parallel crew workers, set the storage and retrieval settings in tar_target() or tar_option_set():\n\ntar_option_set(storage = \"worker\", retrieval = \"worker\")\n\nBut be sure those workers have access to the data. They must either find the local data, or the targets must use cloud storage.\n\n\n14.3.3 Local targets\nIn distributed computing with targets, not every target needs to run on a remote worker. For targets that run quickly and cheaply, consider setting deployment = \"main\" in tar_target() to run them on the main local process:\n\ntar_target(dataset, get_dataset(), deployment = \"main\")\ntar_target(summary, compute_summary_statistics(), deployment = \"main\")",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#monitoring-progress",
    "href": "performance.html#monitoring-progress",
    "title": "14  Performance",
    "section": "14.4 Monitoring progress",
    "text": "14.4 Monitoring progress\nEven the most efficient targets pipelines can take time to complete because the user-defined tasks themselves are slow. There are convenient ways to monitor the progress of a running pipeline:\n\ntar_poll() continuously refreshes a text summary of runtime progress in the R console. Run it in a new R session at the project root directory. (Only supported in targets version 0.3.1.9000 and higher.)\ntar_visnetwork(), tar_progress_summary(), tar_progress_branches(), and tar_progress() show runtime information at a single moment in time.\ntar_watch() launches an Shiny app that automatically refreshes the graph every few seconds.\n\n\n\n\n\n\n\nTipExample: monitoring the pipeline with tar_watch()\n\n\n\n\n\n\n# Define an example target script file with a slow pipeline.\nlibrary(targets)\nlibrary(tarchetypes)\ntar_script({\n  sleep_run &lt;- function(...) {\n    Sys.sleep(10)\n  }\n  list(\n    tar_target(settings, sleep_run()),\n    tar_target(data1, sleep_run(settings)),\n    tar_target(data2, sleep_run(settings)),\n    tar_target(data3, sleep_run(settings)),\n    tar_target(model1, sleep_run(data1)),\n    tar_target(model2, sleep_run(data2)),\n    tar_target(model3, sleep_run(data3)),\n    tar_target(figure1, sleep_run(model1)),\n    tar_target(figure2, sleep_run(model2)),\n    tar_target(figure3, sleep_run(model3)),\n    tar_target(conclusions, sleep_run(c(figure1, figure2, figure3)))\n  )\n})\n\n# Launch the app in a background process.\n# You may need to refresh the browser if the app is slow to start.\n# The graph automatically refreshes every 10 seconds\ntar_watch(seconds = 10, outdated = FALSE, targets_only = TRUE)\n\n# Now run the pipeline and watch the graph change.\npx &lt;- tar_make()\n\n tar_watch_ui() and tar_watch_server() make this functionality available to other apps through a Shiny module.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#profiling",
    "href": "performance.html#profiling",
    "title": "14  Performance",
    "section": "14.5 Profiling",
    "text": "14.5 Profiling\nProfiling tools like profvis and proffer empirically identify the places where code runs slowly. It is important to identify these bottlenecks before you try to optimize. Visit https://r-prof.github.io/proffer/#why-use-a-profiler to see a motivating example of profiling in action.\nTo profile your project with profvis, run:\n\nresults &lt;- profvis::profvis(\n  targets::tar_make(\n    callr_function = NULL, # Do not run the pipline behind a callr::r() process.\n    use_crew = FALSE, # Disable parallel computing with crew (optional)\n    as_job = FALSE # Do not run the pipeline in a Posit Workbench / RStudio background job.\n  )\n)\nprint(results, aggregate = TRUE) # aggregate = TRUE is crucial for interpretable flame graphs.\n\nWith proffer, profiling is similar.\n\nproffer::pprof(\n  targets::tar_make(\n    callr_function = NULL, # Do not run the pipline behind a callr::r() process.\n    use_crew = FALSE, # Disable parallel computing with crew (optional)\n    as_job = FALSE # Do not run the pipeline in a Posit Workbench / RStudio background job.\n  )\n)\n\nPoth packages render flame graphs that show where bottlenecks occur.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#resource-usage",
    "href": "performance.html#resource-usage",
    "title": "14  Performance",
    "section": "14.6 Resource usage",
    "text": "14.6 Resource usage\nThe autometric package can monitor the CPU and memory consumption of the various processes in a targets pipeline. This is mainly useful for high-performance computing workloads with parallel workers. Please read https://wlandau.github.io/crew/articles/logging.html for details and examples.",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "performance.html#footnotes",
    "href": "performance.html#footnotes",
    "title": "14  Performance",
    "section": "",
    "text": "cue = tar_cue(file = FALSE) is no longer recommended for cloud storage. This unwise shortcut is no longer necessary, as of https://github.com/ropensci/targets/pull/1181 (targets version &gt;= 1.3.2.9003).↩︎\nAs of version 1.8.0.9011, the default behavior of tar_make() (encoded with memory = \"auto\") is to release dynamic branches as soon as possible and keeps other targets in memory until the pipeline is finished. In addition, in targets &gt;= 1.8.0.9003, the garbage_collection option can be a non-negative integer to control how often garbage collection happens (e.g. every 10th target).↩︎",
    "crumbs": [
      "Heavy workloads",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Performance</span>"
    ]
  },
  {
    "objectID": "dynamic.html",
    "href": "dynamic.html",
    "title": "15  Dynamic branching",
    "section": "",
    "text": "15.1 Branching\nSometimes, a pipeline contains more targets than a user can comfortably type by hand. For projects with hundreds of thousands of targets, branching can make the code in _targets.R shorter and more concise.\ntargets supports two types of branching: dynamic branching and static branching. Some projects are better suited to dynamic branching, while others benefit more from static branching or a combination of both. Here is a short list of tradeoffs.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#branching",
    "href": "dynamic.html#branching",
    "title": "15  Dynamic branching",
    "section": "",
    "text": "Dynamic\nStatic\n\n\n\n\nPipeline creates new targets at runtime.\nAll targets defined in advance.\n\n\nCryptic target names.\nFriendly target names.\n\n\nScales to hundreds of thousands of branches.\nDoes not scale as easily, especially with tar_visnetwork() graphs\n\n\nNo metaprogramming required.\nFamiliarity with metaprogramming is helpful.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#about-dynamic-branching",
    "href": "dynamic.html#about-dynamic-branching",
    "title": "15  Dynamic branching",
    "section": "15.2 About dynamic branching",
    "text": "15.2 About dynamic branching\nDynamic branching is the act of defining new targets (called branches) while the pipeline is running (e.g. during tar_make()). Prior to launching the pipeline, the user does not need to know the number of branches or the input data of each branch.\nTo use dynamic branching, set the pattern argument of tar_target(). The pattern determines how dynamic branches are created and how the input data is partitioned among the branches. A branch is single iteration of the target’s command on a single piece of the input data. Branches are automatically created based on how the input data breaks into pieces, and targets automatically combines the output from all the branches when you reference the dynamic target as a whole.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#example",
    "href": "dynamic.html#example",
    "title": "15  Dynamic branching",
    "section": "15.3 Example",
    "text": "15.3 Example\nTo illustrate, consider the example pipeline below. It uses dynamic branching to generate random spirographs using code borrowed from W. Joel Schneider’s spiro package.1. A spirograph is a type of two-dimensional algebraic curve determined (in part) by parameters fixed_radius and cycling_radius. Targets fixed_radius and cycling_radius draw random parameter values, and the dynamic target points generates a spirograph dataset for each set of parameters (one spirograph per dynamic branch). Target single_plot plots each spirograph separately, and combine_plot plots all the spirographs together.\n\n# _targets.R\nlibrary(ggplot2)\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\n\n# From https://github.com/wjschne/spiro/blob/87f73ec37ceb0a7a9d09856ada8ae28d587a2ebd/R/spirograph.R\n# Adapted under the CC0 1.0 Universal license: https://github.com/wjschne/spiro/blob/87f73ec37ceb0a7a9d09856ada8ae28d587a2ebd/LICENSE.md\nspirograph_points &lt;- function(fixed_radius, cycling_radius) {\n  t &lt;- seq(1, 30 * pi, length.out = 1e4)\n  diff &lt;- (fixed_radius - cycling_radius)\n  ratio &lt;- diff / cycling_radius\n  x &lt;- diff * cos(t) + cos(t * ratio)\n  y &lt;- diff * sin(t) - sin(t * ratio)\n  tibble(x = x, y = y, fixed_radius = fixed_radius, cycling_radius = cycling_radius)\n}\n\nplot_spirographs &lt;- function(points) {\n  label &lt;- \"fixed_radius = %s, cycling_radius = %s\"\n  points$parameters &lt;- sprintf(label, points$fixed_radius, points$cycling_radius)\n  ggplot(points) +\n    geom_point(aes(x = x, y = y, color = parameters), size = 0.1) +\n    facet_wrap(~parameters) +\n    theme_gray(16) +\n    guides(color = \"none\")\n}\n\nlist(\n  tar_target(fixed_radius, sample.int(n = 10, size = 2)),\n  tar_target(cycling_radius, sample.int(n = 10, size = 2)),\n  tar_target(\n    points,\n    spirograph_points(fixed_radius, cycling_radius),\n    pattern = map(fixed_radius, cycling_radius)\n  ),\n  tar_target(\n    single_plot,\n    plot_spirographs(points),\n    pattern = map(points),\n    iteration = \"list\"\n  ),\n  tar_target(combined_plot, plot_spirographs(points))\n)\n\n\ntar_make()\n#&gt; + cycling_radius dispatched\n#&gt; ✔ cycling_radius completed [0ms, 52 B]\n#&gt; + fixed_radius dispatched\n#&gt; ✔ fixed_radius completed [0ms, 51 B]\n#&gt; + points declared [2 branches]\n#&gt; ✔ points completed [11ms, 309.30 kB]\n#&gt; + combined_plot dispatched\n#&gt; ✔ combined_plot completed [64ms, 3.83 MB]\n#&gt; + single_plot declared [2 branches]\n#&gt; ✔ single_plot completed [69ms, 4.87 MB]\n#&gt; ✔ ended pipeline [2.3s, 7 completed, 0 skipped]\n\nThe final plot shows all the spirographs together.\n\ntar_read(combined_plot)\n\n\n\n\n\n\n\n\nThis plot comes from all the branches of points aggregated together. Because target points has iteration = \"vector\" in tar_target(), any reference to the whole target automatically aggregates the branches using vctrs::vec_c(). For data frames, this just binds all the rows.\n\ntar_read(points)\n#&gt; # A tibble: 20,000 × 4\n#&gt;         x     y fixed_radius cycling_radius\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1 -1.26  -2.94            5              9\n#&gt;  2 -1.23  -2.95            5              9\n#&gt;  3 -1.20  -2.97            5              9\n#&gt;  4 -1.17  -2.98            5              9\n#&gt;  5 -1.14  -3.00            5              9\n#&gt;  6 -1.11  -3.01            5              9\n#&gt;  7 -1.08  -3.03            5              9\n#&gt;  8 -1.05  -3.04            5              9\n#&gt;  9 -1.02  -3.06            5              9\n#&gt; 10 -0.985 -3.07            5              9\n#&gt; # ℹ 19,990 more rows\n\nBy contrast, target single_plot list of branches because of iteration = \"list\".\n\ntar_load(single_plot)\nclass(single_plot)\n#&gt; [1] \"list\"\nlength(single_plot)\n#&gt; [1] 2\n\nUse the branches argument of tar_read() to read an individual branch or subset of branches.\n\ntar_read(single_plot, branches = 1)\n#&gt; $single_plot_b415e3b98a99d41d",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#provenance",
    "href": "dynamic.html#provenance",
    "title": "15  Dynamic branching",
    "section": "15.4 Provenance",
    "text": "15.4 Provenance\nRecall our dynamic target points with branches for spirograph datasets. Each branch has columns fixed_radius and cycling_radius so we know which parameter set each spirograph used. It is good practice to proactively append this metadata to each branch, e.g. in spirograph_points(). That way, if a branch errors out, it is easy to track down the upstream data that caused it.2\n\ntar_read(points, branches = 1) # first branch\n#&gt; # A tibble: 10,000 × 4\n#&gt;         x     y fixed_radius cycling_radius\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1 -1.26  -2.94            5              9\n#&gt;  2 -1.23  -2.95            5              9\n#&gt;  3 -1.20  -2.97            5              9\n#&gt;  4 -1.17  -2.98            5              9\n#&gt;  5 -1.14  -3.00            5              9\n#&gt;  6 -1.11  -3.01            5              9\n#&gt;  7 -1.08  -3.03            5              9\n#&gt;  8 -1.05  -3.04            5              9\n#&gt;  9 -1.02  -3.06            5              9\n#&gt; 10 -0.985 -3.07            5              9\n#&gt; # ℹ 9,990 more rows\n\n\ntar_read(points, branches = 2) # second branch\n#&gt; # A tibble: 10,000 × 4\n#&gt;           x     y fixed_radius cycling_radius\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;          &lt;int&gt;\n#&gt;  1 -0.112   -1.44            6              8\n#&gt;  2 -0.0965  -1.44            6              8\n#&gt;  3 -0.0813  -1.45            6              8\n#&gt;  4 -0.0659  -1.46            6              8\n#&gt;  5 -0.0505  -1.47            6              8\n#&gt;  6 -0.0350  -1.47            6              8\n#&gt;  7 -0.0194  -1.48            6              8\n#&gt;  8 -0.00377 -1.49            6              8\n#&gt;  9  0.0120  -1.49            6              8\n#&gt; 10  0.0278  -1.50            6              8\n#&gt; # ℹ 9,990 more rows",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#patterns",
    "href": "dynamic.html#patterns",
    "title": "15  Dynamic branching",
    "section": "15.5 Patterns",
    "text": "15.5 Patterns\ntargets supports many more types of dynamic branching patterns.\n\nmap(): one branch per tuple of elements.\ncross(): one branch per combination of elements.\nslice(): select individual pieces to branch over. For example, pattern = slice(x, index = c(3, 4)) branches over the third and fourth slices (or branches) of target x.\nhead(): branch over the first few elements.\ntail(): branch over the last few elements.\nsample(): branch over a random subset of elements.\n\nPatterns are composable. For example, pattern = cross(other_parameter, map(fixed_radius, cycling_radius)) is conceptually equivalent to tidyr::crossing(other_parameter, tidyr::nesting(fixed_radius, cycling_radius)). You can test and experiment with branching structures using tar_pattern(). In the output below, suffixes _1, _2, and _3, denote both dynamic branches and the slices of upstream data they branch over.\n\ntar_pattern(\n  cross(other_parameter, map(fixed_radius, cycling_radius)),\n  other_parameter = 3,\n  fixed_radius = 2,\n  cycling_radius = 2\n)\n#&gt; # A tibble: 6 × 3\n#&gt;   other_parameter   fixed_radius   cycling_radius  \n#&gt;   &lt;chr&gt;             &lt;chr&gt;          &lt;chr&gt;           \n#&gt; 1 other_parameter_1 fixed_radius_1 cycling_radius_1\n#&gt; 2 other_parameter_1 fixed_radius_2 cycling_radius_2\n#&gt; 3 other_parameter_2 fixed_radius_1 cycling_radius_1\n#&gt; 4 other_parameter_2 fixed_radius_2 cycling_radius_2\n#&gt; 5 other_parameter_3 fixed_radius_1 cycling_radius_1\n#&gt; 6 other_parameter_3 fixed_radius_2 cycling_radius_2",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#iteration",
    "href": "dynamic.html#iteration",
    "title": "15  Dynamic branching",
    "section": "15.6 Iteration",
    "text": "15.6 Iteration\nThe iteration argument of tar_target() determines how to split non-dynamic targets and how to aggregate dynamic ones. There are two major types of iteration: \"vector\" (default) and \"list\". There is also iteration = \"group\", which this chapter covers in the later section on branching over row groups.\n\n15.6.1 Vector iteration\nVector iteration uses the vctrs package to intelligently split and combine dynamic branches based on the underlying type of the object. Branches of vectors are automatically vectors, branches of data frames are automatically data frames, aggregates of vectors are automatically vectors, and aggregates of data frames are automatically data frames. This consistency makes most data processing tasks extremely smooth.\nConsider the following pipeline:\n\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\n\nlist(\n  tar_target(\n    name = cycling_radius,\n    command = c(1, 2),\n    iteration = \"vector\"\n  ),\n  tar_target(\n    name = points_template,\n    command = tibble(x = c(1, 2), y = c(1, 2), fixed_radius = c(1, 2)),\n    iteration = \"vector\"\n  ),\n  tar_target(\n    name = points_branches,\n    command = add_column(points_template, cycling_radius = cycling_radius),\n    pattern = map(cycling_radius, points_template),\n    iteration = \"vector\"\n  ),\n  tar_target(\n    name = combined_points,\n    command = points_branches\n  )\n)\n\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_make()\n#&gt; + cycling_radius dispatched\n#&gt; ✔ cycling_radius completed [0ms, 55 B]\n#&gt; + points_template dispatched\n#&gt; ✔ points_template completed [2ms, 159 B]\n#&gt; + points_branches declared [2 branches]\n#&gt; ✔ points_branches completed [3ms, 338 B]\n#&gt; + combined_points dispatched\n#&gt; ✔ combined_points completed [0ms, 180 B]\n#&gt; ✔ ended pipeline [174ms, 5 completed, 0 skipped]\n\nWe observe the following:\n\ntar_read(points_branches)\n#&gt; # A tibble: 2 × 4\n#&gt;       x     y fixed_radius cycling_radius\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1     1     1            1              3\n#&gt; 2     2     2            2              4\n\ntar_read(points_branches, branches = 2)\n#&gt; # A tibble: 1 × 4\n#&gt;       x     y fixed_radius cycling_radius\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1     2     2            2              4\n\ntar_read(combined_points)\n#&gt; # A tibble: 2 × 4\n#&gt;       x     y fixed_radius cycling_radius\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1     1     1            1              3\n#&gt; 2     2     2            2              4\n\niteration = \"vector\" produces convenient tibbles because:\n\nvctrs::vec_slice() intelligently splits the non-dynamic targets for branching.\nvctrs::vec_c() implicitly combines branches when you reference a dynamic target as a whole.\n\nSo the pipeline is equivalent to:\n\n# cycling_radius target:\ncycling_radius &lt;- c(3, 4)\n\n# points_template target:\npoints_template &lt;- tibble(x = c(1, 2), y = c(1, 2), fixed_radius = c(1, 2))\n\n# points_branches target:\npoints_branches &lt;- lapply(\n  X = seq_len(2),\n  FUN = function(index) {\n    # effect of iteration = \"vector\" in cycling_radius:\n    branch_cycling_radius &lt;- vctrs::vec_slice(cycling_radius, index)\n    # effect of iteration = \"vector\" in points_template:\n    branch_points_template &lt;- vctrs::vec_slice(points_template, index)\n    # command of points_branches target:\n    add_column(branch_points_template, cycling_radius = branch_cycling_radius)\n  }\n)\n\n# combined_points target:\npoints_branches$.name_spec = \"{outer}_{inner}\"\ncombined_points &lt;- do.call( # effect of iteration = \"vector\" in points_branches\n  what = vctrs::vec_c,\n  args = points_branches\n)\n\n\n\n15.6.2 List iteration\niteration = \"vector\" does not know how to split or aggregate every data type. For example, vctrs cannot combine ggplot2 objects into a vector. iteration = \"list\" is a simple workaround that treats everything as a list during splitting and aggregation. Let’s demonstrate on a simple pipeline:\n\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\n\nlist(\n  tar_target(\n    name = radius_origin,\n    command = c(1, 2),\n    iteration = \"list\"\n  ),\n  tar_target(\n    name = radius_branches,\n    command = radius_origin + 5,\n    pattern = map(radius_origin),\n    iteration = \"list\"\n  ),\n  tar_target(\n    name = radius_combined,\n    command = radius_branches\n  )\n)\n\n\ntar_visnetwork()\n\n\n\n\n\n\ntar_make()\n#&gt; + radius_origin dispatched\n#&gt; ✔ radius_origin completed [0ms, 55 B]\n#&gt; + radius_branches declared [2 branches]\n#&gt; ✔ radius_branches completed [0ms, 102 B]\n#&gt; + radius_combined dispatched\n#&gt; ✔ radius_combined completed [0ms, 138 B]\n#&gt; ✔ ended pipeline [160ms, 4 completed, 0 skipped]\n\nWe observe the following:\n\ntar_read(radius_branches)\n#&gt; $radius_branches_a296b06d24ea0879\n#&gt; [1] 6\n#&gt; \n#&gt; $radius_branches_5944b58b912631c4\n#&gt; [1] 7\n\ntar_read(radius_branches, branches = 2)\n#&gt; $radius_branches_5944b58b912631c4\n#&gt; [1] 7\n\ntar_read(radius_combined)\n#&gt; $radius_branches_a296b06d24ea0879\n#&gt; [1] 6\n#&gt; \n#&gt; $radius_branches_5944b58b912631c4\n#&gt; [1] 7\n\nAs we see above, iteration = \"list\" uses [[ to split non-dynamic targets and list() to combine dynamic branches. Except for the special branch names above, our example pipeline is equivalent to:\n\n# radius_origin target:\nradius_origin &lt;- c(1, 2)\n\n# radius_branches target:\nradius_branches &lt;- lapply(\n  X = seq_len(2),\n  FUN = function(index) {\n    # effect of iteration = \"list\" in radius_origin:\n    branch_radius_origin &lt;- radius_origin[[index]]\n    # command of radius_branches:\n    branch_radius_origin + 5\n  }\n)\n\n# command of radius_combined:\nradius_combined &lt;- do.call( # effect of iteration = \"list\" in radius_branches\n  what = list,\n  args = radius_branches\n)",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#branching-over-row-groups",
    "href": "dynamic.html#branching-over-row-groups",
    "title": "15  Dynamic branching",
    "section": "15.7 Branching over row groups",
    "text": "15.7 Branching over row groups\nTo dynamically branch over dplyr::group_by() row groups of a non-dynamic data frame, use iteration = \"group\" together with tar_group(). The target with iteration = \"group\" must not already be a dynamic target. (In other words, it is invalid to set iteration = \"group\" and pattern = map(...) for the same target.)\nTo demonstrate group iteration, consider the following alternative version of the spirograph pipeline. Below, we start with a monolithic data frame with all the spirographs together, and then we branch over the row groups of that data frame to create one visual for each dynamic branch.\n\n# _targets.R\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\n\n# From https://github.com/wjschne/spiro/blob/87f73ec37ceb0a7a9d09856ada8ae28d587a2ebd/R/spirograph.R\n# Adapted under the CC0 1.0 Universal license: https://github.com/wjschne/spiro/blob/87f73ec37ceb0a7a9d09856ada8ae28d587a2ebd/LICENSE.md\nspirograph_points &lt;- function(fixed_radius, cycling_radius) {\n  t &lt;- seq(1, 30 * pi, length.out = 1e4)\n  diff &lt;- (fixed_radius - cycling_radius)\n  ratio &lt;- diff / cycling_radius\n  x &lt;- diff * cos(t) + cos(t * ratio)\n  y &lt;- diff * sin(t) - sin(t * ratio)\n  tibble(x = x, y = y, fixed_radius = fixed_radius, cycling_radius = cycling_radius)\n}\n\nplot_spirographs &lt;- function(points) {\n  label &lt;- \"fixed_radius = %s, cycling_radius = %s\"\n  points$parameters &lt;- sprintf(label, points$fixed_radius, points$cycling_radius)\n  ggplot(points) +\n    geom_point(aes(x = x, y = y, color = parameters), size = 0.1) +\n    facet_wrap(~parameters) +\n    theme_gray(16) +\n    guides(color = \"none\")\n}\n\nlist(\n  tar_target(\n    points,\n    bind_rows(\n      spirograph_points(3, 9),\n      spirograph_points(7, 2)\n    ) %&gt;%\n      group_by(fixed_radius, cycling_radius) %&gt;%\n      tar_group(),\n    iteration = \"group\"\n  ),\n  tar_target(\n    single_plot,\n    plot_spirographs(points),\n    pattern = map(points),\n    iteration = \"list\"\n  )\n)\n\n\ntar_make()\n#&gt; + points dispatched\n#&gt; ✔ points completed [55ms, 309.26 kB]\n#&gt; + single_plot declared [2 branches]\n#&gt; ✔ single_plot completed [106ms, 4.86 MB]\n#&gt; ✔ ended pipeline [1.6s, 3 completed, 0 skipped]\n\n\ntar_read(single_plot, branches = 1)\n#&gt; $single_plot_f078249e1178261a\n\n\n\n\n\n\n\n\nThe tar_group_by() function in tarchetypes makes this branching easier. Using tar_group_by(), the pipeline condenses down to:\n\nlist(\n  tar_group_by(\n    points,\n    bind_rows(\n      spirograph_points(3, 9),\n      spirograph_points(7, 2)\n    ),\n    fixed_radius,\n    cycling_radius\n  ),\n  tar_target(\n    single_plot,\n    plot_spirographs(points),\n    pattern = map(points),\n    iteration = \"list\"\n  )\n)\n\nFor similar functions that branch across row groups, visit https://docs.ropensci.org/tarchetypes/reference/index.html#dynamic-grouped-data-frames.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#branching-over-files",
    "href": "dynamic.html#branching-over-files",
    "title": "15  Dynamic branching",
    "section": "15.8 Branching over files",
    "text": "15.8 Branching over files\nDynamic branching over files is tricky. A target with format = \"file\" treats the entire set of files as an irreducible bundle. That means in order to branch over files downstream, each file must already have its own branch. Here is a pipeline that begins with spirograph data files and loads each into a different dynamic branch.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlist(\n  tar_target(paths, c(\"spirograph_dataset_1.csv\", \"spirograph_dataset_1.csv\")),\n  tar_target(files, paths, format = \"file\", pattern = map(paths)),\n  tar_target(data, read_csv(files), pattern = map(files))\n)\n\nThe tar_files() function from the tarchetypes package is shorthand for the first two targets above.\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlist(\n  tar_files(files, c(\"spirograph_dataset_1.csv\", \"spirograph_dataset_1.csv\")),\n  tar_target(data, read_csv(files), pattern = map(files))\n)",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#performance-and-batching",
    "href": "dynamic.html#performance-and-batching",
    "title": "15  Dynamic branching",
    "section": "15.9 Performance and batching",
    "text": "15.9 Performance and batching\nDynamic branching makes it easy to create many targets. Unfortunately, if the number of targets exceeds several hundred thousand, overhead may build up and the package may slow down. Temporary workarounds can avoid overhead in specific cases: for example, the shortcut argument of tar_make(), and choosing a pattern like slice() or head() instead of a full map(). But to minimize overhead at scale, it is better to accomplish the same amount of work with a fewer number of targets. In other words, do more work inside each dynamic branch.\nBatching is particularly useful to reduce overhead. In batching, each dynamic branch performs multiple computations instead of just one. The tarchetypes package supports several general-purpose functions that do batching automatically: most notably tar_rep() and tar_map_rep() for simulation studies and tar_group_count(), tar_group_size(), and tar_group_select() for batching over the rows of a data frame.\nThe packages in the R Targetopia support batching for specific use cases. For example, in stantargets, tar_stan_mcmc_rep_summary() dynamically branches over batches of simulated datasets for Stan models.\nThe targets-stan repository has an example of custom batching implemented from scratch. The goal of the pipeline is to validate a Bayesian model by simulating thousands of dataset, analyzing each with a Bayesian model, and assessing the overall accuracy of the inference. Rather than define a target for each dataset in model, the pipeline breaks up the work into batches, where each batch has multiple datasets or multiple analyses. Here is a version of the pipeline with 40 batches and 25 simulation reps per batch (1000 reps total in a pipeline of 82 targets).\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlist(\n  tar_target(model_file, compile_model(\"stan/model.stan\"), format = \"file\"),\n  tar_target(index_batch, seq_len(40)),\n  tar_target(index_sim, seq_len(25)),\n  tar_target(\n    data_continuous,\n    purrr::map_dfr(index_sim, ~simulate_data_continuous()),\n    pattern = map(index_batch)\n  ),\n  tar_target(\n    fit_continuous,\n    map_sims(data_continuous, model_file = model_file),\n    pattern = map(data_continuous)\n  )\n)",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "dynamic.html#footnotes",
    "href": "dynamic.html#footnotes",
    "title": "15  Dynamic branching",
    "section": "",
    "text": "The pipeline uses code borrowed from the spiro package. The code in the spirograph_points() function is adapted from https://github.com/wjschne/spiro/blob/87f73ec37ceb0a7a9d09856ada8ae28d587a2ebd/R/spirograph.R under the CC0 1.0 Universal license: https://github.com/wjschne/spiro/blob/87f73ec37ceb0a7a9d09856ada8ae28d587a2ebd/LICENSE.md↩︎\nSee also https://books.ropensci.org/targets/debugging.html#workspaces.↩︎",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Dynamic branching</span>"
    ]
  },
  {
    "objectID": "static.html",
    "href": "static.html",
    "title": "16  Static branching",
    "section": "",
    "text": "16.1 Branching\nSometimes, a pipeline contains more targets than a user can comfortably type by hand. For projects with hundreds of targets, branching can make the _targets.R file more concise and easier to read and maintain.\ntargets supports two types of branching: dynamic branching and static branching. Some projects are better suited to dynamic branching, while others benefit more from static branching or a combination of both. Here is a short list of tradeoffs.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#branching",
    "href": "static.html#branching",
    "title": "16  Static branching",
    "section": "",
    "text": "TipPerformance\n\n\n\nBranched pipelines can be computationally demanding. See the performance chapter for options, settings, and other choices to optimize and monitor large pipelines.\n\n\n\n\n\n\n\n\n\n\n\nDynamic\nStatic\n\n\n\n\nPipeline creates new targets at runtime.\nAll targets defined in advance.\n\n\nCryptic target names.\nFriendly target names.\n\n\nScales to hundreds of thousands of branches.\nDoes not scale as easily, especially with tar_visnetwork() graphs\n\n\nNo metaprogramming required.\nFamiliarity with metaprogramming is helpful.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#when-to-use-static-branching",
    "href": "static.html#when-to-use-static-branching",
    "title": "16  Static branching",
    "section": "16.2 When to use static branching",
    "text": "16.2 When to use static branching\nStatic branching is the act of defining a group of targets in bulk before the pipeline starts. Whereas dynamic branching uses last-minute dependency data to define the branches, static branching uses metaprogramming to modify the code of the pipeline up front. Whereas dynamic branching excels at creating a large number of very similar targets, static branching is most useful for smaller number of heterogeneous targets. Some users find it more convenient because they can use tar_manifest() and tar_visnetwork() to check the correctness of static branching before launching the pipeline.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#map",
    "href": "static.html#map",
    "title": "16  Static branching",
    "section": "16.3 Map",
    "text": "16.3 Map\ntar_map() from the tarchetypes package creates copies of existing target definition objects, where each new command is a variation on the original. In the example below, we have a data analysis workflow that iterates over datasets and analysis methods. The values data frame has the operational parameters of each data analysis, and tar_map() creates one new target per row.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\nvalues &lt;- tibble(\n  method_function = rlang::syms(c(\"method1\", \"method2\")),\n  data_source = c(\"NIH\", \"NIAID\")\n)\ntargets &lt;- tar_map(\n  values = values,\n  tar_target(analysis, method_function(data_source, reps = 10)),\n  tar_target(summary, summarize_analysis(analysis, data_source))\n)\nlist(targets)\n\n\ntar_manifest()\n#&gt; # A tibble: 4 × 3\n#&gt;   name                   command                                     description\n#&gt;   &lt;chr&gt;                  &lt;chr&gt;                                       &lt;chr&gt;      \n#&gt; 1 analysis_method2_NIAID \"method2(\\\"NIAID\\\", reps = 10)\"             method2 NI…\n#&gt; 2 analysis_method1_NIH   \"method1(\\\"NIH\\\", reps = 10)\"               method1 NIH\n#&gt; 3 summary_method2_NIAID  \"summarize_analysis(analysis_method2_NIAID… method2 NI…\n#&gt; 4 summary_method1_NIH    \"summarize_analysis(analysis_method1_NIH, … method1 NIH\n\n\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\nFor shorter target names, use the names argument of tar_map(). And for more combinations of settings, use tidyr::expand_grid() on values.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tidyr)\nvalues &lt;- expand_grid( # Use all possible combinations of input settings.\n  method_function = rlang::syms(c(\"method1\", \"method2\")),\n  data_source = c(\"NIH\", \"NIAID\")\n)\ntargets &lt;- tar_map(\n  values = values,\n  names = \"data_source\", # Select columns from `values` for target names.\n  tar_target(analysis, method_function(data_source, reps = 10)),\n  tar_target(summary, summarize_analysis(analysis, data_source))\n)\nlist(targets)\n\nIt is extra important to run tar_manifest() to check that tar_map() generates the right R code for the targets. Sometimes, the metaprogramming may not produce the desired commands on your first try.\n\ntar_manifest()\n#&gt; # A tibble: 8 × 3\n#&gt;   name             command                                           description\n#&gt;   &lt;chr&gt;            &lt;chr&gt;                                             &lt;chr&gt;      \n#&gt; 1 analysis_NIAID_1 \"method2(\\\"NIAID\\\", reps = 10)\"                   method2 NI…\n#&gt; 2 analysis_NIAID   \"method1(\\\"NIAID\\\", reps = 10)\"                   method1 NI…\n#&gt; 3 analysis_NIH_1   \"method2(\\\"NIH\\\", reps = 10)\"                     method2 NIH\n#&gt; 4 analysis_NIH     \"method1(\\\"NIH\\\", reps = 10)\"                     method1 NIH\n#&gt; 5 summary_NIAID_1  \"summarize_analysis(analysis_NIAID_1, \\\"NIAID\\\")\" method2 NI…\n#&gt; 6 summary_NIAID    \"summarize_analysis(analysis_NIAID, \\\"NIAID\\\")\"   method1 NI…\n#&gt; 7 summary_NIH_1    \"summarize_analysis(analysis_NIH_1, \\\"NIH\\\")\"     method2 NIH\n#&gt; 8 summary_NIH      \"summarize_analysis(analysis_NIH, \\\"NIH\\\")\"       method1 NIH\n\nAnd of course, check the dependency graph to ensure the pipeline is properly connected. If tar_map() generates a lot of targets, the graph may render slowly or look too cumbersome. If that happens, choose a small subset of rows of values for tar_map() and then try again on the smaller pipeline.\n\n# You may need to zoom out on this interactive graph to see all 8 targets.\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\n\n16.3.1 Limitations\ntar_map() generates R expressions to serve as commands in other targets. When it substitutes an element from values, it needs a way to transform the element into valid R code. For elements even a little bit complicated, especially nested data frames and objects with attributes, this is not always possible. For these complicated elements, it is best to use quote() to work with the underlying expressions instead of the objects themselves. See https://github.com/ropensci/tarchetypes/discussions/105 for an example.",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#dynamic-within-static-branching",
    "href": "static.html#dynamic-within-static-branching",
    "title": "16  Static branching",
    "section": "16.4 Dynamic-within-static branching",
    "text": "16.4 Dynamic-within-static branching\nYou can even combine together static and dynamic branching. The static tar_map() is an excellent outer layer on top of targets with patterns. The following is a sketch of a pipeline that runs each of two data analysis methods 10 times, once per random seed. Static branching iterates over the method functions, while dynamic branching iterates over the seeds. tar_map() creates new patterns as well as new commands. So below, the summary methods map over the analysis methods both statically and dynamically.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\nrandom_seed_target &lt;- tar_target(random_seed, seq_len(10))\ntargets &lt;- tar_map(\n  values = tibble(method_function = rlang::syms(c(\"method1\", \"method2\"))),\n  tar_target(\n    analysis,\n    method_function(\"NIH\", seed = random_seed),\n    pattern = map(random_seed)\n  ),\n  tar_target(\n    summary,\n    summarize_analysis(analysis),\n    pattern = map(analysis)\n  )\n)\nlist(random_seed_target, targets)\n\n\ntar_manifest()\n#&gt; # A tibble: 5 × 4\n#&gt;   name             command                                pattern    description\n#&gt;   &lt;chr&gt;            &lt;chr&gt;                                  &lt;chr&gt;      &lt;chr&gt;      \n#&gt; 1 random_seed      \"seq_len(10)\"                          &lt;NA&gt;       &lt;NA&gt;       \n#&gt; 2 analysis_method1 \"method1(\\\"NIH\\\", seed = random_seed)\" map(rando… method1    \n#&gt; 3 analysis_method2 \"method2(\\\"NIH\\\", seed = random_seed)\" map(rando… method2    \n#&gt; 4 summary_method1  \"summarize_analysis(analysis_method1)\" map(analy… method1    \n#&gt; 5 summary_method2  \"summarize_analysis(analysis_method2)\" map(analy… method2\n\n\ntar_visnetwork(targets_only = TRUE)",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#combine",
    "href": "static.html#combine",
    "title": "16  Static branching",
    "section": "16.5 Combine",
    "text": "16.5 Combine\ntar_combine() from the tarchetypes package creates a new target to aggregate the results of upstream targets. In the simple example below, our combined target simply aggregates the rows returned from two other targets.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\noptions(crayon.enabled = FALSE)\ntarget1 &lt;- tar_target(head, head(mtcars, 1))\ntarget2 &lt;- tar_target(tail, tail(mtcars, 1))\ntarget3 &lt;- tar_combine(combined_target, target1, target2)\nlist(target1, target2, target3)\n\n\ntar_manifest()\n#&gt; # A tibble: 3 × 2\n#&gt;   name            command                                                       \n#&gt;   &lt;chr&gt;           &lt;chr&gt;                                                         \n#&gt; 1 head_mtcars     head(mtcars, 1)                                               \n#&gt; 2 tail_mtcars     tail(mtcars, 1)                                               \n#&gt; 3 combined_target vctrs::vec_c(head_mtcars = head_mtcars, tail_mtcars = tail_mt…\n\n\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\n\ntar_make()\n#&gt; + head_mtcars dispatched\n#&gt; ✔ head_mtcars completed [1ms, 215 B]\n#&gt; + tail_mtcars dispatched\n#&gt; ✔ tail_mtcars completed [1ms, 221 B]\n#&gt; + combined_target dispatched\n#&gt; ✔ combined_target completed [0ms, 276 B]\n#&gt; ✔ ended pipeline [154ms, 3 completed, 0 skipped]\n\n\ntar_read(combined_target)\n#&gt;             mpg cyl disp  hp drat   wt  qsec vs am gear carb\n#&gt; Mazda RX4  21.0   6  160 110 3.90 2.62 16.46  0  1    4    4\n#&gt; Volvo 142E 21.4   4  121 109 4.11 2.78 18.60  1  1    4    2\n\nTo use tar_combine() and tar_map() together in more complicated situations, you may need to supply unlist = FALSE to tar_map(). That way, tar_map() will return a nested list of target definition objects, and you can combine the ones you want. The pipeline extends our previous tar_map() example by combining just the summaries, omitting the analyses from tar_combine(). Also note the use of bind_rows(!!!.x) below. This is how you supply custom code to combine the return values of other targets. .x is a placeholder for the return values, and !!! is the “unquote-splice” operator from the rlang package.\n\n# _targets.R file:\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\nrandom_seed &lt;- tar_target(random_seed, seq_len(10))\nmapped &lt;- tar_map(\n  unlist = FALSE, # Return a nested list from tar_map()\n  values = tibble(method_function = rlang::syms(c(\"method1\", \"method2\"))),\n  tar_target(\n    analysis,\n    method_function(\"NIH\", seed = random_seed),\n    pattern = map(random_seed)\n  ),\n  tar_target(\n    summary,\n    summarize_analysis(analysis),\n    pattern = map(analysis)\n  )\n)\ncombined &lt;- tar_combine(\n  combined_summaries,\n  mapped[[\"summary\"]],\n  command = dplyr::bind_rows(!!!.x, .id = \"method\")\n)\nlist(random_seed, mapped, combined)\n\n\ntar_manifest()\n#&gt; Warning message:\n#&gt; Targets and globals must have unique names. Ignoring global objects that conflict with target names: random_seed. Warnings like this one are important, but if you must suppress them, you can do so with Sys.setenv(TAR_WARN = \"false\").\n#&gt; # A tibble: 6 × 4\n#&gt;   name               command                                 pattern description\n#&gt;   &lt;chr&gt;              &lt;chr&gt;                                   &lt;chr&gt;   &lt;chr&gt;      \n#&gt; 1 random_seed        \"seq_len(10)\"                           &lt;NA&gt;    &lt;NA&gt;       \n#&gt; 2 analysis_method1   \"method1(\\\"NIH\\\", seed = random_seed)\"  map(ra… method1    \n#&gt; 3 analysis_method2   \"method2(\\\"NIH\\\", seed = random_seed)\"  map(ra… method2    \n#&gt; 4 summary_method1    \"summarize_analysis(analysis_method1)\"  map(an… method1    \n#&gt; 5 summary_method2    \"summarize_analysis(analysis_method2)\"  map(an… method2    \n#&gt; 6 combined_summaries \"dplyr::bind_rows(summary_method1 = su… &lt;NA&gt;    &lt;NA&gt;\n\n\ntar_visnetwork(targets_only = TRUE)\n#&gt; Warning message:\n#&gt; Targets and globals must have unique names. Ignoring global objects that conflict with target names: random_seed. Warnings like this one are important, but if you must suppress them, you can do so with Sys.setenv(TAR_WARN = \"false\").",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#metaprogramming",
    "href": "static.html#metaprogramming",
    "title": "16  Static branching",
    "section": "16.6 Metaprogramming",
    "text": "16.6 Metaprogramming\nCustom metaprogramming is a more flexible alternative to tar_map() and tar_combine(). tar_eval() from tarchetypes accepts an arbitrary expression and iteratively plugs in symbols. Below, we use it to branch over datasets.\n\n# _targets.R\nlibrary(rlang)\nlibrary(targets)\nlibrary(tarchetypes)\nstring &lt;- c(\"gapminder\", \"who\", \"imf\")\nsymbol &lt;- syms(string)\ntar_eval(\n  tar_target(symbol, get_data(string)),\n  values = list(string = string, symbol = symbol)\n)\n\ntar_eval() has fewer guardrails than tar_map() or tar_combine(), so tar_manifest() is especially important for checking the correctness of your metaprogramming.\n\ntar_manifest(fields = command)\n#&gt; # A tibble: 3 × 2\n#&gt;   name      command                  \n#&gt;   &lt;chr&gt;     &lt;chr&gt;                    \n#&gt; 1 imf       \"get_data(\\\"imf\\\")\"      \n#&gt; 2 gapminder \"get_data(\\\"gapminder\\\")\"\n#&gt; 3 who       \"get_data(\\\"who\\\")\"",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "static.html#hooks",
    "href": "static.html#hooks",
    "title": "16  Static branching",
    "section": "16.7 Hooks",
    "text": "16.7 Hooks\nHooks are supported in tarchetypes version 0.2.0 and above, and they allow you to prepend or wrap code in multiple targets at a time. For example, tar_hook_before() is a robust way to invoke the conflicted package to resolve namespace conflicts that works with distributed computing and does not require a project-level .Rprofile file.\n\n# _targets.R file\nlibrary(tarchetypes)\nlibrary(magrittr)\ntar_option_set(packages = c(\"conflicted\", \"dplyr\"))\nsource(\"R/functions.R\")\nlist(\n  tar_target(data, get_time_series_data()),\n  tar_target(analysis1, analyze_months(data)),\n  tar_target(analysis2, analyze_weeks(data))\n) %&gt;%\n  tar_hook_before(\n    hook = conflicted_prefer(\"filter\", \"dplyr\"),\n    names = starts_with(\"analysis\")\n  )\n\n\n# R console\ntargets::tar_manifest(fields = command)\n#&gt; # A tibble: 3 × 2\n#&gt;   name      command                                                             \n#&gt;   &lt;chr&gt;     &lt;chr&gt;                                                               \n#&gt; 1 data      \"get_time_series_data()\"                                            \n#&gt; 2 analysis1 \"{\\n     conflicted_prefer(\\\"filter\\\", \\\"dplyr\\\")\\n     analyze(dat…\n#&gt; 3 analysis2 \"{\\n     conflicted_prefer(\\\"filter\\\", \\\"dplyr\\\")\\n     analyze(dat…\n\nSimilarly, tar_hook_outer() wraps expressions around target commands, and tar_hook_inner() wraps expressions around target dependencies. These hooks could potentially help encrypt targets before storage in _targets/ and decrypt targets before retrieval, as demonstrated in the sketch below.\nData security is the sole responsibility of the user and not the responsibility of targets, tarchetypes, or related pipeline packages. You as the user are responsible for validating your own target specifications and custom code and applying additional security precautions as appropriate for the situation.\n\n# _targets.R file\nlibrary(tarchetypes)\nlibrary(magrittr)\nlist(\n  tar_target(data1, get_data1()),\n  tar_target(data2, get_data2()),\n  tar_target(analysis, analyze(data1, data2))\n) %&gt;%\n  tar_hook_outer(encrypt(.x, threads = 2)) %&gt;%\n  tar_hook_inner(decrypt(.x))\n\n\n# R console\ntargets::tar_manifest(fields = command)\n#&gt; # A tibble: 3 × 2\n#&gt;   name     command                                                      \n#&gt;   &lt;chr&gt;    &lt;chr&gt;                                                        \n#&gt; 1 data1    encrypt(get_data1(), threads = 2)                            \n#&gt; 2 data2    encrypt(get_data2(), threads = 2)                            \n#&gt; 3 analysis encrypt(analyze(decrypt(data1), decrypt(data2)), threads = 2)",
    "crumbs": [
      "Branching",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Static branching</span>"
    ]
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "Appendix A — High-performance computing (the old way)",
    "section": "",
    "text": "A.1 Clustermq\ntargets supports parallel and distributed computing with the tar_make_clustermq() and tar_make_future() functions. These functions are like tar_make(), but they allow multiple targets to run simultaneously over parallel workers.1 These workers can be processes on your local machine, or they can be jobs on a computing cluster. The main process automatically sends a target to a worker as soon as\nConsider the following sketch of a pipeline.\nWhen we run this pipeline with high-performance computing, targets automatically knows to wait for data to finish running before moving on to the other targets. Once data is finished, it moves on to targets fast_fit and slow_fit. If fast_fit finishes before slow_fit, target plot_1 begins even as slow_fit is still running.2\nThe following sections cover the implementation details of parallel and distributed pipelines in targets.\ntar_make_clustermq() uses the clustermq package, and prior familiarity with clustermq is extremely helpful for configuring targets and diagnosing errors. So before you use tar_make_clustermq(), please read the documentation at https://mschubert.github.io/clustermq/ and try out clustermq directly. If you plan to use a scheduler like SLURM or SGE, please configure and experiment with clustermq on your scheduler without targets. And if you later experience issues with tar_make_clustermq(), try to isolate the problem by creating a reproducible example that uses clustermq and not targets. Peeling back layers can help isolate problems and point toward specific solutions, and targets is usually one of the outer layers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>High-performance computing (the old way)</span>"
    ]
  },
  {
    "objectID": "hpc.html#clustermq",
    "href": "hpc.html#clustermq",
    "title": "Appendix A — High-performance computing (the old way)",
    "section": "",
    "text": "A.1.1 Persistent workers\ntar_make_clustermq() uses persistent workers. A persistent worker is an R process that launches early in the pipeline and stays running until the whole pipeline starts to wind down. A persistent worker usually runs multiple targets during its lifecycle, and it is not possible to precisely predict in advance which targets will be assigned to which workers. The video clip below visualizes the concept.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.1.2 Clustermq installation\nPersistent workers require the clustermq R package, which in turn requires ZeroMQ. Please refer to the clustermq installation guide for specific instructions.\n\n\nA.1.3 Automatic configuration with use_targets()\nIf clustermq is installed, then use_targets() function configures clustermq automatically. use_targets() tries to detect the cluster you are using and write the appropriate _targets.R settings clustermq configuration files commensurate with your system resources. On most systems, this should allow you you to run tar_make_clustermq() with no extra configuration. If so, you can skip the rest of the clustermq section. If not, read on to learn how to configure clustermq for targets.\n\n\nA.1.4 Clustermq local configuration\nTo utilize parallel computing on your local machine, set the clustermq.scheduler option to \"multicore\" or \"multiprocess\" in your _targets.R file. See the configuration instructions for details.\n\n# _targets.R\noptions(clustermq.scheduler = \"multiprocess\")\nlist(\n  tar_target(data, get_data()),\n  tar_target(fast_fit, fit_small_model(data)),\n  tar_target(slow_fit, fit_slow_model(data)),\n  tar_target(plot_1, make_plot(fast_fit)),\n  tar_target(plot_2, make_plot(slow_fit))\n)\n\nThen, run tar_make_clustermq() with the appropriate number of workers. These workers are local R processes that run concurrently to run the outdated targets in the pipeline. The workers launch as soon as there is an outdated target with deployment = \"worker\", and they continue running until there are no more targets to run.\n\n# R console\ntar_make_clustermq(workers = 2)\n\n\n\nA.1.5 Clustermq remote configuration\nA cluster is a collection of multiple computers that work together to run computationally demanding workloads. A cluster is capable of running far more tasks simultaneously than a local workstation like a PC or laptop. When configured for a cluster, tar_make_clustermq() uses clustermq to run persistent workers as long-running distributed jobs. To manually set the configuration,\n\nChoose a scheduler listed here that corresponds to your cluster’s resource manager.\nCreate a template file that configures the computing requirements and other settings for the cluster.\n\nSupply the scheduler option and template file to the clustermq.scheduler and clustermq.template global options in your target script file (default: _targets.R).\n\n# _targets.R\noptions(clustermq.scheduler = \"sge\", clustermq.template = \"sge.tmpl\")\nlist(\n  tar_target(data, get_data()),\n  tar_target(fast_fit, fit_small_model(data)),\n  tar_target(slow_fit, fit_slow_model(data)),\n  tar_target(plot_1, make_plot(fast_fit)),\n  tar_target(plot_2, make_plot(slow_fit))\n)\n\nAbove, sge_tmpl refers to a template file like the one below.\n## From https://github.com/mschubert/clustermq/wiki/SGE\n#$ -N {{ job_name }}  # Worker name.\n#$ -t 1-{{ n_jobs }}  # Submit workers as an array.\n#$ -j y               # Combine stdout and stderr into one worker log file.\n#$ -o /dev/null       # Worker log files.\n#$ -cwd               # Use project root as working directory.\n#$ -V                 # Use environment variables.\nmodule load R/3.6.3   # Needed if R is an environment module on the cluster.\nCMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker(\"{{ main }}\")' # Leave alone.\nThen, run tar_make_clustermq() as before. Instead of running locally, the workers will run as jobs on the cluster. Because they are persistent workers, they launch as soon as there is an outdated target with deployment = \"worker\", and they continue running until they complete their current targets and there are no more targets to assign.\n\n# R console\ntar_make_clustermq(workers = 2)\n\nSee the examples linked from here to see how this setup works in real-world projects.\n\n\nA.1.6 Clustermq template file configuration\nIn addition to configuration options hard-coded in the template file, you can supply custom computing resources with the resources argument of tar_option_set(). As an example, let’s use a wildcard for the number of cores per worker on an SGE cluster. In the template file, supply { num_cores } wildcard to the -pe smp flag.\n#$ -pe smp {{ num_cores }} # Number of cores per worker\n#$ -N {{ job_name | 1 }}\n#$ -t 1-{{ n_jobs }}\n#$ -j y\n#$ -o /dev/null\n#$ -cwd\n#$ -V\nmodule load R/3.6.3\nCMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker(\"{{ main }}\")'\nThen, supply the value of num_cores to the resources option from within the target script file (default: _targets.R). In older version of targets, resources was a named list. In targets 0.5.0.9000 and above, please create the resources argument with helpers tar_resources() and tar_resources_clustermq().\n\n# _targets.R\n# With older versions of targets:\n# tar_option_set(resources = list(num_cores = 2))\n# With targets &gt;= 0.5.0.9000:\ntar_option_set(\n  resources = tar_resources(\n    clustermq = tar_resources_clustermq(template = list(num_cores = 2))\n  )\n)\nlist(\n  tar_target(...),\n  ... # more targets\n)\n\nFinally, call tar_make_clustermq() normally.\n\n# R console\ntar_make_clustermq(workers = 2)\n\nThis particular use case comes up when you have custom parallel computing within targets and need to take advantage of multiple cores.\n\n\n\n\n\n\nWarningclustermq resources\n\n\n\nRemember: with tar_make_clustermq(), the workers are persistent, which means there is not a one-to-one correspondence between workers and targets. In the resources arguemnt of tar_option_set(), the clustermq-specific resources apply to the workers, not the targets directly. So in the resources argument of tar_target(), any clustermq-specific will be ignored. tar_option_set() is the only way to set clustermq resources.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>High-performance computing (the old way)</span>"
    ]
  },
  {
    "objectID": "hpc.html#future",
    "href": "hpc.html#future",
    "title": "Appendix A — High-performance computing (the old way)",
    "section": "A.2 Future",
    "text": "A.2 Future\ntar_make_future() uses the future package, and prior familiarity with future is extremely helpful for configuring targets and diagnosing errors. So before you use tar_make_future(), please read the documentation at https://future.futureverse.org/ and try out future directly, ideally with backends like future.callr and possibly future.batchtools. If you plan to use a scheduler like SLURM or SGE, please configure and experiment with future on your scheduler without targets. And if you later experience issues with tar_make_future(), try to isolate the problem by creating a reproducible example that uses future and not targets. Same goes for future.batchtools if applicable. Peeling back layers can help isolate problems and point toward specific solutions, and targets is usually one of the outer layers.\n\nA.2.1 Transient workers\ntar_make_future() runs transient workers. That means each target gets its own worker which initializes when the target begins and terminates when the target ends. The following video clip demonstrates the concept.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.2.2 Automatic configuration with use_targets()\nIf future and future.batchtools are installed, then use_targets() function configures future automatically. use_targets() tries to detect the cluster you are using and write the appropriate _targets.R settings, and if appropriate, the batchtools configuration file commensurate with your system resources. On most systems, this should allow you you to run tar_make_future() with no extra configuration. If so, you can skip the rest of the future section. If not, read on to learn how to configure future for targets.\n\n\nA.2.3 Future installation\nInstall the future package.\n\ninstall.packages(\"future\")\n\nIf you intend to use a cluster, be sure to install the future.batchtools package too.\n\ninstall.packages(\"future.batchtools\")\n\nThe future ecosystem contains even more packages that extend future’s parallel computing functionality, such as future.callr.\n\ninstall.packages(\"future.callr\")\n\n\n\nA.2.4 Future locally\nIf you ar manually configuring future/future.batchtools for your pipeline, declare a future plan in your target script file (default: _targets.R). The callr plan from the future.callr package is recommended.3 It is crucial that future::plan() is called in the target script file itself - defining a plan interactively before invoking tar_make_future() does not leverage the future package.\n\n# _targets.R\nlibrary(future)\nlibrary(future.callr)\nplan(callr)\nlist(\n  tar_target(data, get_data()),\n  tar_target(fast_fit, fit_small_model(data)),\n  tar_target(slow_fit, fit_slow_model(data)),\n  tar_target(plot_1, make_plot(fast_fit)),\n  tar_target(plot_2, make_plot(slow_fit))\n)\n\nThen, run tar_make_future() with the desired number of workers. Here, the workers argument specifies the maximum number of transient workers to allow at a given time. Some future plans also have optional workers arguments that set their own caps.\n\n# R console\ntar_make_future(workers = 2)\n\n\n\nA.2.5 Future remotely\nTo run transient workers on a cluster, first install the future.batchtools package. Then, set one of these plans in your target script file (default: _targets.R).\n\n# _targets.R\nlibrary(future)\nlibrary(future.batchtools)\nplan(batchtools_sge, template = \"sge.tmpl\")\nlist(\n  tar_target(data, get_data()),\n  tar_target(fast_fit, fit_small_model(data)),\n  tar_target(slow_fit, fit_slow_model(data)),\n  tar_target(plot_1, make_plot(fast_fit)),\n  tar_target(plot_2, make_plot(slow_fit))\n)\n\nHere, our template file sge.tmpl is configured for batchtools.\n#!/bin/bash\n#$ -cwd               # Run in the current working directory.\n#$ -j y               # Direct stdout and stderr to the same file.\n#$ -o &lt;%= log.file %&gt; # log file\n#$ -V                 # Use environment variables.\n#$ -N &lt;%= job.name %&gt; # job name\nmodule load R/3.6.3   # Uncomment and adjust if R is an environment module.\nRscript -e 'batchtools::doJobCollection(\"&lt;%= uri %&gt;\")' # Leave alone.\nexit 0 # Leave alone.\n\n\nA.2.6 Future configuration\nThe tar_target(), tar_target_raw(), and tar_option_set() functions accept a resources argument.4 For example, if our batchtools template file has a wildcard for the number of cores for a job,\n#!/bin/bash\n#$ -pe smp &lt;%= resources[[\"num_cores\"]] | 1 %&gt; # Wildcard for cores per job.\n#$ -cwd\n#$ -j y\n#$ -o &lt;%= log.file %&gt;\n#$ -V\n#$ -N &lt;%= job.name %&gt;\nmodule load R/3.6.3\nRscript -e 'batchtools::doJobCollection(\"&lt;%= uri %&gt;\")'\nexit 0\nthen you can set the number of cores for an individual target using a target-specific future plan. In the case below, maybe the slow model needs 2 cores to run fast enough. Because of the resources[[\"num_cores\"]] placeholder in the above template file, we can control the number of cores in each target through its local plan.5\n\n# _targets.R\nlibrary(future)\nlibrary(future.batchtools)\nplan(batchtools_sge, template = \"sge.tmpl\")\nlist(\n  tar_target(data, get_data()),\n  tar_target(fast_fit, fit_small_model(data)),\n  # With older version of targets:\n  # tar_target(slow_fit, fit_slow_model(data), resources = list(num_cores = 2)),\n  # With targets &gt;= 0.5.0.9000:\n  tar_target(\n    slow_fit,\n    fit_slow_model(data),\n    resources = tar_resources(\n      future = tar_resources_future(\n        plan = tweak(\n          batchtools_sge,\n          template = \"sge.tmpl\",\n          resources = list(num_cores = 2)\n        )\n      )\n    )\n  ),\n  tar_target(plot_1, make_plot(fast_fit)),\n  tar_target(plot_2, make_plot(slow_fit))\n)\n\nThen, run tar_make_future() as usual.\n\n# R console\ntar_make_future(workers = 2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>High-performance computing (the old way)</span>"
    ]
  },
  {
    "objectID": "hpc.html#advanced",
    "href": "hpc.html#advanced",
    "title": "Appendix A — High-performance computing (the old way)",
    "section": "A.3 Advanced",
    "text": "A.3 Advanced\nFunctions tar_target(), tar_target_raw(), and tar_option_set() support advanced configuration options for heavy-duty pipelines that require high-performance computing.\n\ndeployment: With the deployment argument, you can choose to run some targets locally on the main process instead of on a high-performance computing worker. This options is suitable for lightweight targets such as R Markdown reports where runtime is quick and a cluster would be excessive.\nmemory: Choose whether to retain a target in memory or remove it from memory whenever it is not needed at the moment. This is a tradeoff between memory consumption and storage read speeds, and like all of the options listed here, you can set it on a target-by-target basis. The default settings consume a lot of memory to avoid frequently reading from storage. To keep memory usage down to a minimum, set memory = \"transient\" and garbage_collection = TRUE in tar_target() or tar_option_set(). For cloud-based file targets such as format = \"aws_file\", this memory policy applies to temporary local copies of the file in _targets/scratch/: \"persistent\" means they remain until the end of the pipeline, and \"transient\" means they get deleted from the file system as soon as possible. The former conserves bandwidth, and the latter conserves local storage.\ngarbage_collection: Choose whether to run base::gc() just before running the target.\nstorage: Choose whether the parallel workers or the main process is responsible for saving the target’s value. For slow network file systems on clusters, storage = \"main\" is often faster for small numbers of targets. For large numbers of targets or low-bandwidth connections between the main and workers, storage = \"worker\" is often faster. Always choose storage = \"main\" if the workers do not have access to the file system with the _targets/ data store.\nretrieval: Choose whether the parallel workers or the main process is responsible for reading dependency targets from disk. Should usually be set to whatever you choose for storage (default). Always choose retrieval = \"main\" if the workers do not have access to the file system with the _targets/ data store.\nformat: If your pipeline has large computation, it may also have large data. Consider setting the format argument to help targets store and retrieve your data faster.\nerror: Set error to \"continue\" to let the rest of the pipeline keep running even if a target encounters an error.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>High-performance computing (the old way)</span>"
    ]
  },
  {
    "objectID": "hpc.html#cloud-computing",
    "href": "hpc.html#cloud-computing",
    "title": "Appendix A — High-performance computing (the old way)",
    "section": "A.4 Cloud computing",
    "text": "A.4 Cloud computing\nRight now, targets does not have built-in cloud-based distributed computing support. However, future development plans include seamless integration with AWS Batch. As a temporary workaround, it is possible to deploy a burstable SLURM cluster using AWS ParallelCluster and leverage targets’ existing support for traditional schedulers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>High-performance computing (the old way)</span>"
    ]
  },
  {
    "objectID": "hpc.html#footnotes",
    "href": "hpc.html#footnotes",
    "title": "Appendix A — High-performance computing (the old way)",
    "section": "",
    "text": "Please use tar_make_clustermq() or tar_make_future() instead of multiple concurrent calls to tar_make(). The former is the proper way to use parallel computing in targets, and the latter will probably break the data store.↩︎\nUnlike drake, targets applies this behavior not only to stem targets, but also to branches of patterns.↩︎\nSome alternative local future plans are listed here.↩︎\nThe resources of tar_target() defaults to tar_option_get(\"resources\"). You can set the default value for all targets using tar_option_set().↩︎\nIn older version of targets, resources was a named list. In targets version 0.5.0.9000 and above, please create the resources argument with helpers tar_resources() and tar_resources_future().↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>High-performance computing (the old way)</span>"
    ]
  },
  {
    "objectID": "drake.html",
    "href": "drake.html",
    "title": "Appendix B — What about drake?",
    "section": "",
    "text": "B.1 Why is drake superseded?\ntargets is the successor of drake, an older pipeline tool. As of 2021-01-21, drake is superseded, which means there are no plans for new features or discretionary enhancements, but basic maintenance and support will continue indefinitely. Existing projects that use drake can safely continue to use drake, and there is no need to retrofit targets. New projects should use targets because it is friendlier and more robust.\nNearly four years of community feedback have exposed major user-side limitations regarding data management, collaboration, dynamic branching, and parallel efficiency. Unfortunately, these limitations are permanent. Solutions in drake itself would make the package incompatible with existing projects that use it, and the internal architecture is too copious, elaborate, and mature for such extreme refactoring. That is why targets was created. The targets package borrows from past learnings, user suggestions, discussions, complaints, success stories, and feature requests, and it improves the user experience in ways that will never be possible in drake.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>What about `drake`?</span>"
    ]
  },
  {
    "objectID": "drake.html#transitioning-to-targets",
    "href": "drake.html#transitioning-to-targets",
    "title": "Appendix B — What about drake?",
    "section": "B.2 Transitioning to targets",
    "text": "B.2 Transitioning to targets\nIf you know drake, then you already almost know targets. The programming style is similar, and most functions in targets have counterparts in drake.\n\n\n\nFunctions in drake\nCounterparts in targets\n\n\n\n\nuse_drake(), drake_script()\ntar_script()\n\n\ndrake_plan()\ntar_manifest(), tarchetypes::tar_plan()\n\n\ntarget()\ntar_target(), tar_target_raw()\n\n\ndrake_config()\ntar_option_set()\n\n\noutdated(), r_outdated()\ntar_outdated()\n\n\nvis_drake_graph(), r_vis_drake_graph()\ntar_visnetwork(), tar_glimpse()\n\n\ndrake_graph_info(), r_drake_graph_info()\ntar_network()\n\n\nmake(), r_make()\ntar_make(), tar_make_clustermq(), tar_make_future()\n\n\nloadd()\ntar_load()\n\n\nreadd()\ntar_read()\n\n\ndiagnose(), build_times(), cached(), drake_cache_log()\ntar_meta()\n\n\ndrake_progress(), drake_running(), drake_done(), drake_failed(), drake_cancelled()\ntar_progress()\n\n\nclean()\ntar_deduplicate(), tar_delete(), tar_destroy(), tar_invalidate()\n\n\ndrake_gc()\ntar_prune()\n\n\nid_chr()\ntar_name(), tar_path()\n\n\nknitr_in()\ntarchetypes::tar_render()\n\n\ncancel(), cancel_if()\ntar_cancel()\n\n\ntrigger()\ntar_cue()\n\n\ndrake_example(), drake_example(), load_mtcars_example(), clean_mtcars_example()\nUnsupported. Example targets pipelines are in individual repositories linked from here.\n\n\ndrake_build()\nUnsupported in targets to ensure coherence with dynamic branching.\n\n\ndrake_debug()\nSee the debugging chapter.\n\n\ndrake_history(), recoverable()\nUnsupported in targets. Instead of trying to manage history and data recovery directly, targets maintains a much lighter/friendlier data store to make it easier to use external data versioning tools instead.\n\n\nmissed(), tracked(), deps_code(), deps_target(), deps_knitr(), deps_profile()\nUnsupported in targets because dependency detection is easier to understand than in drake.\n\n\ndrake_hpc_template_file(), drake_hpc_template_files()\nDeemed out of scope for targets.\n\n\ndrake_cache(), new_cache(), find_cache().\nUnsupported because targets is far more strict and paternalistic about data/file management.\n\n\nrescue_cache(), which_clean(), cache_planned(), cache_unplanned()\nUnsupported due to the simplified data management system and storage cleaning functions.\n\n\ndrake_get_session_info()\nDeemed superfluous and a potential bottleneck. Discarded for targets.\n\n\nread_drake_seed()\nSuperfluous because targets always uses the same global seed. tar_meta() shows all the target-level seeds.\n\n\nshow_source()\nDeemed superfluous. Discarded in targets to conserve storage space in _targets/meta/meta.\n\n\ndrake_tempfile()\nSuperfluous in targets because there is no special disk.frame storage format. (File targets targets are much better for managing disk.frames.)\n\n\nfile_store()\nSuperfluous in targets because all files targets are tracked the same way and there is no longer a need to Base32-encode any file names.\n\n\n\nLikewise, many make() arguments have equivalent arguments elsewhere.\n\n\n\nArgument of drake::make()\nCounterparts in targets\n\n\n\n\ntargets\nnames in tar_make() etc.\n\n\nenvir\nenvir in tar_option_set()\n\n\nverbose\nreporter in tar_make() etc.\n\n\nparallelism\nChoice of function: tar_make() vs tar_make_clustermq() vs tar_make_future()\n\n\njobs\nworkers in tar_make_clustermq() and tar_make_future()\n\n\npackages\npackages in tar_target() and tar_option_set()\n\n\nlib_loc\nlibrary in tar_target() and tar_option_set()\n\n\ntrigger\ncue in tar_target() and tar_option_set()\n\n\ncaching\nstorage and retrieval in tar_target() and tar_option_set()\n\n\nkeep_going\nerror in tar_target() and tar_option_set()\n\n\nmemory_strategy\nmemory in tar_target() and tar_option_set()\n\n\ngarbage_collection\ngarbage_collection in tar_target() and tar_option_set()\n\n\ntemplate\nresources in tar_target() and tar_option_set(), along with helpers like tar_resources().\n\n\ncurl_handles\nhandle element of resources argument of tar_target() and tar_option_set()\n\n\nformat\nformat in tar_target() and tar_option_set()\n\n\nseed\nSuperfluous because targets always uses the same global seed. [tar_meta()] shows all the target-level seeds.\n\n\n\nIn addition, many optional columns of drake plans are expressed differently in targets.\n\n\n\nOptional column of drake plans\nFeature in targets\n\n\n\n\nformat\nformat argument of tar_target() and tar_option_set()\n\n\ndynamic\npattern argument of tar_target() and tar_option_set()\n\n\ntransform\nstatic branching functions in tarchetypes such as tar_map() and tar_combine()\n\n\ntrigger\ncue argument of tar_target() and tar_option_set()\n\n\nhpc\ndeployment argument of tar_target() and tar_option_set()\n\n\nresources\nresources argument of tar_target() and tar_option_set()\n\n\ncaching\nstorage and retrieval arguments of tar_target() and tar_option_set()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>What about `drake`?</span>"
    ]
  },
  {
    "objectID": "drake.html#advantages-of-targets-over-drake",
    "href": "drake.html#advantages-of-targets-over-drake",
    "title": "Appendix B — What about drake?",
    "section": "B.3 Advantages of targets over drake",
    "text": "B.3 Advantages of targets over drake\n\nB.3.1 Better guardrails by design\ndrake leaves ample room for user-side mistakes, and some of these mistakes require extra awareness or advanced knowledge of R to consistently avoid. The example behaviors below are too systemic to solve and still preserve back-compatibility.\n\nBy default, make() looks for functions and global objects in the parent environment of the calling R session. Because the global environment is often old and stale in practical situations, which causes targets to become incorrectly invalidated. Users need to remember to restart the session before calling make(). The issue is discussed here, and the discussion led to functions like r_make() which always create a fresh session to do the work. However, r_make() is not a complete replacement for make(), and beginner users still run into the original problems.\nSimilar to the above, make() does not find the intended functions and global objects if it is called in a different environment. Edge cases like this one and this one continue to surprise users.\ndrake is extremely flexible about the location of the .drake/ cache. When a user calls readd(), loadd(), make(), and similar functions, drake searches up through the parent directories until it finds a .drake/ folder. This flexibility seldom helps, and it creates uncertainty and inconsistency when it comes to initializing and accessing projects, especially if there are multiple projects with nested file systems.\n\nThe targets package solves all these issues by design. Functions tar_make(), tar_make_clustermq(), and tar_make_future() all create fresh new R sessions by default. They all require a _targets.R configuration file in the project root (working directory of the tar_make() call) so that the functions, global objects, and settings are all populated in the exact same way each session, leading to less frustration, greater consistency, and greater reproducibility. In addition, the _targets/ data store always lives in the project root.\n\n\nB.3.2 Enhanced debugging support\ntargets has enhanced debugging support. With the workspaces argument to tar_option_set(), users can locally recreate the conditions under which a target runs. This includes packages, global functions and objects, and the random number generator seed. Similarly, tar_option_set(error = \"workspace\") automatically saves debugging workspaces for targets that encounter errors. The debug option lets users enter an interactive debugger for a given target while the pipeline is running. And unlike drake, all debugging features are fully compatible with dynamic branching.\n\n\nB.3.3 Improved tracking of package functions\nBy default, targets ignores changes to functions inside external packages. However, if a workflow centers on a custom package with methodology under development, users can make targets automatically watch the package’s functions for changes. Simply supply the names of the relevant packages to the imports argument of tar_option_set(). Unlike drake, targets can track multiple packages this way, and the internal mechanism is much safer.\n\n\nB.3.4 Lighter, friendlier data management\ndrake’s cache is an intricate file system in a hidden .drake folder. It contains multiple files for each target, and those names are not informative. (See the files in the data/ folder in the diagram below.) Users often have trouble understanding how drake manages data, resolving problems when files are corrupted, placing the data under version control, collaborating with others on the same pipeline, and clearing out superfluous data when the cache grows large in storage.\n.drake/\n├── config/\n├── data/\n├───── 17bfcef645301416.rds\n├───── 21935c86f12692e2.rds\n├───── 37caf5df2892cfc4.rds\n├───── ...\n├── drake/\n├───── history/\n├───── return/\n├───── tmp/\n├── keys/ # A surprisingly large number of tiny text files live here.\n├───── memoize/\n├───── meta/\n├───── objects/\n├───── progress/\n├───── recover/\n├───── session/\n└── scratch/ # This folder should be temporary, but it gets egregiously large.\nThe targets takes a friendlier, more transparent, less mysterious approach to data management. Its data store is a visible _targets folder, and it contains far fewer files: a spreadsheet of metadata, a spreadsheet of target progress, and one informatively named data file for each target. It is much easier to understand the data management process, identify and diagnose problems, place projects under version control, and avoid consuming unnecessary storage resources. Sketch:\n_targets/\n├── meta/\n├───── meta\n├───── process\n├───── progress\n├── objects/\n├───── target_name_1\n├───── target_name_2\n├───── target_name_3\n├───── ...\n├── scratch/ # tar_make() deletes this folder after it finishes.\n└── user/ # gittargets users can put custom files here for data version control.\n\n\nB.3.5 Cloud storage\nThanks to the simplified data store and simplified internals, targets can automatically upload data to the Amazon S3 bucket of your choice. Simply configure aws.s3, create a bucket, and select one of the AWS-powered storage formats. Then, targets will automatically upload the return values to the cloud.\n\n# _targets.R\ntar_option_set(resources = list(bucket = \"my-bucket-name\"))\nlist(\n  tar_target(dataset, get_large_dataset(), format = \"aws_fst_tbl\"),\n  tar_target(analysis, analyze_dataset(dataset), format = \"aws_qs\")\n)\n\nData retrieval is still super easy.\n\ntar_read(dataset)\n\n\n\nB.3.6 Show status of functions and global objects\ndrake has several utilities that inform users which targets are up to date and which need to rerun. However, those utilities are limited by how drake manages functions and other global objects. Whenever drake inspects globals, it stores their values in its cache and loses track of their previous state from the last run of the pipeline. As a result, it has trouble informing users exactly why a given target is out of date. And because the system for tracking global objects is tightly coupled with the cache, this limitation is permanent.\nIn targets, the metadata management system only updates information on global objects when the pipeline actually runs. This makes it possible to understand which specific changes to your code could have invalided your targets. In large projects with long runtimes, this feature contributes significantly to reproducibility and peace of mind.\n\n\n\nB.3.7 Dynamic branching with dplyr::group_by()\nDynamic branching was an architecturally difficult fit in drake, and it can only support one single (vctrs-based) method of slicing and aggregation for processing sub-targets. This limitation has frustrated members of the community, as discussed here and here.\ntargets, on the other hand, is more flexible regarding slicing and aggregation. When it branches over an object, it can iterate over vectors, lists, and even data frames grouped with dplyr::group_by(). To branch over chunks of a data frame, our data frame target needs to have a special tar_group column. We can create this column in our target’s return value with the tar_group() function.\n\nlibrary(dplyr)\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\nlibrary(targets)\nlibrary(tarchetypes)\nlibrary(tibble)\ntibble(\n  x = seq_len(6),\n  id = rep(letters[seq_len(3)], each = 2)\n) %&gt;%\n  group_by(id) %&gt;%\n  tar_group()\n#&gt; # A tibble: 6 × 3\n#&gt;       x id    tar_group\n#&gt;   &lt;int&gt; &lt;chr&gt;     &lt;int&gt;\n#&gt; 1     1 a             1\n#&gt; 2     2 a             1\n#&gt; 3     3 b             2\n#&gt; 4     4 b             2\n#&gt; 5     5 c             3\n#&gt; 6     6 c             3\n\nOur actual target has the command above and iteration = \"group\".\n\ntar_target(\n  data,\n  tibble(\n    x = seq_len(6),\n    id = rep(letters[seq_len(3)], each = 2)\n  ) %&gt;%\n    group_by(id) %&gt;%\n    tar_group(),\n  iteration = \"group\"\n)\n\nNow, any target that maps over data is going to define one branch for each group in the data frame. The following target creates three branches when run in a pipeline: one returning 3, one returning 7, and one returning 11.\n\ntar_target(\n  sums,\n  sum(data$x),\n  pattern = map(data)\n)\n\n\n\nB.3.8 Composable dynamic branching\nBecause the design of targets is fundamentally dynamic, users can create complicated dynamic branching patterns that are never going to be possible in drake. Below, target z creates six branches, one for each combination of w and tuple (x, y). The pattern cross(w, map(x, y)) is equivalent to tidyr::crossing(w, tidyr::nesting(x, y)).\n\n# _targets.R\nlibrary(targets)\nlibrary(tarchetypes)\nlist(\n  tar_target(w, seq_len(2)),\n  tar_target(x, head(letters, 3)),\n  tar_target(y, head(LETTERS, 3)),\n  tar_target(\n    z,\n    data.frame(w = w, x = x, y = y),\n    pattern = cross(w, map(x, y))\n  )\n)\n\nThanks to glep and djbirke on GitHub for the idea.\n\n\nB.3.9 Improved parallel efficiency\nDynamic branching in drake is staged. In other words, all the sub-targets of a dynamic target must complete before the pipeline moves on to downstream targets. The diagram below illustrates this behavior in a pipeline with a dynamic target B that maps over another dynamic target A. For thousands of dynamic sub-targets with highly variable runtimes, this behavior consumes unnecessary runtime and computing resources. And because drake’s architecture was designed at a fundamental level for static branching only, this limitation is permanent.\n\nBy contrast, the internal data structures in targets are dynamic by design, which allows for a dynamic branching model with more flexibility and parallel efficiency. Branches can always start as soon as their upstream dependencies complete, even if some of those upstream dependencies are branches. This behavior reduces runtime and reduces consumption of computing resources.\n\n\n\nB.3.10 Metaprogramming\nIn drake, pipelines are defined with the drake_plan() function. drake_plan() supports an elaborate domain specific language that diffuses user-supplied R expressions. This makes it convenient to assign commands to targets in the vast majority of cases, but it also obstructs custom metaprogramming by users (example here). Granted, it is possible to completely circumvent drake_plan() and create the whole data frame from scratch, but this is hardly ideal and seldom done in practice.\nThe targets package tries to make customization easier. Relative to drake, targets takes a decentralized approach to setting up pipelines, moving as much custom configuration as possible to the target level rather than the whole pipeline level. In addition, the tar_target_raw() function avoids non-standard evaluation while mirroring tar_target() in all other respects. All this makes it much easier to create custom metaprogrammed pipelines and target archetypes while avoiding an elaborate domain specific language for static branching, which was extremely difficult to understand and error prone in drake. The R Targetopia is an emerging ecosystem of workflow frameworks that take full advantage of this customization and democratize reproducible pipelines.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>What about `drake`?</span>"
    ]
  },
  {
    "objectID": "markdown.html",
    "href": "markdown.html",
    "title": "Appendix C — Target Markdown",
    "section": "",
    "text": "C.1 Superseded\nTarget Markdown is superseded. Please instead use the tar_tangle() function from the tarchetypes package.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#about",
    "href": "markdown.html#about",
    "title": "Appendix C — Target Markdown",
    "section": "C.2 About",
    "text": "C.2 About\nTarget Markdown, available in targets &gt; 0.6.0, is knitr-based interface for reproducible analysis pipelines.1 With Target Markdown, you can define a fully scalable pipeline from within one or more Quarto or R Markdown reports or projects (even spreading a single pipeline over multiple source documents). You get the best of both worlds: the human readable narrative of literate programming, and the sophisticated caching and dependency management systems of targets.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#access",
    "href": "markdown.html#access",
    "title": "Appendix C — Target Markdown",
    "section": "C.3 Access",
    "text": "C.3 Access\nThis chapter’s example Target Markdown document is itself a tutorial and a simplified version of the chapter. There are two convenient ways to access the file:\n\nThe use_targets() function.\nThe RStudio R Markdown template system.\n\nFor (2), in the RStudio IDE, select a new Quarto or R Markdown document in the New File dropdown menu in the upper left-hand corner of the window.\n\nThen, select the Target Markdown template and click OK to open a copy of the report for editing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#purpose",
    "href": "markdown.html#purpose",
    "title": "Appendix C — Target Markdown",
    "section": "C.4 Purpose",
    "text": "C.4 Purpose\nTarget Markdown has two primary objectives:\n\nInteractively explore, prototype, and test the components of a targets pipeline using the Quarto notebook interface or the R Markdown notebook interface.\nSet up a targets pipeline using convenient Markdown-like code chunks.\n\nTarget Markdown supports a special {targets} language engine with an interactive mode for (1) and a non-interactive mode for (2). By default, the mode is interactive in the notebook interface and non-interactive when you knit/render the whole document.2. You can set the mode using the tar_interactive chunk option.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#example",
    "href": "markdown.html#example",
    "title": "Appendix C — Target Markdown",
    "section": "C.5 Example",
    "text": "C.5 Example\nThe following example is based on the minimal targets project at https://github.com/wlandau/targets-minimal/. We process the base airquality dataset, fit a model, and display a histogram of ozone concentration.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#required-packages",
    "href": "markdown.html#required-packages",
    "title": "Appendix C — Target Markdown",
    "section": "C.6 Required packages",
    "text": "C.6 Required packages\nThis example requires several R packages, and targets must be version 0.6.0 or above.\n\n# R console\ninstall.packages(c(\"biglm\", \"dplyr\", \"ggplot2\", \"readr\", \"targets\", \"tidyr\"))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#setup",
    "href": "markdown.html#setup",
    "title": "Appendix C — Target Markdown",
    "section": "C.7 Setup",
    "text": "C.7 Setup\nFirst, load targets to activate the specialized knitr engine for Target Markdown.\n```{r}\nlibrary(targets)\nlibrary(tarchetypes)\n```\nNon-interactive Target Markdown writes scripts to a special _targets_r/ directory to define individual targets and global objects. In order to keep your target definitions up to date, it is recommended to remove _targets_r/ at the beginning of the R Markdown document(s) in order to clear out superfluous targets and globals from a previous version. tar_unscript() is a convenient way to do this.\n```{r}\ntar_unscript()\n```",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#globals",
    "href": "markdown.html#globals",
    "title": "Appendix C — Target Markdown",
    "section": "C.8 Globals",
    "text": "C.8 Globals\nAs usual, your targets depend on custom functions, global objects, and tar_option_set() options you define before the pipeline begins. Define these globals using the {targets} engine with tar_globals = TRUE chunk option.\n```{targets some-globals, tar_globals = TRUE, tar_interactive = TRUE}\noptions(tidyverse.quiet = TRUE)\ntar_option_set(packages = c(\"biglm\", \"dplyr\", \"ggplot2\", \"readr\", \"tidyr\"))\ncreate_plot &lt;- function(data) {\n  ggplot(data) +\n    geom_histogram(aes(x = Ozone), bins = 12) +\n    theme_gray(24)\n}\n```\nIn interactive mode, the chunk simply runs the R code in the tar_option_get(\"envir\") environment (usually the global environment) and displays a message:\n\n#&gt; Run code and assign objects to the environment.\n\nHere is the same chunk in non-interactive mode. Normally, there is no need to duplicate chunks like this, but we do so here in order to demonstrate both modes.\n```{targets chunk-name, tar_globals = TRUE, tar_interactive = FALSE}\noptions(tidyverse.quiet = TRUE)\ntar_option_set(packages = c(\"biglm\", \"dplyr\", \"ggplot2\", \"readr\", \"tidyr\"))\ncreate_plot &lt;- function(data) {\n  ggplot(data) +\n    geom_histogram(aes(x = Ozone), bins = 12) +\n    theme_gray(24)\n}\n```\nIn non-interactive mode, the chunk establishes a common _targets.R file and writes the R code to a script in _targets_r/globals/, and displays an informative message:3\n\n#&gt; Establish _targets.R and _targets_r/globals/chunk-name.R.\n\nIt is good practice to assign explicit chunk labels or set the tar_name chunk option on a chunk-by-chunk basis. Each chunk writes code to a script path that depends on the name, and all script paths need to be unique.4",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#target-definitions",
    "href": "markdown.html#target-definitions",
    "title": "Appendix C — Target Markdown",
    "section": "C.9 Target definitions",
    "text": "C.9 Target definitions\nTo define targets of the pipeline, use the {targets} language engine with the tar_globals chunk option equal FALSE or NULL (default). The return value of the chunk must be a target definition object or a list of target definition objects, created by tar_target() or a similar function.\nBelow, we define a target to establish the air quality dataset in the pipeline.\n```{targets raw-data, tar_interactive = TRUE}\ntar_target(raw_data, airquality)\n```\nIf you run this chunk in interactive mode, the target’s R command runs, the engine tests if the output can be saved and loaded from disk correctly, and then the return value gets assigned to the tar_option_get(\"envir\") environment (usually the global environment).\n\n#&gt; Run targets and assign them to the environment.\n\nIn the process, some temporary files are created and destroyed, but your local file space will remain untouched (barring any custom side effects in your custom code).\nAfter you run a target in interactive mode, the return value is available in memory, and you can write an ordinary R code chunk to read it.\n```{r}\nhead(raw_data)\n```\nThe output is the same as what tar_read(raw_data) would show after a serious pipeline run.\n\nhead(raw_data)\n#&gt;   Ozone Solar.R Wind Temp Month Day\n#&gt; 1    41     190  7.4   67     5   1\n#&gt; 2    36     118  8.0   72     5   2\n#&gt; 3    12     149 12.6   74     5   3\n#&gt; 4    18     313 11.5   62     5   4\n#&gt; 5    NA      NA 14.3   56     5   5\n#&gt; 6    28      NA 14.9   66     5   6\n\nFor demonstration purposes, here is the raw_data target code chunk in non-interactive mode.\n```{targets chunk-name-with-target, tar_interactive = FALSE}\ntar_target(raw_data, airquality)\n```\nIn non-interactive mode, the {targets} engine does not actually run any targets. Instead, it establishes a common _targets.R and writes the code to a script in _targets_r/targets/.\n\n#&gt; Establish _targets.R and _targets_r/targets/chunk-name-with-target.R.\n\nNext, we define more targets to process the raw data and plot a histogram. Only the returned value of the chunk code actually becomes part of the pipeline, so if you define multiple targets in a single chunk, be sure to wrap them all in a list.\n```{targets downstream-targets}\nlist(\n  tar_target(data, raw_data %&gt;% filter(!is.na(Ozone))),\n  tar_target(hist, create_plot(data))\n)\n```\nIn non-interactive mode, the whole target list gets written to a single script.\n\n#&gt; Establish _targets.R and _targets_r/targets/downstream-targets.R.\n\nLastly, we define a target to fit a model to the data. For simple targets like this one, we can use convenient shorthand to convert the code in a chunk into a valid target. Simply set the tar_simple chunk option to TRUE.\n```{targets fit, tar_simple = TRUE}\nanalysis_data &lt;- data\nbiglm(Ozone ~ Wind + Temp, analysis_data)\n```\nWhen the chunk is preprocessed, chunk label (or the tar_name chunk option if you set it) becomes the target name, and the chunk code becomes the target command. All other arguments of tar_target() remain at their default values (configurable with tar_option_set() in a tar_globals = TRUE chunk). The output in the rendered R Markdown document reflects this preprocessing.\n\ntar_target(fit, {\n  biglm(Ozone ~ Wind + Temp, data)\n})\n#&gt; Define target fit from chunk code.\n#&gt; Establish _targets.R and _targets_r/targets/fit.R.\n\n\nC.9.1 Pipeline\nIf you ran all the {targets} chunks in non-interactive mode (i.e. pipeline construction mode), then the target script file and helper scripts should all be established, and you are ready to run the pipeline in with tar_make() in an ordinary {r} code chunk. This time, the output is written to persistent storage at the project root.\n```{r}\ntar_make()\n```\n\n#&gt; • start target raw_data\n#&gt; • built target raw_data [0.585 seconds]\n#&gt; • start target data\n#&gt; • built target data [0.009 seconds]\n#&gt; • start target fit\n#&gt; • built target fit [0.003 seconds]\n#&gt; • start target hist\n#&gt; • built target hist [0.014 seconds]\n#&gt; • end pipeline [0.765 seconds]\n\n\n\nC.9.2 Output\nYou can retrieve results from the _targets/ data store using tar_read() or tar_load().\n```{r}\nlibrary(biglm)\ntar_read(fit)\n```\n\n#&gt; Large data regression model: biglm(Ozone ~ Wind + Temp, data)\n#&gt; Sample size =  116\n\n```{r}\ntar_read(hist)\n```\n\n\n\n\n\n\n\n\n\nThe targets dependency graph helps your readers understand the steps of your pipeline at a high level.\n```{r}\ntar_visnetwork()\n```\n\n\n\n\n\n\nAt this point, you can go back and run {targets} chunks in interactive mode without interfering with the code or data of the non-interactive pipeline.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#conditioning-on-interactive-mode",
    "href": "markdown.html#conditioning-on-interactive-mode",
    "title": "Appendix C — Target Markdown",
    "section": "C.10 Conditioning on interactive mode",
    "text": "C.10 Conditioning on interactive mode\ntargets version 0.6.0.9001 and above supports the tar_interactive() function, which suppresses code unless Target Markdown interactive mode is turned on. Similarly, tar_noninteractive() suppresses code in interactive mode, and tar_toggle() selects alternative pieces of code based on the current mode.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#tar_interactive",
    "href": "markdown.html#tar_interactive",
    "title": "Appendix C — Target Markdown",
    "section": "C.11 tar_interactive()",
    "text": "C.11 tar_interactive()\ntar_interactive() is useful for dynamic branching. If a dynamic target branches over a target from a different chunk, this ordinarily breaks interactive mode.\n```{targets condition, tar_interactive = TRUE}\ntar_target(y, x ^ 2, pattern = map(x))\n```\n\n#&gt; Run targets and assign them to the environment.\n#&gt; Error:\n#&gt; ! Target y tried to branch over x, which is illegal...\n\nHowever, with tar_interactive(), you can define a version of x just for testing and prototyping in interactive mode. The chunk below fixes interactive mode without changing the pipeline in non-interactive mode.\n```{targets condition-fixed, tar_interactive = TRUE}\nlist(\n  tar_interactive(tar_target(x, seq_len(2))),\n  tar_target(y, x ^ 2, pattern = map(x))\n)\n```\n\n#&gt; Run targets and assign them to the environment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#tar_toggle",
    "href": "markdown.html#tar_toggle",
    "title": "Appendix C — Target Markdown",
    "section": "C.12 tar_toggle()",
    "text": "C.12 tar_toggle()\ntar_toggle() is useful for scaling up and down the amount of work based on the current mode. Interactive mode should finish quickly for prototyping and testing, and non-interactive mode should take on the full level work required for a serious pipeline. Below, tar_toggle() seamlessly scales up and down the number of simulations repetitions in the example target from https://wlandau.github.io/rmedicine2021-pipeline/#target-definitions. To learn more about stantargets, visit https://docs.ropensci.org/stantargets/.\n```{targets bayesian-model-validation, tar_interactive = TRUE}\ntar_stan_mcmc_rep_summary(\n  name = mcmc,\n  stan_files = \"model.stan\",\n  data = simulate_data(), # Defined in another code chunk.\n  batches = tar_toggle(1, 100),\n  reps = tar_toggle(1, 10),\n  chains = tar_toggle(1, 4),\n  parallel_chains = tar_toggle(1, 4),\n  iter_warmup = tar_toggle(100, 4e4),\n  iter_sampling = tar_toggle(100, 4e4),\n  summaries = list(\n    ~posterior::quantile2(.x, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)),\n    rhat = ~posterior::rhat(.x)\n  ),\n  deployment = \"worker\"\n)\n```",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#chunk-options",
    "href": "markdown.html#chunk-options",
    "title": "Appendix C — Target Markdown",
    "section": "C.13 Chunk options",
    "text": "C.13 Chunk options\n\ntar_globals: Logical of length 1, whether to define globals or targets. If TRUE, the chunk code defines functions, objects, and options common to all the targets. If FALSE or NULL (default), then the chunk returns formal targets for the pipeline.\ntar_interactive: Logical of length 1 to choose whether to run the chunk in interactive mode or non-interactive mode.\ntar_name: name to use for writing helper script files (e.g. _targets_r/targets/target_script.R) and specifying target names if the tar_simple chunk option is TRUE. All helper scripts and target names must have unique names, so please do not set this option globally with knitr::opts_chunk$set().\ntar_script: Character of length 1, where to write the target script file in non-interactive mode. Most users can skip this option and stick with the default _targets.R script path. Helper script files are always written next to the target script in a folder with an \"_r\" suffix. The tar_script path must either be absolute or be relative to the project root (where you call tar_make() or similar). If not specified, the target script path defaults to tar_config_get(\"script\") (default: _targets.R; helpers default: _targets_r/). When you run tar_make() etc. with a non-default target script, you must select the correct target script file either with the script argument or with tar_config_set(script = ...). The function will source() the script file from the current working directory (i.e. with chdir = FALSE in source()).\ntar_simple: Logical of length 1. Set to TRUE to define a single target with a simplified interface. In code chunks with tar_simple equal to TRUE, the chunk label (or the tar_name chunk option if you set it) becomes the name, and the chunk code becomes the command. In other words, a code chunk with label targetname and command mycommand() automatically gets converted to tar_target(name = targetname, command = mycommand()). All other arguments of tar_target() remain at their default values (configurable with tar_option_set() in a tar_globals = TRUE chunk).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  },
  {
    "objectID": "markdown.html#footnotes",
    "href": "markdown.html#footnotes",
    "title": "Appendix C — Target Markdown",
    "section": "",
    "text": "Target Markdown is powered entirely by targets and knitr. It does not actually require Markdown, although Markdown is the recommended way to interact with it.↩︎\nIn targets version 0.6.0, the mode is interactive if interactive() is TRUE. In subsequent versions, the mode is interactive if !isTRUE(getOption(\"knitr.in.progress\")) is TRUE.↩︎\nThe _targets.R file from Target Markdown never changes from chunk to chunk or report to report, so you can spread your work over multiple reports without worrying about aligning _targets.R scripts. Just be sure all your chunk names are unique across all the reports of a project, or you set the tar_name chunk option to specify base names of script file paths.↩︎\nIn addition, for bookdown projects, chunk labels should only use alphanumeric characters and dashes.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Target Markdown</span>"
    ]
  }
]